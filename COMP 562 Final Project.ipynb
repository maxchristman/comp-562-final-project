{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6183c9b",
   "metadata": {},
   "source": [
    "# COMP 562 - Machine Learning Final Project\n",
    "## Plan\n",
    "\n",
    "Our goal is to distinguish between tweets which are about real disasters and those which are about fake, metaphorical, or otherwise not real ones.\n",
    "### Turning tweets into features\n",
    "\n",
    "- Start with trigrams, can tune later\n",
    "- Can consider bigrams, bag of words, or other n-grams\n",
    "- Ignore location information, at least for now\n",
    "- Almost all tweets have keywords, use as another feature\n",
    "- Make sure to process \"keyword\" values, removing special characters\n",
    "\n",
    "### Criteria for disaster\n",
    "- Meant to track if tweets are referring to ongoing disasters\n",
    "- Also includes historical events\n",
    "\n",
    "\n",
    "### Training\n",
    "- Train and validate our model on `train.csv` \n",
    "- Test by sending results to Kaggle\n",
    "\n",
    "### Random forest\n",
    "- Use Gini criterion for efficiency\n",
    "\n",
    "### Neural networks\n",
    "- Use multi-layer perceptron classifier\n",
    "- Tweak alpha values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdba33",
   "metadata": {},
   "source": [
    "## Disaster tweet classification\n",
    "### Important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f756010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7a1c2",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c957f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"disaster-tweets/data/train.csv\")\n",
    "test_df = pd.read_csv(\"disaster-tweets/data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47049af",
   "metadata": {},
   "source": [
    "### Finding all characters in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"http://t\\.co/\\S+\", \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa81936",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = set()\n",
    "\n",
    "for tweet in train_df['text']:\n",
    "    all_characters = all_characters.union(set(standardize_string(tweet)))\n",
    "\n",
    "char_list = list(all_characters)\n",
    "char_list.sort()\n",
    "print(char_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8ed8c",
   "metadata": {},
   "source": [
    "### Narrowing down characters\n",
    "\n",
    "We decided that from these characters, we would only keep letters, numbers, and a few accented characters. We also kept '#' and '@' due to their importance on Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "included_chars = list(string.ascii_lowercase + string.digits) + ['#', '@', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', 'ü', ' ']\n",
    "print(included_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d2846",
   "metadata": {},
   "source": [
    "### Removing invalid characters\n",
    "- Try both with and without removing special characters\n",
    "- Consider skipping data points with bad characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    for c in char_list:\n",
    "        if c not in included_chars:\n",
    "            s = s.replace(c, \"\")\n",
    "    return s\n",
    "\n",
    "def format_tweet(t):\n",
    "    # Makes lowercase\n",
    "    formatted_tweet = t.lower()\n",
    "    # Removed links\n",
    "    formatted_tweet = re.sub(\" http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    formatted_tweet = re.sub(\"http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    # Removes any special characters, other than a-z, numbers, spaces, hashtags, and @\n",
    "    formatted_tweet = remove_special_characters(formatted_tweet)\n",
    "    final_tweet_array = []\n",
    "    \n",
    "    # Removes multiple consecutive spaces\n",
    "    for i, char in enumerate(formatted_tweet):\n",
    "        if i == 0:\n",
    "            if char != ' ':\n",
    "                final_tweet_array.append(char)\n",
    "                continue\n",
    "        prev_char = formatted_tweet[i-1]\n",
    "        if char == ' ' and prev_char == ' ':\n",
    "            continue\n",
    "        final_tweet_array.append(char)\n",
    "    final_tweet = \"\".join(final_tweet_array)\n",
    "    return final_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_train_tweets = []\n",
    "for i, tweet in enumerate(train_df[\"text\"]):\n",
    "    formatted_train_tweets.append(format_tweet(tweet))\n",
    "\n",
    "formatted_test_tweets = []\n",
    "for tweet in test_df[\"text\"]:\n",
    "    formatted_test_tweets.append(format_tweet(tweet))\n",
    "    \n",
    "test_ids = test_df['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5449f6",
   "metadata": {},
   "source": [
    "### Splitting tweets into bigrams\n",
    "\n",
    "Tweets were processed into bigram representations, which includes information about two consecutive words at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb957c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_train = bigram_vectorizer.fit_transform(formatted_train_tweets)\n",
    "bigram_test = bigram_vectorizer.transform(formatted_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78587c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gram no string formatting\n",
    "# array([0.55543823, 0.50891089, 0.54221388, 0.51913133, 0.68794326])\n",
    "# 1 and 2-gram, no string formatting\n",
    "# array([0.46118721, 0.45027322, 0.43412527, 0.44141069, 0.61523626])\n",
    "# 1-gram basic string formatting\n",
    "# array([0.57556936, 0.48219736, 0.5530303 , 0.51859504, 0.68586387])\n",
    "# 1 & 2-gram, basic string formatting\n",
    "# array([0.5039019 , 0.41150442, 0.41241685, 0.45823928, 0.62327416])\n",
    "# bigram only, basic string formatting\n",
    "# array([0.24096386, 0.25725095, 0.1682243 , 0.17475728, 0.31060606])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25714b",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=10, max_depth=None, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812152d",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50555ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameters = {\n",
    "    'min_samples_split': range(2, 5),\n",
    "    'min_samples_leaf': range(1, 4),\n",
    "    'n_estimators': [50, 100, 500]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(rf, rf_parameters, verbose=3, n_jobs=10)\n",
    "rf_cv.fit(bigram_train, train_df['target'])\n",
    "print(rf_cv.best_params_)\n",
    "\n",
    "# {'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 500}\n",
    "# {'class_weight': 'balanced', 'max_depth': None, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2819",
   "metadata": {},
   "source": [
    "#### Predicting test data\n",
    "\n",
    "As the disaster tweets dataset is from a Kaggle competition, the creators chose to not make the test labels public. As such, we have the random forest model make predictions on the test data. We then submitted this data to Kaggle to get an accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predicted_classes = rf_cv.predict(bigram_test)\n",
    "print(rf_predicted_classes)\n",
    "rf_out_array = []\n",
    "for i, pred_class in enumerate(rf_predicted_classes):\n",
    "    rf_out_array.append([int(test_ids[i]), pred_class])\n",
    "\n",
    "np.savetxt(\"disaster-tweets/rf-results.csv\", rf_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6747b78",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classifier\n",
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = MLPClassifier(verbose=True, tol=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53246529",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_parameters = {\n",
    "    \"alpha\": [.0001, .001, .01, .1]\n",
    "}\n",
    "mlpc_cv = GridSearchCV(mlpc, mlpc_parameters, verbose=3, n_jobs=-1)\n",
    "mlpc_cv.fit(bigram_train, train_df['target'])\n",
    "print(mlpc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a7f07",
   "metadata": {},
   "source": [
    "#### Predicting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602abec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_predicted_classes = mlpc_cv.predict(bigram_test)\n",
    "mlpc_out_array = []\n",
    "for i, pred_class in enumerate(mlpc_predicted_classes):\n",
    "    mlpc_out_array.append([int(test_ids[i]), pred_class])\n",
    "    \n",
    "np.savetxt(\"disaster-tweets/mlpc-results.csv\", mlpc_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18e351",
   "metadata": {},
   "source": [
    "#### Retrying with tri-grams\n",
    "\n",
    "Due to undesirable low accuracy, we tried to train a multilayer perceptron classifier again, this time with tweet data represented as tri-grams rather than bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "trigram_train = trigram_vectorizer.fit_transform(formatted_train_tweets)\n",
    "trigram_test = trigram_vectorizer.transform(formatted_test_tweets)\n",
    "\n",
    "mlpc_cv.fit(trigram_train, train_df['target'])\n",
    "print(mlpc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_trigram_predicted_classes = mlpc_cv.predict(trigram_test)\n",
    "mlpc_trigram_out_array = []\n",
    "for i, pred_class in enumerate(mlpc_trigram_predicted_classes):\n",
    "    mlpc_trigram_out_array.append([int(test_ids[i]), pred_class])\n",
    "    \n",
    "np.savetxt(\"disaster-tweets/mlpc-trigam-results.csv\", mlpc_trigram_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de92e1f",
   "metadata": {},
   "source": [
    "## Humor detection\n",
    "### Important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032d477",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "\n",
    "We decided to train on only 10 percent of the data, as there are 200,000 data points. Training on, for example, 60 percent of the data would take prohibitively long (more than 10 minutes for one fit). However, evaluation time is more reasonable than training time, so we can still test on the remaining 90 percent of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4daf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_text = pd.read_csv(\"humor-dataset.csv\")[\"text\"]\n",
    "all_humor = pd.read_csv(\"humor-dataset.csv\")[\"humor\"]\n",
    "\n",
    "text_train, text_test, humor_train, humor_test = train_test_split(all_text, all_humor, train_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e9452",
   "metadata": {},
   "source": [
    "### Making bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_train = bigram_vectorizer.fit_transform(text_train)\n",
    "bigram_test = bigram_vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc1b38",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "#### Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d309f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=None, class_weight=\"balanced\", n_jobs=8, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aed44e",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 500]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(rf, rf_params, n_jobs=8, verbose=3)\n",
    "rf_cv.fit(bigram_train, humor_train)\n",
    "print(rf_cv.best_params_)\n",
    "# {'n_estimators': 100}: 0.8332"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83a27b",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The humor dataset, unlike the disaster tweets dataset, included labels for every piece of text in the dataset. As such, we are able to directly compute accuracy/F1 scores and also to plot confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_cm(preds):\n",
    "    return confusion_matrix(humor_test, preds, labels=[False, True], normalize='all')\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    fx = sb.heatmap(cm, annot=True, cmap='turbo')\n",
    "\n",
    "    # labels the title and x, y axis of plot\n",
    "    fx.set_title(title + '\\n\\n');\n",
    "    fx.set_xlabel('Predicted Values')\n",
    "    fx.set_ylabel('Actual Values ');\n",
    "\n",
    "    # labels the boxes\n",
    "    fx.xaxis.set_ticklabels(['False','True'])\n",
    "    fx.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7607f",
   "metadata": {},
   "source": [
    "#### Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52be7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "humor_preds = rf.predict(bigram_test)\n",
    "rf_cm = get_cm(humor_preds)\n",
    "plot_confusion_matrix(rf_cm, \"Random forest with bigrams\")\n",
    "rf_cv.score(bigram_test, humor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e7ba8",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classifier\n",
    "#### Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier(tol=.001, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bcd59",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6df991",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_params = {\n",
    "    \"alpha\": [.0001, .001, .01, .1]\n",
    "}\n",
    "print(\"Starting CV...\")\n",
    "mlpc_cv = GridSearchCV(mlpc, mlpc_params, verbose=3, n_jobs=8)\n",
    "mlpc_cv.fit(bigram_train, humor_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167e5ce",
   "metadata": {},
   "source": [
    "#### Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46086c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "humor_preds = mlpc.predict(bigram_test)\n",
    "mlpc_cm = get_cm(humor_preds)\n",
    "plot_confusion_matrix(mlpc_cm, \"Multilayer perceptron with bigrams\")\n",
    "mlpc_cv.score(bigram_test, humor_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
