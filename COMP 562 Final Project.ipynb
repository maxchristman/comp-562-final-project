{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6183c9b",
   "metadata": {},
   "source": [
    "## Plan\n",
    "### Turning tweets into features\n",
    "\n",
    "- Start with trigrams, can tune later\n",
    "- Can consider bigrams, bag of words, or other n-grams\n",
    "- Ignore location information, at least for now\n",
    "- Almost all tweets have keywords, use as another feature\n",
    "- Make sure to process \"keyword\" values, removing special characters\n",
    "\n",
    "### Criteria for disaster\n",
    "- Meant to track if tweets are referring to ongoing disasters\n",
    "- Also includes historical events\n",
    "\n",
    "\n",
    "### Training\n",
    "- Train and validate our model on `train.csv` \n",
    "- Test by sending results to Kaggle\n",
    "\n",
    "### Random forest\n",
    "- Use Gini criterion for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdba33",
   "metadata": {},
   "source": [
    "## Importing and vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f756010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c957f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08227f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4651bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"http://t\\.co/\\S+\", \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b09e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://t.co/\n"
     ]
    }
   ],
   "source": [
    "print(standardize_string(\"http://t.co/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa81936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x89', '\\x9d', '¡', '¢', '£', '¤', '¨', '©', 'ª', '«', '¬', '´', '¼', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', '÷', 'û', 'ü']\n"
     ]
    }
   ],
   "source": [
    "all_characters = set()\n",
    "\n",
    "for tweet in train_df['text']:\n",
    "    all_characters = all_characters.union(set(standardize_string(tweet)))\n",
    "\n",
    "char_list = list(all_characters)\n",
    "char_list.sort()\n",
    "print(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc3fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '#', '@', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', 'ü', ' ']\n"
     ]
    }
   ],
   "source": [
    "included_chars = list(string.ascii_lowercase + string.digits) + ['#', '@', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', 'ü', ' ']\n",
    "print(included_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d2846",
   "metadata": {},
   "source": [
    "### Stripping characters\n",
    "- Try both with and without removing special characters\n",
    "- Consider skipping data points with bad characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "096b1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    for c in char_list:\n",
    "        if c not in included_chars:\n",
    "            s = s.replace(c, \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3140441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_array(t):\n",
    "    t = standardize_string(t)\n",
    "    tweet_array = t.split()\n",
    "    return tweet_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d38ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data():\n",
    "    all_tweets = []\n",
    "    for tweet in train_df['text']:\n",
    "        tweet_array = tweet_to_array(tweet)\n",
    "        invalid_fields = []\n",
    "        for field in tweet_array:\n",
    "            for c in excluded_chars:\n",
    "                if c in field:\n",
    "                    invalid_fields.append(field)\n",
    "                    continue\n",
    "        \n",
    "        for f in invalid_fields:\n",
    "            del f\n",
    "        all_tweets.append(tweet_array)\n",
    "        \n",
    "    return all_tweets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5496e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Jarem:\n",
    "# mock-up of random forest, using these sites as reference:\n",
    "# https://machinelearningmastery.com/random-forest-ensemble-in-python/\n",
    "# https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial/notebook\n",
    "# getting super bad performance - worse than chance often - and I don't know much hyperparameter tuning will fix it\n",
    "# not sure what I'm doing wrong here - n-grams and text formatting seem to make it worse\n",
    "\n",
    "def format_tweet(t):\n",
    "    # Makes lowercase\n",
    "    formatted_tweet = t.lower()\n",
    "    # Removed links\n",
    "    formatted_tweet = re.sub(\" http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    formatted_tweet = re.sub(\"http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    # Removes any special characters, other than a-z, numbers, spaces, hashtags, and @\n",
    "    formatted_tweet = remove_special_characters(formatted_tweet)\n",
    "    final_tweet_array = []\n",
    "    \n",
    "    # Removes multiple consecutive spaces\n",
    "    for i, char in enumerate(formatted_tweet):\n",
    "        if i == 0:\n",
    "            if char != ' ':\n",
    "                final_tweet_array.append(char)\n",
    "                continue\n",
    "        prev_char = formatted_tweet[i-1]\n",
    "        if char == ' ' and prev_char == ' ':\n",
    "            continue\n",
    "        final_tweet_array.append(char)\n",
    "    final_tweet = \"\".join(final_tweet_array)\n",
    "    return final_tweet\n",
    "\n",
    "formatted_train_tweets = []\n",
    "\n",
    "#basic string formatting\n",
    "for i, tweet in enumerate(train_df[\"text\"]):\n",
    "    formatted_train_tweets.append(format_tweet(tweet))\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "train_vectors = count_vectorizer.fit_transform(formatted_train_tweets)\n",
    "\n",
    "formatted_test_tweets = []\n",
    "\n",
    "for tweet in test_df[\"text\"]:\n",
    "    formatted_test_tweets.append(format_tweet(tweet))\n",
    "    \n",
    "test_ids = test_df['id']\n",
    "\n",
    "test_vectors = count_vectorizer.transform(formatted_test_tweets)\n",
    "\n",
    "\n",
    "# 1-gram no string formatting\n",
    "# array([0.55543823, 0.50891089, 0.54221388, 0.51913133, 0.68794326])\n",
    "# 1 and 2-gram, no string formatting\n",
    "# array([0.46118721, 0.45027322, 0.43412527, 0.44141069, 0.61523626])\n",
    "# 1-gram basic string formatting\n",
    "# array([0.57556936, 0.48219736, 0.5530303 , 0.51859504, 0.68586387])\n",
    "# 1 & 2-gram, basic string formatting\n",
    "# array([0.5039019 , 0.41150442, 0.41241685, 0.45823928, 0.62327416])\n",
    "# bigram only, basic string formatting\n",
    "# array([0.24096386, 0.25725095, 0.1682243 , 0.17475728, 0.31060606])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "348bb7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m rf_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m500\u001b[39m]\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     13\u001b[0m rf_cv \u001b[38;5;241m=\u001b[39m GridSearchCV(rf, parameters, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrf_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(rf_cv\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=10, max_depth=None, class_weight=\"balanced\")\n",
    "\n",
    "rf_parameters = {\n",
    "    'min_samples_split': range(2, 5),\n",
    "    'min_samples_leaf': range(1, 4),\n",
    "    'n_estimators': [50, 100, 500]\n",
    "}\n",
    "\n",
    "\n",
    "rf_cv = GridSearchCV(rf, parameters, verbose=3, n_jobs=10)\n",
    "rf_cv.fit(train_vectors, train_df['target'])\n",
    "print(rf_cv.best_params_)\n",
    "\n",
    "# {'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 500}\n",
    "# {'class_weight': 'balanced', 'max_depth': None, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "726504ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 1 1]\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.641 total time= 1.3min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.667 total time=  35.2s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.654 total time= 1.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.611 total time= 5.9min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.671 total time=  24.3s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.671 total time=  46.4s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.670 total time= 3.8min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.630 total time=  42.6s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.649 total time=  22.7s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.653 total time=  46.3s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.625 total time= 3.5min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.632 total time=  10.8s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.612 total time=  11.7s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.625 total time=  20.5s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.605 total time= 1.7min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.657 total time= 1.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.616 total time=  36.4s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.614 total time= 1.2min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.658 total time= 5.8min\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.617 total time=  23.5s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.627 total time=  46.8s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.613 total time= 3.8min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.651 total time=  44.9s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.628 total time=  22.1s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.624 total time=  44.1s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.634 total time= 3.5min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.642 total time=  11.4s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.613 total time=  10.7s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.645 total time=  22.3s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.628 total time= 1.7min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.605 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.618 total time=  34.5s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.641 total time= 1.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.599 total time= 5.7min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.632 total time=  21.0s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.635 total time=  45.2s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.652 total time= 3.8min\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.619 total time=  22.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.612 total time=  45.7s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.625 total time=  23.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.609 total time=  46.3s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.616 total time= 3.8min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.621 total time=  19.3s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.643 total time= 1.8min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.642 total time=  37.8s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.658 total time= 6.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.618 total time=  34.3s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.609 total time= 1.1min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.611 total time= 5.7min\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.615 total time= 3.8min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.641 total time=  23.2s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.611 total time= 1.6min\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.618 total time= 1.8min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.609 total time= 1.2min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.642 total time=  36.4s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.619 total time= 1.1min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.641 total time= 5.9min\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.620 total time=  22.5s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.611 total time=  43.7s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.622 total time= 3.6min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.628 total time=  21.1s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.626 total time=  42.1s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.631 total time=  22.2s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.632 total time=  43.0s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.655 total time= 3.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.671 total time=  11.7s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.620 total time=  22.5s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.670 total time= 1.8min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.616 total time=  36.7s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.601 total time= 5.9min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.640 total time=  34.3s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.617 total time= 1.1min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.639 total time= 5.8min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.663 total time=  45.8s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.670 total time=  23.0s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.675 total time=  45.4s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.668 total time= 3.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.673 total time=  23.2s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.626 total time=  10.5s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.649 total time=  22.6s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.626 total time= 1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.610 total time=  36.3s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.618 total time= 5.9min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.621 total time=  32.6s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.640 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.619 total time= 5.6min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.658 total time=  21.5s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.622 total time= 3.6min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.624 total time=  10.2s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.665 total time=  11.8s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.660 total time=  23.0s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.667 total time= 1.9min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.615 total time=  20.1s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.620 total time= 1.4min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.604 total time=  35.5s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.637 total time= 6.2min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.660 total time=  34.4s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.658 total time= 1.1min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.662 total time= 5.7min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.673 total time= 3.8min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.621 total time=  20.2s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.641 total time= 1.8min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.603 total time=  10.3s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.629 total time=  22.1s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.644 total time= 1.4min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.609 total time= 1.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.598 total time=  34.6s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.599 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.618 total time= 5.7min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.653 total time=  22.9s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.649 total time=  48.2s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.634 total time= 3.6min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.655 total time=  23.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.614 total time=  22.3s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.631 total time= 3.5min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.647 total time=  11.3s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.615 total time=   9.7s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.605 total time=  21.0s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.628 total time= 1.7min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.643 total time=  11.1s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.624 total time=  11.8s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.624 total time=  23.6s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.621 total time= 1.3min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.659 total time=  37.1s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.609 total time= 6.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.599 total time=  33.4s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.595 total time= 1.1min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.601 total time= 5.7min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.654 total time= 3.8min\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.622 total time=  11.4s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.621 total time=  21.9s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.621 total time= 1.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.673 total time=  12.1s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.677 total time=  23.8s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.670 total time= 1.3min\n"
     ]
    }
   ],
   "source": [
    "rf_predicted_classes = rf_cv.predict(test_vectors)\n",
    "print(rf_predicted_classes)\n",
    "rf_out_array = []\n",
    "for i, pred_class in enumerate(rf_predicted_classes):\n",
    "    rf_out_array.append([int(test_ids[i]), pred_class])\n",
    "\n",
    "np.savetxt(\"rf-results.csv\", rf_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c621309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Iteration 1, loss = 0.64550188\n",
      "Iteration 2, loss = 0.37110657\n",
      "Iteration 3, loss = 0.16113751\n",
      "Iteration 4, loss = 0.09245533\n",
      "Iteration 5, loss = 0.06919119\n",
      "Iteration 6, loss = 0.05685333\n",
      "Iteration 7, loss = 0.05051970\n",
      "Iteration 8, loss = 0.04652030\n",
      "Iteration 9, loss = 0.04329465\n",
      "Iteration 10, loss = 0.04027972\n",
      "Iteration 11, loss = 0.03963507\n",
      "Iteration 12, loss = 0.03706136\n",
      "Iteration 13, loss = 0.03634189\n",
      "Iteration 14, loss = 0.03578973\n",
      "Iteration 15, loss = 0.03438279\n",
      "Iteration 16, loss = 0.03405455\n",
      "Iteration 17, loss = 0.03368897\n",
      "Iteration 18, loss = 0.03330628\n",
      "Iteration 19, loss = 0.03267945\n",
      "Iteration 20, loss = 0.03235284\n",
      "Iteration 21, loss = 0.03212069\n",
      "Iteration 22, loss = 0.03165786\n",
      "Iteration 23, loss = 0.03103042\n",
      "Iteration 24, loss = 0.03137677\n",
      "Iteration 25, loss = 0.03200844\n",
      "Iteration 26, loss = 0.03082211\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "{'alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = MLPClassifier(verbose=True, tol=.001)\n",
    "mlpc_parameters = {\n",
    "    \"alpha\": [.0001, .001, .01, .1]\n",
    "}\n",
    "mlpc_cv = GridSearchCV(mlpc, mlpc_parameters, verbose=3, n_jobs=-1)\n",
    "mlpc_cv.fit(train_vectors, train_df['target'])\n",
    "print(mlpc_cv.best_params_)\n",
    "\n",
    "mlpc_predicted_classes = mlpc_cv.predict(test_vectors)\n",
    "mlpc_out_array = []\n",
    "for i, pred_class in enumerate(mlpc_predicted_classes):\n",
    "    mlpc_out_array.append([int(test_ids[i]), pred_class])\n",
    "    \n",
    "np.savetxt(\"mlpc-results.csv\", mlpc_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bfb716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
