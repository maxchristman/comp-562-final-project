{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6183c9b",
   "metadata": {},
   "source": [
    "# COMP 562 - Machine Learning Final Project\n",
    "## Plan\n",
    "\n",
    "Our goal is to distinguish between tweets which are about real disasters and those which are about fake, metaphorical, or otherwise not real ones.\n",
    "### Turning tweets into features\n",
    "\n",
    "- Start with trigrams, can tune later\n",
    "- Can consider bigrams, bag of words, or other n-grams\n",
    "- Ignore location information, at least for now\n",
    "- Almost all tweets have keywords, use as another feature\n",
    "- Make sure to process \"keyword\" values, removing special characters\n",
    "\n",
    "### Criteria for disaster\n",
    "- Meant to track if tweets are referring to ongoing disasters\n",
    "- Also includes historical events\n",
    "\n",
    "\n",
    "### Training\n",
    "- Train and validate our model on `train.csv` \n",
    "- Test by sending results to Kaggle\n",
    "\n",
    "### Random forest\n",
    "- Use Gini criterion for efficiency\n",
    "\n",
    "### Neural networks\n",
    "- Use multi-layer perceptron classifier\n",
    "- Tweak alpha values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdba33",
   "metadata": {},
   "source": [
    "## Disaster tweet classification\n",
    "### Important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f756010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd7a1c2",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c957f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"diaster-tweets/data/train.csv\")\n",
    "test_df = pd.read_csv(\"disaster-tweets/data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47049af",
   "metadata": {},
   "source": [
    "### Finding all characters in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4651bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"http://t\\.co/\\S+\", \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa81936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x89', '\\x9d', '¡', '¢', '£', '¤', '¨', '©', 'ª', '«', '¬', '´', '¼', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', '÷', 'û', 'ü']\n"
     ]
    }
   ],
   "source": [
    "all_characters = set()\n",
    "\n",
    "for tweet in train_df['text']:\n",
    "    all_characters = all_characters.union(set(standardize_string(tweet)))\n",
    "\n",
    "char_list = list(all_characters)\n",
    "char_list.sort()\n",
    "print(char_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8ed8c",
   "metadata": {},
   "source": [
    "### Narrowing down characters\n",
    "\n",
    "We decided that from these characters, we would only keep letters, numbers, and a few accented characters. We also kept '#' and '@' due to their importance on Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc3fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '#', '@', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', 'ü', ' ']\n"
     ]
    }
   ],
   "source": [
    "included_chars = list(string.ascii_lowercase + string.digits) + ['#', '@', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', 'ü', ' ']\n",
    "print(included_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d2846",
   "metadata": {},
   "source": [
    "### Removing invalid characters\n",
    "- Try both with and without removing special characters\n",
    "- Consider skipping data points with bad characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "096b1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    for c in char_list:\n",
    "        if c not in included_chars:\n",
    "            s = s.replace(c, \"\")\n",
    "    return s\n",
    "\n",
    "def format_tweet(t):\n",
    "    # Makes lowercase\n",
    "    formatted_tweet = t.lower()\n",
    "    # Removed links\n",
    "    formatted_tweet = re.sub(\" http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    formatted_tweet = re.sub(\"http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    # Removes any special characters, other than a-z, numbers, spaces, hashtags, and @\n",
    "    formatted_tweet = remove_special_characters(formatted_tweet)\n",
    "    final_tweet_array = []\n",
    "    \n",
    "    # Removes multiple consecutive spaces\n",
    "    for i, char in enumerate(formatted_tweet):\n",
    "        if i == 0:\n",
    "            if char != ' ':\n",
    "                final_tweet_array.append(char)\n",
    "                continue\n",
    "        prev_char = formatted_tweet[i-1]\n",
    "        if char == ' ' and prev_char == ' ':\n",
    "            continue\n",
    "        final_tweet_array.append(char)\n",
    "    final_tweet = \"\".join(final_tweet_array)\n",
    "    return final_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5496e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_train_tweets = []\n",
    "for i, tweet in enumerate(train_df[\"text\"]):\n",
    "    formatted_train_tweets.append(format_tweet(tweet))\n",
    "\n",
    "formatted_test_tweets = []\n",
    "for tweet in test_df[\"text\"]:\n",
    "    formatted_test_tweets.append(format_tweet(tweet))\n",
    "    \n",
    "test_ids = test_df['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5449f6",
   "metadata": {},
   "source": [
    "### Splitting tweets into bigrams\n",
    "\n",
    "Tweets were processed into bigram representations, which includes information about two consecutive words at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb957c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_train = bigram_vectorizer.fit_transform(formatted_train_tweets)\n",
    "bigram_test = bigram_vectorizer.transform(formatted_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78587c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gram no string formatting\n",
    "# array([0.55543823, 0.50891089, 0.54221388, 0.51913133, 0.68794326])\n",
    "# 1 and 2-gram, no string formatting\n",
    "# array([0.46118721, 0.45027322, 0.43412527, 0.44141069, 0.61523626])\n",
    "# 1-gram basic string formatting\n",
    "# array([0.57556936, 0.48219736, 0.5530303 , 0.51859504, 0.68586387])\n",
    "# 1 & 2-gram, basic string formatting\n",
    "# array([0.5039019 , 0.41150442, 0.41241685, 0.45823928, 0.62327416])\n",
    "# bigram only, basic string formatting\n",
    "# array([0.24096386, 0.25725095, 0.1682243 , 0.17475728, 0.31060606])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25714b",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "411a5b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m rf_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m500\u001b[39m]\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     13\u001b[0m rf_cv \u001b[38;5;241m=\u001b[39m GridSearchCV(rf, parameters, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrf_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(rf_cv\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=10, max_depth=None, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812152d",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50555ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_parameters = {\n",
    "    'min_samples_split': range(2, 5),\n",
    "    'min_samples_leaf': range(1, 4),\n",
    "    'n_estimators': [50, 100, 500]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(rf, rf_parameters, verbose=3, n_jobs=10)\n",
    "rf_cv.fit(bigram_train, train_df['target'])\n",
    "print(rf_cv.best_params_)\n",
    "\n",
    "# {'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 500}\n",
    "# {'class_weight': 'balanced', 'max_depth': None, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2819",
   "metadata": {},
   "source": [
    "#### Predicting test data\n",
    "\n",
    "As the disaster tweets dataset is from a Kaggle competition, the creators chose to not make the test labels public. As such, we have the random forest model make predictions on the test data. We then submitted this data to Kaggle to get an accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f80cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 1 1]\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.641 total time= 1.3min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.667 total time=  35.2s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.654 total time= 1.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.611 total time= 5.9min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.671 total time=  24.3s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.671 total time=  46.4s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.670 total time= 3.8min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.630 total time=  42.6s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.649 total time=  22.7s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.653 total time=  46.3s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.625 total time= 3.5min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.632 total time=  10.8s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.612 total time=  11.7s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.625 total time=  20.5s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.605 total time= 1.7min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.657 total time= 1.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.616 total time=  36.4s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.614 total time= 1.2min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.658 total time= 5.8min\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.617 total time=  23.5s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.627 total time=  46.8s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.613 total time= 3.8min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.651 total time=  44.9s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.628 total time=  22.1s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.624 total time=  44.1s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.634 total time= 3.5min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.642 total time=  11.4s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.613 total time=  10.7s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.645 total time=  22.3s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.628 total time= 1.7min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.605 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.618 total time=  34.5s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.641 total time= 1.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.599 total time= 5.7min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.632 total time=  21.0s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.635 total time=  45.2s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.652 total time= 3.8min\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.619 total time=  22.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.612 total time=  45.7s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.625 total time=  23.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.609 total time=  46.3s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.616 total time= 3.8min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.621 total time=  19.3s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.643 total time= 1.8min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.642 total time=  37.8s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.658 total time= 6.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.618 total time=  34.3s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.609 total time= 1.1min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.611 total time= 5.7min\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.615 total time= 3.8min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.641 total time=  23.2s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.611 total time= 1.6min\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.618 total time= 1.8min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.609 total time= 1.2min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.642 total time=  36.4s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.619 total time= 1.1min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.641 total time= 5.9min\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.620 total time=  22.5s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.611 total time=  43.7s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.622 total time= 3.6min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.628 total time=  21.1s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.626 total time=  42.1s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.631 total time=  22.2s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.632 total time=  43.0s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.655 total time= 3.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.671 total time=  11.7s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.620 total time=  22.5s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.670 total time= 1.8min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.616 total time=  36.7s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.601 total time= 5.9min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.640 total time=  34.3s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.617 total time= 1.1min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.639 total time= 5.8min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.663 total time=  45.8s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.670 total time=  23.0s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.675 total time=  45.4s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.668 total time= 3.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.673 total time=  23.2s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.626 total time=  10.5s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.649 total time=  22.6s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.626 total time= 1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.610 total time=  36.3s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.618 total time= 5.9min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.621 total time=  32.6s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.640 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.619 total time= 5.6min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.658 total time=  21.5s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.622 total time= 3.6min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.624 total time=  10.2s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.665 total time=  11.8s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.660 total time=  23.0s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.667 total time= 1.9min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.615 total time=  20.1s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.620 total time= 1.4min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.604 total time=  35.5s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.637 total time= 6.2min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.660 total time=  34.4s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.658 total time= 1.1min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.662 total time= 5.7min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.673 total time= 3.8min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.621 total time=  20.2s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.641 total time= 1.8min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.603 total time=  10.3s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.629 total time=  22.1s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.644 total time= 1.4min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.609 total time= 1.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.598 total time=  34.6s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.599 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.618 total time= 5.7min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.653 total time=  22.9s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.649 total time=  48.2s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.634 total time= 3.6min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.655 total time=  23.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.614 total time=  22.3s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.631 total time= 3.5min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.647 total time=  11.3s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.615 total time=   9.7s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.605 total time=  21.0s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.628 total time= 1.7min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.643 total time=  11.1s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.624 total time=  11.8s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.624 total time=  23.6s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.621 total time= 1.3min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.659 total time=  37.1s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.609 total time= 6.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.599 total time=  33.4s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.595 total time= 1.1min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.601 total time= 5.7min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.654 total time= 3.8min\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.622 total time=  11.4s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.621 total time=  21.9s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.621 total time= 1.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.673 total time=  12.1s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.677 total time=  23.8s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.670 total time= 1.3min\n"
     ]
    }
   ],
   "source": [
    "rf_predicted_classes = rf_cv.predict(bigram_test)\n",
    "print(rf_predicted_classes)\n",
    "rf_out_array = []\n",
    "for i, pred_class in enumerate(rf_predicted_classes):\n",
    "    rf_out_array.append([int(test_ids[i]), pred_class])\n",
    "\n",
    "np.savetxt(\"disaster-tweets/rf-results.csv\", rf_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6747b78",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classifier\n",
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "941f0a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Iteration 1, loss = 0.64550188\n",
      "Iteration 2, loss = 0.37110657\n",
      "Iteration 3, loss = 0.16113751\n",
      "Iteration 4, loss = 0.09245533\n",
      "Iteration 5, loss = 0.06919119\n",
      "Iteration 6, loss = 0.05685333\n",
      "Iteration 7, loss = 0.05051970\n",
      "Iteration 8, loss = 0.04652030\n",
      "Iteration 9, loss = 0.04329465\n",
      "Iteration 10, loss = 0.04027972\n",
      "Iteration 11, loss = 0.03963507\n",
      "Iteration 12, loss = 0.03706136\n",
      "Iteration 13, loss = 0.03634189\n",
      "Iteration 14, loss = 0.03578973\n",
      "Iteration 15, loss = 0.03438279\n",
      "Iteration 16, loss = 0.03405455\n",
      "Iteration 17, loss = 0.03368897\n",
      "Iteration 18, loss = 0.03330628\n",
      "Iteration 19, loss = 0.03267945\n",
      "Iteration 20, loss = 0.03235284\n",
      "Iteration 21, loss = 0.03212069\n",
      "Iteration 22, loss = 0.03165786\n",
      "Iteration 23, loss = 0.03103042\n",
      "Iteration 24, loss = 0.03137677\n",
      "Iteration 25, loss = 0.03200844\n",
      "Iteration 26, loss = 0.03082211\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "{'alpha': 0.0001}\n",
      "Iteration 1, loss = 0.65317392\n",
      "Iteration 2, loss = 0.41496627\n",
      "Iteration 3, loss = 0.20253419\n",
      "Iteration 4, loss = 0.11423674\n",
      "Iteration 5, loss = 0.08132673\n",
      "Iteration 6, loss = 0.06560472\n",
      "Iteration 7, loss = 0.05682019\n",
      "Iteration 8, loss = 0.05060551\n",
      "Iteration 9, loss = 0.04640707\n",
      "Iteration 10, loss = 0.04314637\n",
      "Iteration 11, loss = 0.04158098\n",
      "Iteration 12, loss = 0.03941803\n",
      "Iteration 13, loss = 0.03815076\n",
      "Iteration 14, loss = 0.03740764\n",
      "Iteration 15, loss = 0.03639001\n",
      "Iteration 16, loss = 0.03535214\n",
      "Iteration 17, loss = 0.03510264\n",
      "Iteration 18, loss = 0.03463497\n",
      "Iteration 19, loss = 0.03490439\n",
      "Iteration 20, loss = 0.03415402\n",
      "Iteration 21, loss = 0.03361590\n",
      "Iteration 22, loss = 0.03298625\n",
      "Iteration 23, loss = 0.03334798\n",
      "Iteration 24, loss = 0.03269923\n",
      "Iteration 25, loss = 0.03289551\n",
      "Iteration 26, loss = 0.03328964\n",
      "Iteration 27, loss = 0.03245165\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ......................alpha=0.0001;, score=0.638 total time= 2.1min\n",
      "Iteration 1, loss = 0.66046207\n",
      "Iteration 2, loss = 0.42538834\n",
      "Iteration 3, loss = 0.22122301\n",
      "Iteration 4, loss = 0.13974113\n",
      "Iteration 5, loss = 0.10833237\n",
      "Iteration 6, loss = 0.09336862\n",
      "Iteration 7, loss = 0.08565146\n",
      "Iteration 8, loss = 0.07993487\n",
      "Iteration 9, loss = 0.07638670\n",
      "Iteration 10, loss = 0.07270396\n",
      "Iteration 11, loss = 0.06998818\n",
      "Iteration 12, loss = 0.06821153\n",
      "Iteration 13, loss = 0.06685982\n",
      "Iteration 14, loss = 0.06567689\n",
      "Iteration 15, loss = 0.06418436\n",
      "Iteration 16, loss = 0.06291569\n",
      "Iteration 17, loss = 0.06262780\n",
      "Iteration 18, loss = 0.06056730\n",
      "Iteration 19, loss = 0.06171131\n",
      "Iteration 20, loss = 0.05970244\n",
      "Iteration 21, loss = 0.06032510\n",
      "Iteration 22, loss = 0.05907584\n",
      "Iteration 23, loss = 0.05815379\n",
      "Iteration 24, loss = 0.05690870\n",
      "Iteration 25, loss = 0.05783689\n",
      "Iteration 26, loss = 0.05608311\n",
      "Iteration 27, loss = 0.05571830\n",
      "Iteration 28, loss = 0.05689447\n",
      "Iteration 29, loss = 0.05651511\n",
      "Iteration 30, loss = 0.05569270\n",
      "Iteration 31, loss = 0.05479608\n",
      "Iteration 32, loss = 0.05537167\n",
      "Iteration 33, loss = 0.05516342\n",
      "Iteration 34, loss = 0.05644232\n",
      "Iteration 35, loss = 0.05724458\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ........................alpha=0.01;, score=0.630 total time= 2.7min\n",
      "Iteration 1, loss = 0.65221208\n",
      "Iteration 2, loss = 0.41504004\n",
      "Iteration 3, loss = 0.20209081\n",
      "Iteration 4, loss = 0.11705366\n",
      "Iteration 5, loss = 0.08456082\n",
      "Iteration 6, loss = 0.06930862\n",
      "Iteration 7, loss = 0.05990593\n",
      "Iteration 8, loss = 0.05391782\n",
      "Iteration 9, loss = 0.05035345\n",
      "Iteration 10, loss = 0.04682661\n",
      "Iteration 11, loss = 0.04402299\n",
      "Iteration 12, loss = 0.04267937\n",
      "Iteration 13, loss = 0.04162773\n",
      "Iteration 14, loss = 0.04017923\n",
      "Iteration 15, loss = 0.03960293\n",
      "Iteration 16, loss = 0.03828057\n",
      "Iteration 17, loss = 0.03809258\n",
      "Iteration 18, loss = 0.03763343\n",
      "Iteration 19, loss = 0.03695362\n",
      "Iteration 20, loss = 0.03673868\n",
      "Iteration 21, loss = 0.03647125\n",
      "Iteration 22, loss = 0.03626439\n",
      "Iteration 23, loss = 0.03569102\n",
      "Iteration 24, loss = 0.03595346\n",
      "Iteration 25, loss = 0.03562392\n",
      "Iteration 26, loss = 0.03545468\n",
      "Iteration 27, loss = 0.03484960\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .......................alpha=0.001;, score=0.664 total time= 2.1min\n",
      "Iteration 1, loss = 0.64576575\n",
      "Iteration 2, loss = 0.38625242\n",
      "Iteration 3, loss = 0.19782391\n",
      "Iteration 4, loss = 0.12857387\n",
      "Iteration 5, loss = 0.10185237\n",
      "Iteration 6, loss = 0.08847874\n",
      "Iteration 7, loss = 0.08049272\n",
      "Iteration 8, loss = 0.07601593\n",
      "Iteration 9, loss = 0.07243790\n",
      "Iteration 10, loss = 0.07047842\n",
      "Iteration 11, loss = 0.06791775\n",
      "Iteration 12, loss = 0.06533851\n",
      "Iteration 13, loss = 0.06329469\n",
      "Iteration 14, loss = 0.06165398\n",
      "Iteration 15, loss = 0.06133235\n",
      "Iteration 16, loss = 0.05950696\n",
      "Iteration 17, loss = 0.05848685\n",
      "Iteration 18, loss = 0.05747577\n",
      "Iteration 19, loss = 0.05757608\n",
      "Iteration 20, loss = 0.05702606\n",
      "Iteration 21, loss = 0.05757157\n",
      "Iteration 22, loss = 0.05627145\n",
      "Iteration 23, loss = 0.05648833\n",
      "Iteration 24, loss = 0.05495353\n",
      "Iteration 25, loss = 0.05361427\n",
      "Iteration 26, loss = 0.05384618\n",
      "Iteration 27, loss = 0.05394131\n",
      "Iteration 28, loss = 0.05268696\n",
      "Iteration 29, loss = 0.05295758\n",
      "Iteration 30, loss = 0.05350960\n",
      "Iteration 31, loss = 0.05182249\n",
      "Iteration 32, loss = 0.05197436\n",
      "Iteration 33, loss = 0.05122342\n",
      "Iteration 34, loss = 0.05192788\n",
      "Iteration 35, loss = 0.05197503\n",
      "Iteration 36, loss = 0.05102077\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ........................alpha=0.01;, score=0.661 total time= 2.8min\n",
      "Iteration 1, loss = 0.66172333\n",
      "Iteration 2, loss = 0.43237638\n",
      "Iteration 3, loss = 0.21769556\n",
      "Iteration 4, loss = 0.11491747\n",
      "Iteration 5, loss = 0.07820873\n",
      "Iteration 6, loss = 0.06026215\n",
      "Iteration 7, loss = 0.05096366\n",
      "Iteration 8, loss = 0.04432530\n",
      "Iteration 9, loss = 0.04012989\n",
      "Iteration 10, loss = 0.03667077\n",
      "Iteration 11, loss = 0.03454083\n",
      "Iteration 12, loss = 0.03284944\n",
      "Iteration 13, loss = 0.03119575\n",
      "Iteration 14, loss = 0.02996884\n",
      "Iteration 15, loss = 0.02939237\n",
      "Iteration 16, loss = 0.02818640\n",
      "Iteration 17, loss = 0.02763807\n",
      "Iteration 18, loss = 0.02694175\n",
      "Iteration 19, loss = 0.02717640\n",
      "Iteration 20, loss = 0.02585025\n",
      "Iteration 21, loss = 0.02571828\n",
      "Iteration 22, loss = 0.02623989\n",
      "Iteration 23, loss = 0.02512339\n",
      "Iteration 24, loss = 0.02465531\n",
      "Iteration 25, loss = 0.02492296\n",
      "Iteration 26, loss = 0.02427301\n",
      "Iteration 27, loss = 0.02487444\n",
      "Iteration 28, loss = 0.02423897\n",
      "Iteration 29, loss = 0.02457499\n",
      "Iteration 30, loss = 0.02485590\n",
      "Iteration 31, loss = 0.02407482\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ......................alpha=0.0001;, score=0.644 total time= 2.4min\n",
      "Iteration 1, loss = 0.66879439\n",
      "Iteration 2, loss = 0.42250105\n",
      "Iteration 3, loss = 0.22247718\n",
      "Iteration 4, loss = 0.13991050\n",
      "Iteration 5, loss = 0.10763078\n",
      "Iteration 6, loss = 0.09322590\n",
      "Iteration 7, loss = 0.08415975\n",
      "Iteration 8, loss = 0.07916952\n",
      "Iteration 9, loss = 0.07482786\n",
      "Iteration 10, loss = 0.07195289\n",
      "Iteration 11, loss = 0.06968726\n",
      "Iteration 12, loss = 0.06741951\n",
      "Iteration 13, loss = 0.06584164\n",
      "Iteration 14, loss = 0.06561827\n",
      "Iteration 15, loss = 0.06334375\n",
      "Iteration 16, loss = 0.06223303\n",
      "Iteration 17, loss = 0.06099782\n",
      "Iteration 18, loss = 0.06055524\n",
      "Iteration 19, loss = 0.05976191\n",
      "Iteration 20, loss = 0.05961862\n",
      "Iteration 21, loss = 0.05895822\n",
      "Iteration 22, loss = 0.05874212\n",
      "Iteration 23, loss = 0.05780151\n",
      "Iteration 24, loss = 0.05632984\n",
      "Iteration 25, loss = 0.05712668\n",
      "Iteration 26, loss = 0.05495556\n",
      "Iteration 27, loss = 0.05504486\n",
      "Iteration 28, loss = 0.05669137\n",
      "Iteration 29, loss = 0.05558894\n",
      "Iteration 30, loss = 0.05446095\n",
      "Iteration 31, loss = 0.05510448\n",
      "Iteration 32, loss = 0.05578781\n",
      "Iteration 33, loss = 0.05533970\n",
      "Iteration 34, loss = 0.05398461\n",
      "Iteration 35, loss = 0.05359996\n",
      "Iteration 36, loss = 0.05387214\n",
      "Iteration 37, loss = 0.05483705\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ........................alpha=0.01;, score=0.679 total time= 2.8min\n",
      "Iteration 1, loss = 0.67720245\n",
      "Iteration 2, loss = 0.44783308\n",
      "Iteration 3, loss = 0.22982617\n",
      "Iteration 4, loss = 0.12739609\n",
      "Iteration 5, loss = 0.09043628\n",
      "Iteration 6, loss = 0.07228062\n",
      "Iteration 7, loss = 0.06227469\n",
      "Iteration 8, loss = 0.05592714\n",
      "Iteration 9, loss = 0.05123422\n",
      "Iteration 10, loss = 0.04865936\n",
      "Iteration 11, loss = 0.04637561\n",
      "Iteration 12, loss = 0.04455833\n",
      "Iteration 13, loss = 0.04251020\n",
      "Iteration 14, loss = 0.04204837\n",
      "Iteration 15, loss = 0.04057752\n",
      "Iteration 16, loss = 0.04052380\n",
      "Iteration 17, loss = 0.03995718\n",
      "Iteration 18, loss = 0.03952755\n",
      "Iteration 19, loss = 0.03784785\n",
      "Iteration 20, loss = 0.03848225\n",
      "Iteration 21, loss = 0.03767283\n",
      "Iteration 22, loss = 0.03810687\n",
      "Iteration 23, loss = 0.03698384\n",
      "Iteration 24, loss = 0.03711771\n",
      "Iteration 25, loss = 0.03667199\n",
      "Iteration 26, loss = 0.03597571\n",
      "Iteration 27, loss = 0.03636658\n",
      "Iteration 28, loss = 0.03649255\n",
      "Iteration 29, loss = 0.03668753\n",
      "Iteration 30, loss = 0.03531386\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .......................alpha=0.001;, score=0.703 total time= 2.4min\n",
      "Iteration 1, loss = 0.66951963\n",
      "Iteration 2, loss = 0.42314903\n",
      "Iteration 3, loss = 0.21755112\n",
      "Iteration 4, loss = 0.13375225\n",
      "Iteration 5, loss = 0.10150572\n",
      "Iteration 6, loss = 0.08730009\n",
      "Iteration 7, loss = 0.07855028\n",
      "Iteration 8, loss = 0.07300919\n",
      "Iteration 9, loss = 0.06871895\n",
      "Iteration 10, loss = 0.06666191\n",
      "Iteration 11, loss = 0.06387116\n",
      "Iteration 12, loss = 0.06190092\n",
      "Iteration 13, loss = 0.06240723\n",
      "Iteration 14, loss = 0.05957506\n",
      "Iteration 15, loss = 0.05854504\n",
      "Iteration 16, loss = 0.05662449\n",
      "Iteration 17, loss = 0.05617187\n",
      "Iteration 18, loss = 0.05597276\n",
      "Iteration 19, loss = 0.05497089\n",
      "Iteration 20, loss = 0.05337786\n",
      "Iteration 21, loss = 0.05436452\n",
      "Iteration 22, loss = 0.05266437\n",
      "Iteration 23, loss = 0.05261219\n",
      "Iteration 24, loss = 0.05241489\n",
      "Iteration 25, loss = 0.05082624\n",
      "Iteration 26, loss = 0.05226766\n",
      "Iteration 27, loss = 0.05077899\n",
      "Iteration 28, loss = 0.05098616\n",
      "Iteration 29, loss = 0.05052464\n",
      "Iteration 30, loss = 0.05007631\n",
      "Iteration 31, loss = 0.05064161\n",
      "Iteration 32, loss = 0.05043393\n",
      "Iteration 33, loss = 0.05149201\n",
      "Iteration 34, loss = 0.04828600\n",
      "Iteration 35, loss = 0.04970748\n",
      "Iteration 36, loss = 0.04936251\n",
      "Iteration 37, loss = 0.04963945\n",
      "Iteration 38, loss = 0.04803425\n",
      "Iteration 39, loss = 0.04945204\n",
      "Iteration 40, loss = 0.04838654\n",
      "Iteration 41, loss = 0.04849394\n",
      "Iteration 42, loss = 0.04734651\n",
      "Iteration 43, loss = 0.04769495\n",
      "Iteration 44, loss = 0.04880685\n",
      "Iteration 45, loss = 0.04836132\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ........................alpha=0.01;, score=0.639 total time= 3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65722038\n",
      "Iteration 2, loss = 0.43626507\n",
      "Iteration 3, loss = 0.21774106\n",
      "Iteration 4, loss = 0.12112475\n",
      "Iteration 5, loss = 0.08493901\n",
      "Iteration 6, loss = 0.06897639\n",
      "Iteration 7, loss = 0.05892004\n",
      "Iteration 8, loss = 0.05208061\n",
      "Iteration 9, loss = 0.04809691\n",
      "Iteration 10, loss = 0.04520381\n",
      "Iteration 11, loss = 0.04264817\n",
      "Iteration 12, loss = 0.04104417\n",
      "Iteration 13, loss = 0.03928738\n",
      "Iteration 14, loss = 0.03786395\n",
      "Iteration 15, loss = 0.03649119\n",
      "Iteration 16, loss = 0.03614867\n",
      "Iteration 17, loss = 0.03583490\n",
      "Iteration 18, loss = 0.03624316\n",
      "Iteration 19, loss = 0.03435628\n",
      "Iteration 20, loss = 0.03388058\n",
      "Iteration 21, loss = 0.03388917\n",
      "Iteration 22, loss = 0.03327046\n",
      "Iteration 23, loss = 0.03332787\n",
      "Iteration 24, loss = 0.03297975\n",
      "Iteration 25, loss = 0.03287427\n",
      "Iteration 26, loss = 0.03236315\n",
      "Iteration 27, loss = 0.03217436\n",
      "Iteration 28, loss = 0.03156761\n",
      "Iteration 29, loss = 0.03180140\n",
      "Iteration 30, loss = 0.03212690\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ......................alpha=0.0001;, score=0.712 total time= 2.4min\n",
      "Iteration 1, loss = 0.64938168\n",
      "Iteration 2, loss = 0.40306638\n",
      "Iteration 3, loss = 0.20994945\n",
      "Iteration 4, loss = 0.13256145\n",
      "Iteration 5, loss = 0.10304767\n",
      "Iteration 6, loss = 0.08818630\n",
      "Iteration 7, loss = 0.07824522\n",
      "Iteration 8, loss = 0.07357071\n",
      "Iteration 9, loss = 0.06932397\n",
      "Iteration 10, loss = 0.06577380\n",
      "Iteration 11, loss = 0.06362716\n",
      "Iteration 12, loss = 0.06169846\n",
      "Iteration 13, loss = 0.05920234\n",
      "Iteration 14, loss = 0.05928878\n",
      "Iteration 15, loss = 0.05635870\n",
      "Iteration 16, loss = 0.05550996\n",
      "Iteration 17, loss = 0.05534489\n",
      "Iteration 18, loss = 0.05338877\n",
      "Iteration 19, loss = 0.05338197\n",
      "Iteration 20, loss = 0.05235159\n",
      "Iteration 21, loss = 0.05194279\n",
      "Iteration 22, loss = 0.05026898\n",
      "Iteration 23, loss = 0.05031123\n",
      "Iteration 24, loss = 0.04994820\n",
      "Iteration 25, loss = 0.04995533\n",
      "Iteration 26, loss = 0.04852336\n",
      "Iteration 27, loss = 0.04920398\n",
      "Iteration 28, loss = 0.04808030\n",
      "Iteration 29, loss = 0.04770749\n",
      "Iteration 30, loss = 0.04778935\n",
      "Iteration 31, loss = 0.04689205\n",
      "Iteration 32, loss = 0.04771159\n",
      "Iteration 33, loss = 0.04683653\n",
      "Iteration 34, loss = 0.04705541\n",
      "Iteration 35, loss = 0.04581633\n",
      "Iteration 36, loss = 0.04649624\n",
      "Iteration 37, loss = 0.04496807\n",
      "Iteration 38, loss = 0.04487257\n",
      "Iteration 39, loss = 0.04527693\n",
      "Iteration 40, loss = 0.04492736\n",
      "Iteration 41, loss = 0.04616491\n",
      "Iteration 42, loss = 0.04501146\n",
      "Iteration 43, loss = 0.04453554\n",
      "Iteration 44, loss = 0.04437157\n",
      "Iteration 45, loss = 0.04431739\n",
      "Iteration 46, loss = 0.04394727\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ........................alpha=0.01;, score=0.626 total time= 3.4min\n",
      "Iteration 1, loss = 0.65846491\n",
      "Iteration 2, loss = 0.41971797\n",
      "Iteration 3, loss = 0.20358946\n",
      "Iteration 4, loss = 0.11490802\n",
      "Iteration 5, loss = 0.07955809\n",
      "Iteration 6, loss = 0.06377862\n",
      "Iteration 7, loss = 0.05389268\n",
      "Iteration 8, loss = 0.04728210\n",
      "Iteration 9, loss = 0.04339373\n",
      "Iteration 10, loss = 0.04085705\n",
      "Iteration 11, loss = 0.03774066\n",
      "Iteration 12, loss = 0.03607751\n",
      "Iteration 13, loss = 0.03490893\n",
      "Iteration 14, loss = 0.03346366\n",
      "Iteration 15, loss = 0.03264543\n",
      "Iteration 16, loss = 0.03255518\n",
      "Iteration 17, loss = 0.03120296\n",
      "Iteration 18, loss = 0.03118674\n",
      "Iteration 19, loss = 0.03012506\n",
      "Iteration 20, loss = 0.03008262\n",
      "Iteration 21, loss = 0.02973851\n",
      "Iteration 22, loss = 0.02852052\n",
      "Iteration 23, loss = 0.02872994\n",
      "Iteration 24, loss = 0.02898247\n",
      "Iteration 25, loss = 0.02839248\n",
      "Iteration 26, loss = 0.02856783\n",
      "Iteration 27, loss = 0.02828055\n",
      "Iteration 28, loss = 0.02833636\n",
      "Iteration 29, loss = 0.02804044\n",
      "Iteration 30, loss = 0.02869530\n",
      "Iteration 31, loss = 0.02819906\n",
      "Iteration 32, loss = 0.02817668\n",
      "Iteration 33, loss = 0.02787702\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .......................alpha=0.001;, score=0.635 total time= 2.6min\n",
      "Iteration 1, loss = 0.67095575\n",
      "Iteration 2, loss = 0.44364881\n",
      "Iteration 3, loss = 0.29825083\n",
      "Iteration 4, loss = 0.23914736\n",
      "Iteration 5, loss = 0.21339967\n",
      "Iteration 6, loss = 0.19797950\n",
      "Iteration 7, loss = 0.18793681\n",
      "Iteration 8, loss = 0.17864630\n",
      "Iteration 9, loss = 0.17332453\n",
      "Iteration 10, loss = 0.16736739\n",
      "Iteration 11, loss = 0.16248447\n",
      "Iteration 12, loss = 0.15834361\n",
      "Iteration 13, loss = 0.15484538\n",
      "Iteration 14, loss = 0.15382777\n",
      "Iteration 15, loss = 0.15172546\n",
      "Iteration 16, loss = 0.15220454\n",
      "Iteration 17, loss = 0.14869894\n",
      "Iteration 18, loss = 0.14730537\n",
      "Iteration 19, loss = 0.14539576\n",
      "Iteration 20, loss = 0.14596486\n",
      "Iteration 21, loss = 0.14241584\n",
      "Iteration 22, loss = 0.14096514\n",
      "Iteration 23, loss = 0.14089438\n",
      "Iteration 24, loss = 0.13907728\n",
      "Iteration 25, loss = 0.14225488\n",
      "Iteration 26, loss = 0.14112245\n",
      "Iteration 27, loss = 0.13793690\n",
      "Iteration 28, loss = 0.13703242\n",
      "Iteration 29, loss = 0.13689491\n",
      "Iteration 30, loss = 0.13634579\n",
      "Iteration 31, loss = 0.13525885\n",
      "Iteration 32, loss = 0.13516179\n",
      "Iteration 33, loss = 0.13449982\n",
      "Iteration 34, loss = 0.13436995\n",
      "Iteration 35, loss = 0.13346252\n",
      "Iteration 36, loss = 0.12995341\n",
      "Iteration 37, loss = 0.13371832\n",
      "Iteration 38, loss = 0.13395535\n",
      "Iteration 39, loss = 0.13284362\n",
      "Iteration 40, loss = 0.13418197\n",
      "Iteration 41, loss = 0.13171243\n",
      "Iteration 42, loss = 0.13196043\n",
      "Iteration 43, loss = 0.13400483\n",
      "Iteration 44, loss = 0.13162239\n",
      "Iteration 45, loss = 0.12793855\n",
      "Iteration 46, loss = 0.12947111\n",
      "Iteration 47, loss = 0.12939962\n",
      "Iteration 48, loss = 0.12733732\n",
      "Iteration 49, loss = 0.12953224\n",
      "Iteration 50, loss = 0.13010372\n",
      "Iteration 51, loss = 0.13109341\n",
      "Iteration 52, loss = 0.12966113\n",
      "Iteration 53, loss = 0.13050591\n",
      "Iteration 54, loss = 0.12832275\n",
      "Iteration 55, loss = 0.12936669\n",
      "Iteration 56, loss = 0.12697777\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .........................alpha=0.1;, score=0.659 total time= 3.8min\n",
      "Iteration 1, loss = 0.65236097\n",
      "Iteration 2, loss = 0.41008488\n",
      "Iteration 3, loss = 0.19994527\n",
      "Iteration 4, loss = 0.11642592\n",
      "Iteration 5, loss = 0.08408810\n",
      "Iteration 6, loss = 0.06925653\n",
      "Iteration 7, loss = 0.05965191\n",
      "Iteration 8, loss = 0.05429208\n",
      "Iteration 9, loss = 0.04997928\n",
      "Iteration 10, loss = 0.04791088\n",
      "Iteration 11, loss = 0.04640971\n",
      "Iteration 12, loss = 0.04412154\n",
      "Iteration 13, loss = 0.04227866\n",
      "Iteration 14, loss = 0.04111040\n",
      "Iteration 15, loss = 0.04060377\n",
      "Iteration 16, loss = 0.04006780\n",
      "Iteration 17, loss = 0.03980595\n",
      "Iteration 18, loss = 0.03878941\n",
      "Iteration 19, loss = 0.03905939\n",
      "Iteration 20, loss = 0.03877897\n",
      "Iteration 21, loss = 0.03748823\n",
      "Iteration 22, loss = 0.03825623\n",
      "Iteration 23, loss = 0.03740967\n",
      "Iteration 24, loss = 0.03719190\n",
      "Iteration 25, loss = 0.03675721\n",
      "Iteration 26, loss = 0.03752014\n",
      "Iteration 27, loss = 0.03673683\n",
      "Iteration 28, loss = 0.03702455\n",
      "Iteration 29, loss = 0.03566867\n",
      "Iteration 30, loss = 0.03602015\n",
      "Iteration 31, loss = 0.03560090\n",
      "Iteration 32, loss = 0.03574855\n",
      "Iteration 33, loss = 0.03609886\n",
      "Iteration 34, loss = 0.03569791\n",
      "Iteration 35, loss = 0.03630148\n",
      "Iteration 36, loss = 0.03601188\n",
      "Iteration 37, loss = 0.03582914\n",
      "Iteration 38, loss = 0.03526301\n",
      "Iteration 39, loss = 0.03494211\n",
      "Iteration 40, loss = 0.03571865\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .......................alpha=0.001;, score=0.638 total time= 3.1min\n",
      "Iteration 1, loss = 0.66984642\n",
      "Iteration 2, loss = 0.46235071\n",
      "Iteration 3, loss = 0.31088969\n",
      "Iteration 4, loss = 0.24929398\n",
      "Iteration 5, loss = 0.22272905\n",
      "Iteration 6, loss = 0.20637693\n",
      "Iteration 7, loss = 0.19413867\n",
      "Iteration 8, loss = 0.18679620\n",
      "Iteration 9, loss = 0.18006836\n",
      "Iteration 10, loss = 0.17535918\n",
      "Iteration 11, loss = 0.17073037\n",
      "Iteration 12, loss = 0.16537158\n",
      "Iteration 13, loss = 0.16241506\n",
      "Iteration 14, loss = 0.15796367\n",
      "Iteration 15, loss = 0.15592245\n",
      "Iteration 16, loss = 0.15491169\n",
      "Iteration 17, loss = 0.15198673\n",
      "Iteration 18, loss = 0.15040240\n",
      "Iteration 19, loss = 0.15171750\n",
      "Iteration 20, loss = 0.15015886\n",
      "Iteration 21, loss = 0.14842596\n",
      "Iteration 22, loss = 0.14891544\n",
      "Iteration 23, loss = 0.14765622\n",
      "Iteration 24, loss = 0.14327214\n",
      "Iteration 25, loss = 0.14388196\n",
      "Iteration 26, loss = 0.14284347\n",
      "Iteration 27, loss = 0.14380138\n",
      "Iteration 28, loss = 0.14298236\n",
      "Iteration 29, loss = 0.14124153\n",
      "Iteration 30, loss = 0.13958533\n",
      "Iteration 31, loss = 0.13928291\n",
      "Iteration 32, loss = 0.14000755\n",
      "Iteration 33, loss = 0.14078459\n",
      "Iteration 34, loss = 0.14036174\n",
      "Iteration 35, loss = 0.13942458\n",
      "Iteration 36, loss = 0.14042699\n",
      "Iteration 37, loss = 0.13874521\n",
      "Iteration 38, loss = 0.13401112\n",
      "Iteration 39, loss = 0.13813767\n",
      "Iteration 40, loss = 0.13647138\n",
      "Iteration 41, loss = 0.13335173\n",
      "Iteration 42, loss = 0.13381491\n",
      "Iteration 43, loss = 0.13455379\n",
      "Iteration 44, loss = 0.13523491\n",
      "Iteration 45, loss = 0.13609919\n",
      "Iteration 46, loss = 0.13544082\n",
      "Iteration 47, loss = 0.13428615\n",
      "Iteration 48, loss = 0.13276931\n",
      "Iteration 49, loss = 0.13319550\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.677 total time= 3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65830580\n",
      "Iteration 2, loss = 0.43244254\n",
      "Iteration 3, loss = 0.21163385\n",
      "Iteration 4, loss = 0.12001160\n",
      "Iteration 5, loss = 0.08455803\n",
      "Iteration 6, loss = 0.06679121\n",
      "Iteration 7, loss = 0.05698308\n",
      "Iteration 8, loss = 0.05125013\n",
      "Iteration 9, loss = 0.04681886\n",
      "Iteration 10, loss = 0.04464012\n",
      "Iteration 11, loss = 0.04103793\n",
      "Iteration 12, loss = 0.03952796\n",
      "Iteration 13, loss = 0.03831575\n",
      "Iteration 14, loss = 0.03781712\n",
      "Iteration 15, loss = 0.03633931\n",
      "Iteration 16, loss = 0.03540449\n",
      "Iteration 17, loss = 0.03477819\n",
      "Iteration 18, loss = 0.03486516\n",
      "Iteration 19, loss = 0.03428989\n",
      "Iteration 20, loss = 0.03414960\n",
      "Iteration 21, loss = 0.03372396\n",
      "Iteration 22, loss = 0.03367372\n",
      "Iteration 23, loss = 0.03248387\n",
      "Iteration 24, loss = 0.03285296\n",
      "Iteration 25, loss = 0.03306433\n",
      "Iteration 26, loss = 0.03244188\n",
      "Iteration 27, loss = 0.03171769\n",
      "Iteration 28, loss = 0.03188833\n",
      "Iteration 29, loss = 0.03160686\n",
      "Iteration 30, loss = 0.03152192\n",
      "Iteration 31, loss = 0.03097679\n",
      "Iteration 32, loss = 0.03131951\n",
      "Iteration 33, loss = 0.03110688\n",
      "Iteration 34, loss = 0.03182258\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .......................alpha=0.001;, score=0.660 total time= 2.7min\n",
      "Iteration 1, loss = 0.66139602\n",
      "Iteration 2, loss = 0.44370640\n",
      "Iteration 3, loss = 0.30439942\n",
      "Iteration 4, loss = 0.24661643\n",
      "Iteration 5, loss = 0.22104643\n",
      "Iteration 6, loss = 0.20450759\n",
      "Iteration 7, loss = 0.19300632\n",
      "Iteration 8, loss = 0.18511814\n",
      "Iteration 9, loss = 0.17751260\n",
      "Iteration 10, loss = 0.17150875\n",
      "Iteration 11, loss = 0.16822318\n",
      "Iteration 12, loss = 0.16588978\n",
      "Iteration 13, loss = 0.16044312\n",
      "Iteration 14, loss = 0.15650656\n",
      "Iteration 15, loss = 0.15346822\n",
      "Iteration 16, loss = 0.15202769\n",
      "Iteration 17, loss = 0.15049294\n",
      "Iteration 18, loss = 0.15123070\n",
      "Iteration 19, loss = 0.14908797\n",
      "Iteration 20, loss = 0.14529991\n",
      "Iteration 21, loss = 0.14315703\n",
      "Iteration 22, loss = 0.14434887\n",
      "Iteration 23, loss = 0.14248607\n",
      "Iteration 24, loss = 0.14179419\n",
      "Iteration 25, loss = 0.14114292\n",
      "Iteration 26, loss = 0.14186308\n",
      "Iteration 27, loss = 0.14018736\n",
      "Iteration 28, loss = 0.14203162\n",
      "Iteration 29, loss = 0.13869709\n",
      "Iteration 30, loss = 0.13883300\n",
      "Iteration 31, loss = 0.13795202\n",
      "Iteration 32, loss = 0.13877023\n",
      "Iteration 33, loss = 0.13720576\n",
      "Iteration 34, loss = 0.13820859\n",
      "Iteration 35, loss = 0.13503876\n",
      "Iteration 36, loss = 0.13621634\n",
      "Iteration 37, loss = 0.13466939\n",
      "Iteration 38, loss = 0.13517486\n",
      "Iteration 39, loss = 0.13827033\n",
      "Iteration 40, loss = 0.13502714\n",
      "Iteration 41, loss = 0.13345059\n",
      "Iteration 42, loss = 0.13403879\n",
      "Iteration 43, loss = 0.13461279\n",
      "Iteration 44, loss = 0.13514259\n",
      "Iteration 45, loss = 0.13387166\n",
      "Iteration 46, loss = 0.12974034\n",
      "Iteration 47, loss = 0.13036666\n",
      "Iteration 48, loss = 0.13258544\n",
      "Iteration 49, loss = 0.13080208\n",
      "Iteration 50, loss = 0.13345740\n",
      "Iteration 51, loss = 0.13215487\n",
      "Iteration 52, loss = 0.12978043\n",
      "Iteration 53, loss = 0.13032926\n",
      "Iteration 54, loss = 0.12961463\n",
      "Iteration 55, loss = 0.13193268\n",
      "Iteration 56, loss = 0.13271290\n",
      "Iteration 57, loss = 0.13096821\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.631 total time= 3.8min\n",
      "Iteration 1, loss = 0.66717984\n",
      "Iteration 2, loss = 0.43866320\n",
      "Iteration 3, loss = 0.23196219\n",
      "Iteration 4, loss = 0.12578751\n",
      "Iteration 5, loss = 0.08643313\n",
      "Iteration 6, loss = 0.06790210\n",
      "Iteration 7, loss = 0.05817036\n",
      "Iteration 8, loss = 0.05063194\n",
      "Iteration 9, loss = 0.04721984\n",
      "Iteration 10, loss = 0.04325857\n",
      "Iteration 11, loss = 0.04126689\n",
      "Iteration 12, loss = 0.03884941\n",
      "Iteration 13, loss = 0.03785002\n",
      "Iteration 14, loss = 0.03662959\n",
      "Iteration 15, loss = 0.03620890\n",
      "Iteration 16, loss = 0.03475793\n",
      "Iteration 17, loss = 0.03416316\n",
      "Iteration 18, loss = 0.03339191\n",
      "Iteration 19, loss = 0.03280232\n",
      "Iteration 20, loss = 0.03260475\n",
      "Iteration 21, loss = 0.03183084\n",
      "Iteration 22, loss = 0.03178784\n",
      "Iteration 23, loss = 0.03186239\n",
      "Iteration 24, loss = 0.03108505\n",
      "Iteration 25, loss = 0.03129847\n",
      "Iteration 26, loss = 0.03093098\n",
      "Iteration 27, loss = 0.02989758\n",
      "Iteration 28, loss = 0.03014692\n",
      "Iteration 29, loss = 0.03032393\n",
      "Iteration 30, loss = 0.02996979\n",
      "Iteration 31, loss = 0.02987744\n",
      "Iteration 32, loss = 0.03036695\n",
      "Iteration 33, loss = 0.02982088\n",
      "Iteration 34, loss = 0.02975403\n",
      "Iteration 35, loss = 0.02998942\n",
      "Iteration 36, loss = 0.02939726\n",
      "Iteration 37, loss = 0.02952386\n",
      "Iteration 38, loss = 0.02949022\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ......................alpha=0.0001;, score=0.668 total time= 3.0min\n",
      "Iteration 1, loss = 0.66437975\n",
      "Iteration 2, loss = 0.44810657\n",
      "Iteration 3, loss = 0.30051576\n",
      "Iteration 4, loss = 0.24288594\n",
      "Iteration 5, loss = 0.21541707\n",
      "Iteration 6, loss = 0.19940906\n",
      "Iteration 7, loss = 0.18845642\n",
      "Iteration 8, loss = 0.17949962\n",
      "Iteration 9, loss = 0.17185560\n",
      "Iteration 10, loss = 0.16713878\n",
      "Iteration 11, loss = 0.16320314\n",
      "Iteration 12, loss = 0.15915306\n",
      "Iteration 13, loss = 0.15664545\n",
      "Iteration 14, loss = 0.15348729\n",
      "Iteration 15, loss = 0.15209800\n",
      "Iteration 16, loss = 0.14867155\n",
      "Iteration 17, loss = 0.14606332\n",
      "Iteration 18, loss = 0.14544374\n",
      "Iteration 19, loss = 0.14382951\n",
      "Iteration 20, loss = 0.14278827\n",
      "Iteration 21, loss = 0.14356000\n",
      "Iteration 22, loss = 0.14213916\n",
      "Iteration 23, loss = 0.14158546\n",
      "Iteration 24, loss = 0.13765319\n",
      "Iteration 25, loss = 0.13735062\n",
      "Iteration 26, loss = 0.13715940\n",
      "Iteration 27, loss = 0.13801017\n",
      "Iteration 28, loss = 0.13542999\n",
      "Iteration 29, loss = 0.13397953\n",
      "Iteration 30, loss = 0.13549784\n",
      "Iteration 31, loss = 0.13474644\n",
      "Iteration 32, loss = 0.13492282\n",
      "Iteration 33, loss = 0.13204963\n",
      "Iteration 34, loss = 0.13224741\n",
      "Iteration 35, loss = 0.13144070\n",
      "Iteration 36, loss = 0.13209747\n",
      "Iteration 37, loss = 0.13164231\n",
      "Iteration 38, loss = 0.12873006\n",
      "Iteration 39, loss = 0.13002886\n",
      "Iteration 40, loss = 0.12964712\n",
      "Iteration 41, loss = 0.13242529\n",
      "Iteration 42, loss = 0.13043880\n",
      "Iteration 43, loss = 0.13212590\n",
      "Iteration 44, loss = 0.12785059\n",
      "Iteration 45, loss = 0.12949311\n",
      "Iteration 46, loss = 0.12883677\n",
      "Iteration 47, loss = 0.12998417\n",
      "Iteration 48, loss = 0.12789923\n",
      "Iteration 49, loss = 0.12579163\n",
      "Iteration 50, loss = 0.12937021\n",
      "Iteration 51, loss = 0.12537637\n",
      "Iteration 52, loss = 0.12910574\n",
      "Iteration 53, loss = 0.12530868\n",
      "Iteration 54, loss = 0.12764147\n",
      "Iteration 55, loss = 0.12670751\n",
      "Iteration 56, loss = 0.12390713\n",
      "Iteration 57, loss = 0.12599874\n",
      "Iteration 58, loss = 0.12111373\n",
      "Iteration 59, loss = 0.12278413\n",
      "Iteration 60, loss = 0.12547508\n",
      "Iteration 61, loss = 0.12508865\n",
      "Iteration 62, loss = 0.12439514\n",
      "Iteration 63, loss = 0.12525848\n",
      "Iteration 64, loss = 0.12348661\n",
      "Iteration 65, loss = 0.12244130\n",
      "Iteration 66, loss = 0.12467769\n",
      "Iteration 67, loss = 0.12364194\n",
      "Iteration 68, loss = 0.12481927\n",
      "Iteration 69, loss = 0.12623630\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.642 total time= 4.0min\n",
      "Iteration 1, loss = 0.67495284\n",
      "Iteration 2, loss = 0.44057935\n",
      "Iteration 3, loss = 0.22880554\n",
      "Iteration 4, loss = 0.11863795\n",
      "Iteration 5, loss = 0.08108055\n",
      "Iteration 6, loss = 0.06367253\n",
      "Iteration 7, loss = 0.05329246\n",
      "Iteration 8, loss = 0.04668989\n",
      "Iteration 9, loss = 0.04237924\n",
      "Iteration 10, loss = 0.03908798\n",
      "Iteration 11, loss = 0.03714621\n",
      "Iteration 12, loss = 0.03552001\n",
      "Iteration 13, loss = 0.03370596\n",
      "Iteration 14, loss = 0.03310953\n",
      "Iteration 15, loss = 0.03185094\n",
      "Iteration 16, loss = 0.03115928\n",
      "Iteration 17, loss = 0.03094331\n",
      "Iteration 18, loss = 0.02981307\n",
      "Iteration 19, loss = 0.02965588\n",
      "Iteration 20, loss = 0.02949151\n",
      "Iteration 21, loss = 0.02928492\n",
      "Iteration 22, loss = 0.02921655\n",
      "Iteration 23, loss = 0.02925486\n",
      "Iteration 24, loss = 0.02765305\n",
      "Iteration 25, loss = 0.02828510\n",
      "Iteration 26, loss = 0.02755385\n",
      "Iteration 27, loss = 0.02813084\n",
      "Iteration 28, loss = 0.02856527\n",
      "Iteration 29, loss = 0.02804729\n",
      "Iteration 30, loss = 0.02765062\n",
      "Iteration 31, loss = 0.02684916\n",
      "Iteration 32, loss = 0.02721486\n",
      "Iteration 33, loss = 0.02697769\n",
      "Iteration 34, loss = 0.02662105\n",
      "Iteration 35, loss = 0.02645728\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ......................alpha=0.0001;, score=0.664 total time= 2.7min\n",
      "Iteration 1, loss = 0.67673172\n",
      "Iteration 2, loss = 0.46800745\n",
      "Iteration 3, loss = 0.31140035\n",
      "Iteration 4, loss = 0.24444527\n",
      "Iteration 5, loss = 0.21568595\n",
      "Iteration 6, loss = 0.19678194\n",
      "Iteration 7, loss = 0.18410424\n",
      "Iteration 8, loss = 0.17617901\n",
      "Iteration 9, loss = 0.16884526\n",
      "Iteration 10, loss = 0.16393161\n",
      "Iteration 11, loss = 0.15764720\n",
      "Iteration 12, loss = 0.15299620\n",
      "Iteration 13, loss = 0.14985916\n",
      "Iteration 14, loss = 0.14637564\n",
      "Iteration 15, loss = 0.14535891\n",
      "Iteration 16, loss = 0.14382300\n",
      "Iteration 17, loss = 0.14027266\n",
      "Iteration 18, loss = 0.14011346\n",
      "Iteration 19, loss = 0.13719422\n",
      "Iteration 20, loss = 0.13611934\n",
      "Iteration 21, loss = 0.13374214\n",
      "Iteration 22, loss = 0.13222693\n",
      "Iteration 23, loss = 0.13061625\n",
      "Iteration 24, loss = 0.13139100\n",
      "Iteration 25, loss = 0.12993268\n",
      "Iteration 26, loss = 0.13093411\n",
      "Iteration 27, loss = 0.13013044\n",
      "Iteration 28, loss = 0.13076636\n",
      "Iteration 29, loss = 0.12813513\n",
      "Iteration 30, loss = 0.12743426\n",
      "Iteration 31, loss = 0.12744515\n",
      "Iteration 32, loss = 0.12624557\n",
      "Iteration 33, loss = 0.12543505\n",
      "Iteration 34, loss = 0.12502725\n",
      "Iteration 35, loss = 0.12537802\n",
      "Iteration 36, loss = 0.12329962\n",
      "Iteration 37, loss = 0.12496308\n",
      "Iteration 38, loss = 0.12486191\n",
      "Iteration 39, loss = 0.12490725\n",
      "Iteration 40, loss = 0.12803006\n",
      "Iteration 41, loss = 0.12621816\n",
      "Iteration 42, loss = 0.12428786\n",
      "Iteration 43, loss = 0.12924919\n",
      "Iteration 44, loss = 0.12228202\n",
      "Iteration 45, loss = 0.12032001\n",
      "Iteration 46, loss = 0.12232871\n",
      "Iteration 47, loss = 0.12073736\n",
      "Iteration 48, loss = 0.12174429\n",
      "Iteration 49, loss = 0.12083305\n",
      "Iteration 50, loss = 0.11888996\n",
      "Iteration 51, loss = 0.11996531\n",
      "Iteration 52, loss = 0.11955364\n",
      "Iteration 53, loss = 0.11793848\n",
      "Iteration 54, loss = 0.12059467\n",
      "Iteration 55, loss = 0.11817430\n",
      "Iteration 56, loss = 0.11641049\n",
      "Iteration 57, loss = 0.11799932\n",
      "Iteration 58, loss = 0.11889867\n",
      "Iteration 59, loss = 0.12490704\n",
      "Iteration 60, loss = 0.11937767\n",
      "Iteration 61, loss = 0.12027085\n",
      "Iteration 62, loss = 0.12324339\n",
      "Iteration 63, loss = 0.12041221\n",
      "Iteration 64, loss = 0.12100144\n",
      "Iteration 65, loss = 0.11531591\n",
      "Iteration 66, loss = 0.11697730\n",
      "Iteration 67, loss = 0.11805728\n",
      "Iteration 68, loss = 0.11751331\n",
      "Iteration 69, loss = 0.11646189\n",
      "Iteration 70, loss = 0.11684752\n",
      "Iteration 71, loss = 0.11629812\n",
      "Iteration 72, loss = 0.11786632\n",
      "Iteration 73, loss = 0.11527536\n",
      "Iteration 74, loss = 0.11425047\n",
      "Iteration 75, loss = 0.11424418\n",
      "Iteration 76, loss = 0.11907825\n",
      "Iteration 77, loss = 0.11680083\n",
      "Iteration 78, loss = 0.11811563\n",
      "Iteration 79, loss = 0.11762561\n",
      "Iteration 80, loss = 0.11574388\n",
      "Iteration 81, loss = 0.11524541\n",
      "Iteration 82, loss = 0.11442230\n",
      "Iteration 83, loss = 0.11647140\n",
      "Iteration 84, loss = 0.11544657\n",
      "Iteration 85, loss = 0.11417997\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .........................alpha=0.1;, score=0.627 total time= 4.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = MLPClassifier(verbose=True, tol=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53246529",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_parameters = {\n",
    "    \"alpha\": [.0001, .001, .01, .1]\n",
    "}\n",
    "mlpc_cv = GridSearchCV(mlpc, mlpc_parameters, verbose=3, n_jobs=-1)\n",
    "mlpc_cv.fit(bigram_train, train_df['target'])\n",
    "print(mlpc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a7f07",
   "metadata": {},
   "source": [
    "#### Predicting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "602abec4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 60424 features, but MLPClassifier is expecting 70186 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlpc_predicted_classes \u001b[38;5;241m=\u001b[39m \u001b[43mmlpc_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbigram_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m mlpc_out_array \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pred_class \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(mlpc_predicted_classes):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/utils/metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:521\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \n\u001b[1;32m    505\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    520\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1167\u001b[0m, in \u001b[0;36mMLPClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m\"\"\"Predict using the multi-layer perceptron classifier.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \n\u001b[1;32m   1156\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;124;03m    The predicted classes.\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1167\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_pass_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1170\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mravel()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:159\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._forward_pass_fast\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_pass_fast\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124;03m\"\"\"Predict using the trained model\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    This is the same as _forward_pass but does not record the activations\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m        The decision function of the samples for each class in the model.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# Initialize first layer\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     activation \u001b[38;5;241m=\u001b[39m X\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 60424 features, but MLPClassifier is expecting 70186 features as input."
     ]
    }
   ],
   "source": [
    "mlpc_predicted_classes = mlpc_cv.predict(bigram_test)\n",
    "mlpc_out_array = []\n",
    "for i, pred_class in enumerate(mlpc_predicted_classes):\n",
    "    mlpc_out_array.append([int(test_ids[i]), pred_class])\n",
    "    \n",
    "np.savetxt(\"disaster-tweets/mlpc-results.csv\", mlpc_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18e351",
   "metadata": {},
   "source": [
    "#### Retrying with tri-grams\n",
    "\n",
    "Due to undesirable low accuracy, we tried to train a multilayer perceptron classifier again, this time with tweet data represented as tri-grams rather than bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50ab3e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Iteration 1, loss = 0.68355896\n",
      "Iteration 2, loss = 0.48614873\n",
      "Iteration 3, loss = 0.34652288\n",
      "Iteration 4, loss = 0.28740941\n",
      "Iteration 5, loss = 0.26029323\n",
      "Iteration 6, loss = 0.24369238\n",
      "Iteration 7, loss = 0.23209767\n",
      "Iteration 8, loss = 0.22412241\n",
      "Iteration 9, loss = 0.21758061\n",
      "Iteration 10, loss = 0.21094973\n",
      "Iteration 11, loss = 0.20776511\n",
      "Iteration 12, loss = 0.20285762\n",
      "Iteration 13, loss = 0.19865968\n",
      "Iteration 14, loss = 0.19885302\n",
      "Iteration 15, loss = 0.19540739\n",
      "Iteration 16, loss = 0.19169593\n",
      "Iteration 17, loss = 0.18792039\n",
      "Iteration 18, loss = 0.18964934\n",
      "Iteration 19, loss = 0.18932816\n",
      "Iteration 20, loss = 0.18457252\n",
      "Iteration 21, loss = 0.18629064\n",
      "Iteration 22, loss = 0.18258087\n",
      "Iteration 23, loss = 0.18106847\n",
      "Iteration 24, loss = 0.18118651\n",
      "Iteration 25, loss = 0.17810902\n",
      "Iteration 26, loss = 0.17847097\n",
      "Iteration 27, loss = 0.17870419\n",
      "Iteration 28, loss = 0.17757657\n",
      "Iteration 29, loss = 0.17859157\n",
      "Iteration 30, loss = 0.17950050\n",
      "Iteration 31, loss = 0.18145595\n",
      "Iteration 32, loss = 0.17257831\n",
      "Iteration 33, loss = 0.17117473\n",
      "Iteration 34, loss = 0.17288556\n",
      "Iteration 35, loss = 0.17171998\n",
      "Iteration 36, loss = 0.16920415\n",
      "Iteration 37, loss = 0.16850789\n",
      "Iteration 38, loss = 0.17100287\n",
      "Iteration 1, loss = 0.66845982\n",
      "Iteration 2, loss = 0.46734661\n",
      "Iteration 3, loss = 0.24053502\n",
      "Iteration 4, loss = 0.12767754\n",
      "Iteration 5, loss = 0.08920044\n",
      "Iteration 6, loss = 0.07235444\n",
      "Iteration 7, loss = 0.06262169\n",
      "Iteration 8, loss = 0.05576890\n",
      "Iteration 9, loss = 0.05111441\n",
      "Iteration 10, loss = 0.04881176\n",
      "Iteration 11, loss = 0.04612314\n",
      "Iteration 12, loss = 0.04475244\n",
      "Iteration 13, loss = 0.04355625\n",
      "Iteration 14, loss = 0.04163603\n",
      "Iteration 15, loss = 0.04050375\n",
      "Iteration 16, loss = 0.03988511\n",
      "Iteration 17, loss = 0.03899605\n",
      "Iteration 18, loss = 0.03874382\n",
      "Iteration 19, loss = 0.03774875\n",
      "Iteration 20, loss = 0.03813121\n",
      "Iteration 21, loss = 0.03779815\n",
      "Iteration 22, loss = 0.03745671\n",
      "Iteration 23, loss = 0.03670331\n",
      "Iteration 24, loss = 0.03635002\n",
      "Iteration 25, loss = 0.03643957\n",
      "Iteration 26, loss = 0.03544602\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ......................alpha=0.0001;, score=0.594 total time= 2.4min\n",
      "Iteration 1, loss = 0.66163123\n",
      "Iteration 2, loss = 0.44149868\n",
      "Iteration 3, loss = 0.22896353\n",
      "Iteration 4, loss = 0.14631194\n",
      "Iteration 5, loss = 0.11476668\n",
      "Iteration 6, loss = 0.10042217\n",
      "Iteration 7, loss = 0.09026335\n",
      "Iteration 8, loss = 0.08462924\n",
      "Iteration 9, loss = 0.07999379\n",
      "Iteration 10, loss = 0.07730272\n",
      "Iteration 11, loss = 0.07525474\n",
      "Iteration 12, loss = 0.07278582\n",
      "Iteration 13, loss = 0.07001206\n",
      "Iteration 14, loss = 0.06871232\n",
      "Iteration 15, loss = 0.06760476\n",
      "Iteration 16, loss = 0.06597548\n",
      "Iteration 17, loss = 0.06486897\n",
      "Iteration 18, loss = 0.06399222\n",
      "Iteration 19, loss = 0.06311053\n",
      "Iteration 20, loss = 0.06242622\n",
      "Iteration 21, loss = 0.06165651\n",
      "Iteration 22, loss = 0.06073562\n",
      "Iteration 23, loss = 0.06055591\n",
      "Iteration 24, loss = 0.05963291\n",
      "Iteration 25, loss = 0.06052975\n",
      "Iteration 26, loss = 0.05953114\n",
      "Iteration 27, loss = 0.05940121\n",
      "Iteration 28, loss = 0.05957269\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ........................alpha=0.01;, score=0.598 total time= 2.5min\n",
      "Iteration 39, loss = 0.17028900\n",
      "Iteration 40, loss = 0.17000697\n",
      "Iteration 41, loss = 0.17258994\n",
      "Iteration 42, loss = 0.16787115\n",
      "Iteration 43, loss = 0.16764346\n",
      "Iteration 44, loss = 0.16978147\n",
      "Iteration 45, loss = 0.16615842\n",
      "Iteration 46, loss = 0.16697978\n",
      "Iteration 47, loss = 0.16637171\n",
      "Iteration 48, loss = 0.16627304\n",
      "Iteration 49, loss = 0.16520794\n",
      "Iteration 50, loss = 0.16726994\n",
      "Iteration 51, loss = 0.16486873\n",
      "Iteration 52, loss = 0.16776010\n",
      "Iteration 53, loss = 0.16592083\n",
      "Iteration 54, loss = 0.16675422\n",
      "Iteration 1, loss = 0.66582203\n",
      "Iteration 2, loss = 0.45892875\n",
      "Iteration 3, loss = 0.22361181\n",
      "Iteration 4, loss = 0.12664112\n",
      "Iteration 5, loss = 0.09109428\n",
      "Iteration 6, loss = 0.07361674\n",
      "Iteration 7, loss = 0.06426647\n",
      "Iteration 8, loss = 0.05718441\n",
      "Iteration 9, loss = 0.05341914\n",
      "Iteration 10, loss = 0.05008648\n",
      "Iteration 11, loss = 0.04788362\n",
      "Iteration 12, loss = 0.04659853\n",
      "Iteration 13, loss = 0.04453454\n",
      "Iteration 14, loss = 0.04339544\n",
      "Iteration 15, loss = 0.04216639\n",
      "Iteration 16, loss = 0.04193455\n",
      "Iteration 17, loss = 0.04096329\n",
      "Iteration 18, loss = 0.04008648\n",
      "Iteration 19, loss = 0.04070374\n",
      "Iteration 20, loss = 0.03921975\n",
      "Iteration 21, loss = 0.03951928\n",
      "Iteration 22, loss = 0.03888038\n",
      "Iteration 23, loss = 0.03885872\n",
      "Iteration 24, loss = 0.03922975\n",
      "Iteration 25, loss = 0.03806906\n",
      "Iteration 26, loss = 0.03780450\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .......................alpha=0.001;, score=0.605 total time= 2.4min\n",
      "Iteration 1, loss = 0.68315238\n",
      "Iteration 2, loss = 0.46994035\n",
      "Iteration 3, loss = 0.25487259\n",
      "Iteration 4, loss = 0.15578429\n",
      "Iteration 5, loss = 0.11958688\n",
      "Iteration 6, loss = 0.10122079\n",
      "Iteration 7, loss = 0.09284686\n",
      "Iteration 8, loss = 0.08698750\n",
      "Iteration 9, loss = 0.08262257\n",
      "Iteration 10, loss = 0.08017716\n",
      "Iteration 11, loss = 0.07770749\n",
      "Iteration 12, loss = 0.07514944\n",
      "Iteration 13, loss = 0.07514697\n",
      "Iteration 14, loss = 0.07156893\n",
      "Iteration 15, loss = 0.07073086\n",
      "Iteration 16, loss = 0.06984961\n",
      "Iteration 17, loss = 0.06792751\n",
      "Iteration 18, loss = 0.06676401\n",
      "Iteration 19, loss = 0.06700523\n",
      "Iteration 20, loss = 0.06549914\n",
      "Iteration 21, loss = 0.06424534\n",
      "Iteration 22, loss = 0.06463387\n",
      "Iteration 23, loss = 0.06292391\n",
      "Iteration 24, loss = 0.06337692\n",
      "Iteration 25, loss = 0.06338540\n",
      "Iteration 26, loss = 0.06164546\n",
      "Iteration 27, loss = 0.06203400\n",
      "Iteration 28, loss = 0.06084516\n",
      "Iteration 29, loss = 0.06029098\n",
      "Iteration 30, loss = 0.06239537\n",
      "Iteration 31, loss = 0.06042765\n",
      "Iteration 32, loss = 0.06049614\n",
      "Iteration 33, loss = 0.05946069\n",
      "Iteration 34, loss = 0.06028477\n",
      "Iteration 35, loss = 0.05953314\n",
      "Iteration 36, loss = 0.05949793\n",
      "Iteration 37, loss = 0.06239548\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ........................alpha=0.01;, score=0.593 total time= 3.3min\n",
      "Iteration 55, loss = 0.16460285\n",
      "Iteration 56, loss = 0.16085799\n",
      "Iteration 57, loss = 0.16023833\n",
      "Iteration 58, loss = 0.16075752\n",
      "Iteration 59, loss = 0.16819299\n",
      "Iteration 60, loss = 0.16371485\n",
      "Iteration 1, loss = 0.67879646\n",
      "Iteration 2, loss = 0.46986352\n",
      "Iteration 3, loss = 0.25478419\n",
      "Iteration 4, loss = 0.12723528\n",
      "Iteration 5, loss = 0.08514214\n",
      "Iteration 6, loss = 0.06623066\n",
      "Iteration 7, loss = 0.05597538\n",
      "Iteration 8, loss = 0.04907653\n",
      "Iteration 9, loss = 0.04513254\n",
      "Iteration 10, loss = 0.04157411\n",
      "Iteration 11, loss = 0.03837989\n",
      "Iteration 12, loss = 0.03711550\n",
      "Iteration 13, loss = 0.03473044\n",
      "Iteration 14, loss = 0.03352592\n",
      "Iteration 15, loss = 0.03331850\n",
      "Iteration 16, loss = 0.03198950\n",
      "Iteration 17, loss = 0.03088175\n",
      "Iteration 18, loss = 0.03053856\n",
      "Iteration 19, loss = 0.02980359\n",
      "Iteration 20, loss = 0.02930898\n",
      "Iteration 21, loss = 0.02904596\n",
      "Iteration 22, loss = 0.02866723\n",
      "Iteration 23, loss = 0.02812416\n",
      "Iteration 24, loss = 0.02822185\n",
      "Iteration 25, loss = 0.02771371\n",
      "Iteration 26, loss = 0.02777051\n",
      "Iteration 27, loss = 0.02746420\n",
      "Iteration 28, loss = 0.02772725\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ......................alpha=0.0001;, score=0.582 total time= 2.5min\n",
      "Iteration 1, loss = 0.66689076\n",
      "Iteration 2, loss = 0.45285482\n",
      "Iteration 3, loss = 0.22635740\n",
      "Iteration 4, loss = 0.14045877\n",
      "Iteration 5, loss = 0.10851918\n",
      "Iteration 6, loss = 0.09307201\n",
      "Iteration 7, loss = 0.08440083\n",
      "Iteration 8, loss = 0.07946151\n",
      "Iteration 9, loss = 0.07435784\n",
      "Iteration 10, loss = 0.07162272\n",
      "Iteration 11, loss = 0.06911923\n",
      "Iteration 12, loss = 0.06791088\n",
      "Iteration 13, loss = 0.06570777\n",
      "Iteration 14, loss = 0.06473723\n",
      "Iteration 15, loss = 0.06312945\n",
      "Iteration 16, loss = 0.06207960\n",
      "Iteration 17, loss = 0.06080749\n",
      "Iteration 18, loss = 0.05999960\n",
      "Iteration 19, loss = 0.05944939\n",
      "Iteration 20, loss = 0.05878324\n",
      "Iteration 21, loss = 0.05917246\n",
      "Iteration 22, loss = 0.05779018\n",
      "Iteration 23, loss = 0.05633840\n",
      "Iteration 24, loss = 0.05554760\n",
      "Iteration 25, loss = 0.05624973\n",
      "Iteration 26, loss = 0.05612389\n",
      "Iteration 27, loss = 0.05431304\n",
      "Iteration 28, loss = 0.05485513\n",
      "Iteration 29, loss = 0.05385858\n",
      "Iteration 30, loss = 0.05398981\n",
      "Iteration 31, loss = 0.05432183\n",
      "Iteration 32, loss = 0.05385114\n",
      "Iteration 33, loss = 0.05329737\n",
      "Iteration 34, loss = 0.05332273\n",
      "Iteration 35, loss = 0.05268959\n",
      "Iteration 36, loss = 0.05260131\n",
      "Iteration 37, loss = 0.05283772\n",
      "Iteration 38, loss = 0.05164917\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ........................alpha=0.01;, score=0.608 total time= 3.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67355953\n",
      "Iteration 2, loss = 0.46689167\n",
      "Iteration 3, loss = 0.25456639\n",
      "Iteration 4, loss = 0.13501289\n",
      "Iteration 5, loss = 0.08953687\n",
      "Iteration 6, loss = 0.06925091\n",
      "Iteration 7, loss = 0.05791273\n",
      "Iteration 8, loss = 0.05116710\n",
      "Iteration 9, loss = 0.04598761\n",
      "Iteration 10, loss = 0.04309239\n",
      "Iteration 11, loss = 0.04081037\n",
      "Iteration 12, loss = 0.03901257\n",
      "Iteration 13, loss = 0.03775687\n",
      "Iteration 14, loss = 0.03629759\n",
      "Iteration 15, loss = 0.03508685\n",
      "Iteration 16, loss = 0.03425316\n",
      "Iteration 17, loss = 0.03284821\n",
      "Iteration 18, loss = 0.03307943\n",
      "Iteration 19, loss = 0.03316874\n",
      "Iteration 20, loss = 0.03216909\n",
      "Iteration 21, loss = 0.03144714\n",
      "Iteration 22, loss = 0.03136626\n",
      "Iteration 23, loss = 0.03114611\n",
      "Iteration 24, loss = 0.03084622\n",
      "Iteration 25, loss = 0.03143071\n",
      "Iteration 26, loss = 0.03024149\n",
      "Iteration 27, loss = 0.02988824\n",
      "Iteration 28, loss = 0.03015446\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ......................alpha=0.0001;, score=0.609 total time= 2.5min\n",
      "Iteration 1, loss = 0.68041795\n",
      "Iteration 2, loss = 0.47921060\n",
      "Iteration 3, loss = 0.25345948\n",
      "Iteration 4, loss = 0.15491153\n",
      "Iteration 5, loss = 0.11849534\n",
      "Iteration 6, loss = 0.10246956\n",
      "Iteration 7, loss = 0.09278454\n",
      "Iteration 8, loss = 0.08546843\n",
      "Iteration 9, loss = 0.08193018\n",
      "Iteration 10, loss = 0.07834228\n",
      "Iteration 11, loss = 0.07577671\n",
      "Iteration 12, loss = 0.07238973\n",
      "Iteration 13, loss = 0.07070306\n",
      "Iteration 14, loss = 0.06959055\n",
      "Iteration 15, loss = 0.06852689\n",
      "Iteration 16, loss = 0.06654773\n",
      "Iteration 17, loss = 0.06588798\n",
      "Iteration 18, loss = 0.06445857\n",
      "Iteration 19, loss = 0.06520316\n",
      "Iteration 20, loss = 0.06398959\n",
      "Iteration 21, loss = 0.06287060\n",
      "Iteration 22, loss = 0.06211186\n",
      "Iteration 23, loss = 0.06124883\n",
      "Iteration 24, loss = 0.06102075\n",
      "Iteration 25, loss = 0.06104174\n",
      "Iteration 26, loss = 0.06020852\n",
      "Iteration 27, loss = 0.05910820\n",
      "Iteration 28, loss = 0.06067841\n",
      "Iteration 29, loss = 0.05904389\n",
      "Iteration 30, loss = 0.05843524\n",
      "Iteration 31, loss = 0.05810266\n",
      "Iteration 32, loss = 0.05778180\n",
      "Iteration 33, loss = 0.05774801\n",
      "Iteration 34, loss = 0.05699699\n",
      "Iteration 35, loss = 0.05779932\n",
      "Iteration 36, loss = 0.05675929\n",
      "Iteration 37, loss = 0.05862292\n",
      "Iteration 38, loss = 0.05743455\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ........................alpha=0.01;, score=0.601 total time= 3.3min\n",
      "Iteration 61, loss = 0.16559550\n",
      "Iteration 62, loss = 0.16431660\n",
      "Iteration 63, loss = 0.16272132\n",
      "Iteration 64, loss = 0.15826583\n",
      "Iteration 65, loss = 0.15936393\n",
      "Iteration 66, loss = 0.16102028\n",
      "Iteration 67, loss = 0.15928779\n",
      "Iteration 68, loss = 0.15782402\n",
      "Iteration 69, loss = 0.16074492\n",
      "Iteration 70, loss = 0.15907845\n",
      "Iteration 71, loss = 0.15932920\n",
      "Iteration 72, loss = 0.15898408\n",
      "Iteration 73, loss = 0.16147341\n",
      "Iteration 74, loss = 0.15812337\n",
      "Iteration 75, loss = 0.15772243\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "{'alpha': 0.1}\n",
      "Iteration 1, loss = 0.66847392\n",
      "Iteration 2, loss = 0.46328723\n",
      "Iteration 3, loss = 0.23850882\n",
      "Iteration 4, loss = 0.12645413\n",
      "Iteration 5, loss = 0.08833113\n",
      "Iteration 6, loss = 0.07072242\n",
      "Iteration 7, loss = 0.06091601\n",
      "Iteration 8, loss = 0.05416614\n",
      "Iteration 9, loss = 0.04981496\n",
      "Iteration 10, loss = 0.04770941\n",
      "Iteration 11, loss = 0.04495465\n",
      "Iteration 12, loss = 0.04291451\n",
      "Iteration 13, loss = 0.04094672\n",
      "Iteration 14, loss = 0.03909367\n",
      "Iteration 15, loss = 0.03967367\n",
      "Iteration 16, loss = 0.03803346\n",
      "Iteration 17, loss = 0.03713471\n",
      "Iteration 18, loss = 0.03676308\n",
      "Iteration 19, loss = 0.03643257\n",
      "Iteration 20, loss = 0.03609238\n",
      "Iteration 21, loss = 0.03544801\n",
      "Iteration 22, loss = 0.03492318\n",
      "Iteration 23, loss = 0.03473514\n",
      "Iteration 24, loss = 0.03451836\n",
      "Iteration 25, loss = 0.03406903\n",
      "Iteration 26, loss = 0.03407608\n",
      "Iteration 27, loss = 0.03353491\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ......................alpha=0.0001;, score=0.607 total time= 2.4min\n",
      "Iteration 1, loss = 0.67063808\n",
      "Iteration 2, loss = 0.44932967\n",
      "Iteration 3, loss = 0.22581977\n",
      "Iteration 4, loss = 0.13926947\n",
      "Iteration 5, loss = 0.10695522\n",
      "Iteration 6, loss = 0.09124146\n",
      "Iteration 7, loss = 0.08303362\n",
      "Iteration 8, loss = 0.07710231\n",
      "Iteration 9, loss = 0.07229233\n",
      "Iteration 10, loss = 0.06921782\n",
      "Iteration 11, loss = 0.06648250\n",
      "Iteration 12, loss = 0.06482466\n",
      "Iteration 13, loss = 0.06324167\n",
      "Iteration 14, loss = 0.06144584\n",
      "Iteration 15, loss = 0.05967636\n",
      "Iteration 16, loss = 0.05882280\n",
      "Iteration 17, loss = 0.05775964\n",
      "Iteration 18, loss = 0.05709771\n",
      "Iteration 19, loss = 0.05596973\n",
      "Iteration 20, loss = 0.05576118\n",
      "Iteration 21, loss = 0.05604039\n",
      "Iteration 22, loss = 0.05499278\n",
      "Iteration 23, loss = 0.05392587\n",
      "Iteration 24, loss = 0.05259059\n",
      "Iteration 25, loss = 0.05292397\n",
      "Iteration 26, loss = 0.05218487\n",
      "Iteration 27, loss = 0.05226637\n",
      "Iteration 28, loss = 0.05196333\n",
      "Iteration 29, loss = 0.05118811\n",
      "Iteration 30, loss = 0.05070210\n",
      "Iteration 31, loss = 0.05205497\n",
      "Iteration 32, loss = 0.05111042\n",
      "Iteration 33, loss = 0.05103584\n",
      "Iteration 34, loss = 0.04925060\n",
      "Iteration 35, loss = 0.04878733\n",
      "Iteration 36, loss = 0.05018803\n",
      "Iteration 37, loss = 0.05046648\n",
      "Iteration 38, loss = 0.05105270\n",
      "Iteration 39, loss = 0.04903097\n",
      "Iteration 40, loss = 0.04847927\n",
      "Iteration 41, loss = 0.04735532\n",
      "Iteration 42, loss = 0.04761094\n",
      "Iteration 43, loss = 0.04847939\n",
      "Iteration 44, loss = 0.04690688\n",
      "Iteration 45, loss = 0.04694302\n",
      "Iteration 46, loss = 0.04799945\n",
      "Iteration 47, loss = 0.04775270\n",
      "Iteration 48, loss = 0.04649877\n",
      "Iteration 49, loss = 0.04653241\n",
      "Iteration 50, loss = 0.04670566\n",
      "Iteration 51, loss = 0.04673851\n",
      "Iteration 52, loss = 0.04777876\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ........................alpha=0.01;, score=0.578 total time= 4.3min\n",
      "Iteration 1, loss = 0.66481917\n",
      "Iteration 2, loss = 0.44008869\n",
      "Iteration 3, loss = 0.20636500\n",
      "Iteration 4, loss = 0.11883894\n",
      "Iteration 5, loss = 0.08449183\n",
      "Iteration 6, loss = 0.06853566\n",
      "Iteration 7, loss = 0.05860878\n",
      "Iteration 8, loss = 0.05320356\n",
      "Iteration 9, loss = 0.04867398\n",
      "Iteration 10, loss = 0.04643016\n",
      "Iteration 11, loss = 0.04340969\n",
      "Iteration 12, loss = 0.04180486\n",
      "Iteration 13, loss = 0.04047105\n",
      "Iteration 14, loss = 0.03947612\n",
      "Iteration 15, loss = 0.03915118\n",
      "Iteration 16, loss = 0.03810769\n",
      "Iteration 17, loss = 0.03846120\n",
      "Iteration 18, loss = 0.03680750\n",
      "Iteration 19, loss = 0.03610106\n",
      "Iteration 20, loss = 0.03630711\n",
      "Iteration 21, loss = 0.03536622\n",
      "Iteration 22, loss = 0.03545799\n",
      "Iteration 23, loss = 0.03558026\n",
      "Iteration 24, loss = 0.03510425\n",
      "Iteration 25, loss = 0.03534851\n",
      "Iteration 26, loss = 0.03488733\n",
      "Iteration 27, loss = 0.03482136\n",
      "Iteration 28, loss = 0.03377285\n",
      "Iteration 29, loss = 0.03380603\n",
      "Iteration 30, loss = 0.03346554\n",
      "Iteration 31, loss = 0.03398422\n",
      "Iteration 32, loss = 0.03316197\n",
      "Iteration 33, loss = 0.03356381\n",
      "Iteration 34, loss = 0.03403727\n",
      "Iteration 35, loss = 0.03363234\n",
      "Iteration 36, loss = 0.03344123\n",
      "Iteration 37, loss = 0.03339130\n",
      "Iteration 38, loss = 0.03431822\n",
      "Iteration 39, loss = 0.03365771\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .......................alpha=0.001;, score=0.608 total time= 3.5min\n",
      "Iteration 1, loss = 0.69917038\n",
      "Iteration 2, loss = 0.52958494\n",
      "Iteration 3, loss = 0.37145638\n",
      "Iteration 4, loss = 0.28147547\n",
      "Iteration 5, loss = 0.24540902\n",
      "Iteration 6, loss = 0.22661730\n",
      "Iteration 7, loss = 0.21319901\n",
      "Iteration 8, loss = 0.20337493\n",
      "Iteration 9, loss = 0.19554552\n",
      "Iteration 10, loss = 0.19020986\n",
      "Iteration 11, loss = 0.18456171\n",
      "Iteration 12, loss = 0.17977550\n",
      "Iteration 13, loss = 0.17696486\n",
      "Iteration 14, loss = 0.17378903\n",
      "Iteration 15, loss = 0.17102318\n",
      "Iteration 16, loss = 0.16903854\n",
      "Iteration 17, loss = 0.16690542\n",
      "Iteration 18, loss = 0.16443452\n",
      "Iteration 19, loss = 0.16267901\n",
      "Iteration 20, loss = 0.16238732\n",
      "Iteration 21, loss = 0.16112975\n",
      "Iteration 22, loss = 0.15931310\n",
      "Iteration 23, loss = 0.15667693\n",
      "Iteration 24, loss = 0.15657580\n",
      "Iteration 25, loss = 0.15632169\n",
      "Iteration 26, loss = 0.15483919\n",
      "Iteration 27, loss = 0.15456540\n",
      "Iteration 28, loss = 0.15345877\n",
      "Iteration 29, loss = 0.15180789\n",
      "Iteration 30, loss = 0.14925879\n",
      "Iteration 31, loss = 0.15213533\n",
      "Iteration 32, loss = 0.15242564\n",
      "Iteration 33, loss = 0.15217008\n",
      "Iteration 34, loss = 0.15013203\n",
      "Iteration 35, loss = 0.14908616\n",
      "Iteration 36, loss = 0.14920343\n",
      "Iteration 37, loss = 0.14474011\n",
      "Iteration 38, loss = 0.14778626\n",
      "Iteration 39, loss = 0.14826543\n",
      "Iteration 40, loss = 0.14763875\n",
      "Iteration 41, loss = 0.14595543\n",
      "Iteration 42, loss = 0.14500312\n",
      "Iteration 43, loss = 0.14650831\n",
      "Iteration 44, loss = 0.14461912\n",
      "Iteration 45, loss = 0.14799107\n",
      "Iteration 46, loss = 0.14439485\n",
      "Iteration 47, loss = 0.14529098\n",
      "Iteration 48, loss = 0.14409720\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.616 total time= 3.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66149006\n",
      "Iteration 2, loss = 0.44871846\n",
      "Iteration 3, loss = 0.21380273\n",
      "Iteration 4, loss = 0.12195563\n",
      "Iteration 5, loss = 0.08529489\n",
      "Iteration 6, loss = 0.06863533\n",
      "Iteration 7, loss = 0.05859439\n",
      "Iteration 8, loss = 0.05201601\n",
      "Iteration 9, loss = 0.04775615\n",
      "Iteration 10, loss = 0.04470486\n",
      "Iteration 11, loss = 0.04229508\n",
      "Iteration 12, loss = 0.04022694\n",
      "Iteration 13, loss = 0.03880612\n",
      "Iteration 14, loss = 0.03775526\n",
      "Iteration 15, loss = 0.03710643\n",
      "Iteration 16, loss = 0.03553730\n",
      "Iteration 17, loss = 0.03526422\n",
      "Iteration 18, loss = 0.03436012\n",
      "Iteration 19, loss = 0.03420731\n",
      "Iteration 20, loss = 0.03380222\n",
      "Iteration 21, loss = 0.03360938\n",
      "Iteration 22, loss = 0.03255339\n",
      "Iteration 23, loss = 0.03368861\n",
      "Iteration 24, loss = 0.03287095\n",
      "Iteration 25, loss = 0.03275173\n",
      "Iteration 26, loss = 0.03237421\n",
      "Iteration 27, loss = 0.03119235\n",
      "Iteration 28, loss = 0.03209477\n",
      "Iteration 29, loss = 0.03129244\n",
      "Iteration 30, loss = 0.03106711\n",
      "Iteration 31, loss = 0.03126018\n",
      "Iteration 32, loss = 0.03108015\n",
      "Iteration 33, loss = 0.03164204\n",
      "Iteration 34, loss = 0.03152470\n",
      "Iteration 35, loss = 0.03126946\n",
      "Iteration 36, loss = 0.03131195\n",
      "Iteration 37, loss = 0.03092438\n",
      "Iteration 38, loss = 0.03076474\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .......................alpha=0.001;, score=0.580 total time= 3.4min\n",
      "Iteration 1, loss = 0.67800011\n",
      "Iteration 2, loss = 0.50064996\n",
      "Iteration 3, loss = 0.33744268\n",
      "Iteration 4, loss = 0.27088767\n",
      "Iteration 5, loss = 0.24128634\n",
      "Iteration 6, loss = 0.22335215\n",
      "Iteration 7, loss = 0.21048925\n",
      "Iteration 8, loss = 0.20052359\n",
      "Iteration 9, loss = 0.19273124\n",
      "Iteration 10, loss = 0.18817287\n",
      "Iteration 11, loss = 0.18245358\n",
      "Iteration 12, loss = 0.17925397\n",
      "Iteration 13, loss = 0.17571418\n",
      "Iteration 14, loss = 0.17151378\n",
      "Iteration 15, loss = 0.16967170\n",
      "Iteration 16, loss = 0.16872472\n",
      "Iteration 17, loss = 0.16513445\n",
      "Iteration 18, loss = 0.16459033\n",
      "Iteration 19, loss = 0.16375458\n",
      "Iteration 20, loss = 0.16124963\n",
      "Iteration 21, loss = 0.15906165\n",
      "Iteration 22, loss = 0.15754966\n",
      "Iteration 23, loss = 0.15581453\n",
      "Iteration 24, loss = 0.15593896\n",
      "Iteration 25, loss = 0.15419091\n",
      "Iteration 26, loss = 0.15336623\n",
      "Iteration 27, loss = 0.15124389\n",
      "Iteration 28, loss = 0.15060225\n",
      "Iteration 29, loss = 0.15315269\n",
      "Iteration 30, loss = 0.15179308\n",
      "Iteration 31, loss = 0.14932137\n",
      "Iteration 32, loss = 0.15141128\n",
      "Iteration 33, loss = 0.14837960\n",
      "Iteration 34, loss = 0.14872121\n",
      "Iteration 35, loss = 0.15044796\n",
      "Iteration 36, loss = 0.14671336\n",
      "Iteration 37, loss = 0.14614375\n",
      "Iteration 38, loss = 0.14462359\n",
      "Iteration 39, loss = 0.14319884\n",
      "Iteration 40, loss = 0.14446043\n",
      "Iteration 41, loss = 0.15298241\n",
      "Iteration 42, loss = 0.14815220\n",
      "Iteration 43, loss = 0.14656230\n",
      "Iteration 44, loss = 0.14752875\n",
      "Iteration 45, loss = 0.14771980\n",
      "Iteration 46, loss = 0.14339093\n",
      "Iteration 47, loss = 0.14491185\n",
      "Iteration 48, loss = 0.14545536\n",
      "Iteration 49, loss = 0.14418089\n",
      "Iteration 50, loss = 0.14402062\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.609 total time= 3.8min\n",
      "Iteration 1, loss = 0.67315270\n",
      "Iteration 2, loss = 0.45038726\n",
      "Iteration 3, loss = 0.22666984\n",
      "Iteration 4, loss = 0.12495834\n",
      "Iteration 5, loss = 0.09008978\n",
      "Iteration 6, loss = 0.07427428\n",
      "Iteration 7, loss = 0.06419957\n",
      "Iteration 8, loss = 0.05957185\n",
      "Iteration 9, loss = 0.05362222\n",
      "Iteration 10, loss = 0.05121103\n",
      "Iteration 11, loss = 0.04941363\n",
      "Iteration 12, loss = 0.04745771\n",
      "Iteration 13, loss = 0.04598195\n",
      "Iteration 14, loss = 0.04563345\n",
      "Iteration 15, loss = 0.04405265\n",
      "Iteration 16, loss = 0.04396851\n",
      "Iteration 17, loss = 0.04368529\n",
      "Iteration 18, loss = 0.04245851\n",
      "Iteration 19, loss = 0.04197141\n",
      "Iteration 20, loss = 0.04124448\n",
      "Iteration 21, loss = 0.04082977\n",
      "Iteration 22, loss = 0.04090697\n",
      "Iteration 23, loss = 0.04120087\n",
      "Iteration 24, loss = 0.04080102\n",
      "Iteration 25, loss = 0.04082323\n",
      "Iteration 26, loss = 0.04052245\n",
      "Iteration 27, loss = 0.04093269\n",
      "Iteration 28, loss = 0.03970236\n",
      "Iteration 29, loss = 0.04045255\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .......................alpha=0.001;, score=0.592 total time= 2.6min\n",
      "Iteration 1, loss = 0.68823576\n",
      "Iteration 2, loss = 0.51020948\n",
      "Iteration 3, loss = 0.34257710\n",
      "Iteration 4, loss = 0.26612028\n",
      "Iteration 5, loss = 0.23415676\n",
      "Iteration 6, loss = 0.21715918\n",
      "Iteration 7, loss = 0.20358902\n",
      "Iteration 8, loss = 0.19546127\n",
      "Iteration 9, loss = 0.18931079\n",
      "Iteration 10, loss = 0.18275377\n",
      "Iteration 11, loss = 0.17789751\n",
      "Iteration 12, loss = 0.17572338\n",
      "Iteration 13, loss = 0.17355897\n",
      "Iteration 14, loss = 0.16809362\n",
      "Iteration 15, loss = 0.16601459\n",
      "Iteration 16, loss = 0.16434585\n",
      "Iteration 17, loss = 0.16301025\n",
      "Iteration 18, loss = 0.16118578\n",
      "Iteration 19, loss = 0.16083949\n",
      "Iteration 20, loss = 0.15802279\n",
      "Iteration 21, loss = 0.15802266\n",
      "Iteration 22, loss = 0.15733681\n",
      "Iteration 23, loss = 0.15687545\n",
      "Iteration 24, loss = 0.15734313\n",
      "Iteration 25, loss = 0.15379067\n",
      "Iteration 26, loss = 0.15516014\n",
      "Iteration 27, loss = 0.15141712\n",
      "Iteration 28, loss = 0.15192330\n",
      "Iteration 29, loss = 0.15161884\n",
      "Iteration 30, loss = 0.15058300\n",
      "Iteration 31, loss = 0.14993509\n",
      "Iteration 32, loss = 0.14922269\n",
      "Iteration 33, loss = 0.14591147\n",
      "Iteration 34, loss = 0.14663974\n",
      "Iteration 35, loss = 0.14793263\n",
      "Iteration 36, loss = 0.14567812\n",
      "Iteration 37, loss = 0.14725713\n",
      "Iteration 38, loss = 0.14497114\n",
      "Iteration 39, loss = 0.14491329\n",
      "Iteration 40, loss = 0.14581115\n",
      "Iteration 41, loss = 0.14634783\n",
      "Iteration 42, loss = 0.14492403\n",
      "Iteration 43, loss = 0.14537254\n",
      "Iteration 44, loss = 0.14388594\n",
      "Iteration 45, loss = 0.14486349\n",
      "Iteration 46, loss = 0.14557635\n",
      "Iteration 47, loss = 0.14438848\n",
      "Iteration 48, loss = 0.14497838\n",
      "Iteration 49, loss = 0.14192715\n",
      "Iteration 50, loss = 0.14362712\n",
      "Iteration 51, loss = 0.14180121\n",
      "Iteration 52, loss = 0.14130414\n",
      "Iteration 53, loss = 0.13961154\n",
      "Iteration 54, loss = 0.14282182\n",
      "Iteration 55, loss = 0.14282758\n",
      "Iteration 56, loss = 0.14676353\n",
      "Iteration 57, loss = 0.14214141\n",
      "Iteration 58, loss = 0.14453556\n",
      "Iteration 59, loss = 0.13994579\n",
      "Iteration 60, loss = 0.14215105\n",
      "Iteration 61, loss = 0.14328706\n",
      "Iteration 62, loss = 0.13990035\n",
      "Iteration 63, loss = 0.13932036\n",
      "Iteration 64, loss = 0.14202568\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.603 total time= 4.8min\n",
      "Iteration 1, loss = 0.68474508\n",
      "Iteration 2, loss = 0.49054388\n",
      "Iteration 3, loss = 0.26417184\n",
      "Iteration 4, loss = 0.13897429\n",
      "Iteration 5, loss = 0.09695382\n",
      "Iteration 6, loss = 0.07776234\n",
      "Iteration 7, loss = 0.06717830\n",
      "Iteration 8, loss = 0.06040455\n",
      "Iteration 9, loss = 0.05625883\n",
      "Iteration 10, loss = 0.05288992\n",
      "Iteration 11, loss = 0.05015442\n",
      "Iteration 12, loss = 0.04882126\n",
      "Iteration 13, loss = 0.04696942\n",
      "Iteration 14, loss = 0.04616341\n",
      "Iteration 15, loss = 0.04509997\n",
      "Iteration 16, loss = 0.04385660\n",
      "Iteration 17, loss = 0.04365025\n",
      "Iteration 18, loss = 0.04265888\n",
      "Iteration 19, loss = 0.04222812\n",
      "Iteration 20, loss = 0.04181521\n",
      "Iteration 21, loss = 0.04071360\n",
      "Iteration 22, loss = 0.04061094\n",
      "Iteration 23, loss = 0.04001187\n",
      "Iteration 24, loss = 0.03957061\n",
      "Iteration 25, loss = 0.03991947\n",
      "Iteration 26, loss = 0.03961276\n",
      "Iteration 27, loss = 0.03960397\n",
      "Iteration 28, loss = 0.03922956\n",
      "Iteration 29, loss = 0.03896937\n",
      "Iteration 30, loss = 0.03856932\n",
      "Iteration 31, loss = 0.03901476\n",
      "Iteration 32, loss = 0.03857577\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .......................alpha=0.001;, score=0.605 total time= 2.9min\n",
      "Iteration 1, loss = 0.67387858\n",
      "Iteration 2, loss = 0.48167887\n",
      "Iteration 3, loss = 0.33007922\n",
      "Iteration 4, loss = 0.26712974\n",
      "Iteration 5, loss = 0.23680378\n",
      "Iteration 6, loss = 0.21926967\n",
      "Iteration 7, loss = 0.20695379\n",
      "Iteration 8, loss = 0.19684552\n",
      "Iteration 9, loss = 0.18945121\n",
      "Iteration 10, loss = 0.18394023\n",
      "Iteration 11, loss = 0.17876869\n",
      "Iteration 12, loss = 0.17361874\n",
      "Iteration 13, loss = 0.17106494\n",
      "Iteration 14, loss = 0.16787043\n",
      "Iteration 15, loss = 0.16621942\n",
      "Iteration 16, loss = 0.16174380\n",
      "Iteration 17, loss = 0.16122212\n",
      "Iteration 18, loss = 0.16063020\n",
      "Iteration 19, loss = 0.15708035\n",
      "Iteration 20, loss = 0.15648512\n",
      "Iteration 21, loss = 0.15577943\n",
      "Iteration 22, loss = 0.15410978\n",
      "Iteration 23, loss = 0.15230227\n",
      "Iteration 24, loss = 0.15050672\n",
      "Iteration 25, loss = 0.14910319\n",
      "Iteration 26, loss = 0.14773512\n",
      "Iteration 27, loss = 0.14650510\n",
      "Iteration 28, loss = 0.14731434\n",
      "Iteration 29, loss = 0.14531098\n",
      "Iteration 30, loss = 0.14515171\n",
      "Iteration 31, loss = 0.14438576\n",
      "Iteration 32, loss = 0.14486015\n",
      "Iteration 33, loss = 0.14436966\n",
      "Iteration 34, loss = 0.14342149\n",
      "Iteration 35, loss = 0.14076993\n",
      "Iteration 36, loss = 0.14127814\n",
      "Iteration 37, loss = 0.14050302\n",
      "Iteration 38, loss = 0.14130134\n",
      "Iteration 39, loss = 0.14259498\n",
      "Iteration 40, loss = 0.14093254\n",
      "Iteration 41, loss = 0.14017812\n",
      "Iteration 42, loss = 0.14160432\n",
      "Iteration 43, loss = 0.13968391\n",
      "Iteration 44, loss = 0.13645197\n",
      "Iteration 45, loss = 0.13806121\n",
      "Iteration 46, loss = 0.14075363\n",
      "Iteration 47, loss = 0.13658608\n",
      "Iteration 48, loss = 0.13856133\n",
      "Iteration 49, loss = 0.13707410\n",
      "Iteration 50, loss = 0.13859114\n",
      "Iteration 51, loss = 0.14543834\n",
      "Iteration 52, loss = 0.14188835\n",
      "Iteration 53, loss = 0.13693600\n",
      "Iteration 54, loss = 0.13375499\n",
      "Iteration 55, loss = 0.13525105\n",
      "Iteration 56, loss = 0.13611639\n",
      "Iteration 57, loss = 0.13534475\n",
      "Iteration 58, loss = 0.13350302\n",
      "Iteration 59, loss = 0.13266327\n",
      "Iteration 60, loss = 0.13332094\n",
      "Iteration 61, loss = 0.13461260\n",
      "Iteration 62, loss = 0.13482733\n",
      "Iteration 63, loss = 0.13531282\n",
      "Iteration 64, loss = 0.13523984\n",
      "Iteration 65, loss = 0.13213858\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .........................alpha=0.1;, score=0.581 total time= 4.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67501335\n",
      "Iteration 2, loss = 0.47119074\n",
      "Iteration 3, loss = 0.25407266\n",
      "Iteration 4, loss = 0.13068693\n",
      "Iteration 5, loss = 0.09074118\n",
      "Iteration 6, loss = 0.07259510\n",
      "Iteration 7, loss = 0.06245738\n",
      "Iteration 8, loss = 0.05623104\n",
      "Iteration 9, loss = 0.05159528\n",
      "Iteration 10, loss = 0.04770466\n",
      "Iteration 11, loss = 0.04522594\n",
      "Iteration 12, loss = 0.04302965\n",
      "Iteration 13, loss = 0.04242634\n",
      "Iteration 14, loss = 0.04095443\n",
      "Iteration 15, loss = 0.03954160\n",
      "Iteration 16, loss = 0.03937467\n",
      "Iteration 17, loss = 0.03868617\n",
      "Iteration 18, loss = 0.03728549\n",
      "Iteration 19, loss = 0.03684525\n",
      "Iteration 20, loss = 0.03696367\n",
      "Iteration 21, loss = 0.03665829\n",
      "Iteration 22, loss = 0.03594930\n",
      "Iteration 23, loss = 0.03559408\n",
      "Iteration 24, loss = 0.03557445\n",
      "Iteration 25, loss = 0.03494436\n",
      "Iteration 26, loss = 0.03435305\n",
      "Iteration 27, loss = 0.03467107\n",
      "Iteration 28, loss = 0.03460492\n",
      "Iteration 29, loss = 0.03435554\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ......................alpha=0.0001;, score=0.607 total time= 2.6min\n",
      "Iteration 1, loss = 0.67371457\n",
      "Iteration 2, loss = 0.48935012\n",
      "Iteration 3, loss = 0.32775509\n",
      "Iteration 4, loss = 0.26431547\n",
      "Iteration 5, loss = 0.23660636\n",
      "Iteration 6, loss = 0.21835850\n",
      "Iteration 7, loss = 0.20719000\n",
      "Iteration 8, loss = 0.19840042\n",
      "Iteration 9, loss = 0.19198772\n",
      "Iteration 10, loss = 0.18578517\n",
      "Iteration 11, loss = 0.18104507\n",
      "Iteration 12, loss = 0.17736714\n",
      "Iteration 13, loss = 0.17352131\n",
      "Iteration 14, loss = 0.17266788\n",
      "Iteration 15, loss = 0.16875532\n",
      "Iteration 16, loss = 0.16885759\n",
      "Iteration 17, loss = 0.16606795\n",
      "Iteration 18, loss = 0.16478152\n",
      "Iteration 19, loss = 0.16096326\n",
      "Iteration 20, loss = 0.16081719\n",
      "Iteration 21, loss = 0.15782343\n",
      "Iteration 22, loss = 0.15955248\n",
      "Iteration 23, loss = 0.15846907\n",
      "Iteration 24, loss = 0.15682384\n",
      "Iteration 25, loss = 0.15631944\n",
      "Iteration 26, loss = 0.15499020\n",
      "Iteration 27, loss = 0.15390216\n",
      "Iteration 28, loss = 0.15154590\n",
      "Iteration 29, loss = 0.15040286\n",
      "Iteration 30, loss = 0.15006013\n",
      "Iteration 31, loss = 0.14888573\n",
      "Iteration 32, loss = 0.14932492\n",
      "Iteration 33, loss = 0.15090708\n",
      "Iteration 34, loss = 0.14890146\n",
      "Iteration 35, loss = 0.15121353\n",
      "Iteration 36, loss = 0.14839368\n",
      "Iteration 37, loss = 0.14638393\n",
      "Iteration 38, loss = 0.14759476\n",
      "Iteration 39, loss = 0.14994857\n",
      "Iteration 40, loss = 0.14939271\n",
      "Iteration 41, loss = 0.14635981\n",
      "Iteration 42, loss = 0.14795375\n",
      "Iteration 43, loss = 0.14696338\n",
      "Iteration 44, loss = 0.14694260\n",
      "Iteration 45, loss = 0.14434884\n",
      "Iteration 46, loss = 0.14400022\n",
      "Iteration 47, loss = 0.14297476\n",
      "Iteration 48, loss = 0.14307503\n",
      "Iteration 49, loss = 0.14164797\n",
      "Iteration 50, loss = 0.14279718\n",
      "Iteration 51, loss = 0.14420958\n",
      "Iteration 52, loss = 0.14158399\n",
      "Iteration 53, loss = 0.14118767\n",
      "Iteration 54, loss = 0.14811202\n",
      "Iteration 55, loss = 0.14733860\n",
      "Iteration 56, loss = 0.14291454\n",
      "Iteration 57, loss = 0.13875134\n",
      "Iteration 58, loss = 0.13930095\n",
      "Iteration 59, loss = 0.13854324\n",
      "Iteration 60, loss = 0.13698279\n",
      "Iteration 61, loss = 0.14230603\n",
      "Iteration 62, loss = 0.13907286\n",
      "Iteration 63, loss = 0.13935285\n",
      "Iteration 64, loss = 0.13834688\n",
      "Iteration 65, loss = 0.13816273\n",
      "Iteration 66, loss = 0.13696673\n",
      "Iteration 67, loss = 0.13706496\n",
      "Iteration 68, loss = 0.13803152\n",
      "Iteration 69, loss = 0.14096660\n",
      "Iteration 70, loss = 0.13773016\n",
      "Iteration 71, loss = 0.13572181\n",
      "Iteration 72, loss = 0.13712636\n",
      "Iteration 73, loss = 0.13350557\n",
      "Iteration 74, loss = 0.13739722\n",
      "Iteration 75, loss = 0.13728642\n",
      "Iteration 76, loss = 0.13752921\n",
      "Iteration 77, loss = 0.13838569\n",
      "Iteration 78, loss = 0.13580473\n",
      "Iteration 79, loss = 0.14121900\n",
      "Iteration 80, loss = 0.13917177\n",
      "Iteration 81, loss = 0.13888248\n",
      "Iteration 82, loss = 0.13513361\n",
      "Iteration 83, loss = 0.13582276\n",
      "Iteration 84, loss = 0.13638452\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .........................alpha=0.1;, score=0.605 total time= 5.5min\n"
     ]
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "trigram_train = trigram_vectorizer.fit_transform(formatted_train_tweets)\n",
    "trigram_test = trigram_vectorizer.transform(formatted_test_tweets)\n",
    "\n",
    "mlpc_cv.fit(trigram_train, train_df['target'])\n",
    "print(mlpc_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81a6aee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_trigram_predicted_classes = mlpc_cv.predict(trigram_test)\n",
    "mlpc_trigram_out_array = []\n",
    "for i, pred_class in enumerate(mlpc_trigram_predicted_classes):\n",
    "    mlpc_trigram_out_array.append([int(test_ids[i]), pred_class])\n",
    "    \n",
    "np.savetxt(\"disaster-tweets/mlpc-trigam-results.csv\", mlpc_trigram_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de92e1f",
   "metadata": {},
   "source": [
    "## Humor detection\n",
    "### Important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "510e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032d477",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "\n",
    "We decided to train on only 10 percent of the data, as there are 200,000 data points. Training on, for example, 60 percent of the data would take prohibitively long (more than 10 minutes for one fit). However, evaluation time is more reasonable than training time, so we can still test on the remaining 90 percent of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4daf154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_text = pd.read_csv(\"humor-dataset.csv\")[\"text\"]\n",
    "all_humor = pd.read_csv(\"humor-dataset.csv\")[\"humor\"]\n",
    "\n",
    "text_train, text_test, humor_train, humor_test = train_test_split(all_text, all_humor, train_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e9452",
   "metadata": {},
   "source": [
    "### Making bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dedf7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_train = bigram_vectorizer.fit_transform(text_train)\n",
    "bigram_test = bigram_vectorizer.transform(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc1b38",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "#### Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "249d309f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.9s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.3s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   35.8s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.4s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:  3.6min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:  3.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:   31.6s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:  3.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:  3.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:  3.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n",
      "[CV 2/5] END ..................n_estimators=100;, score=0.838 total time= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  7.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 50\n",
      "building tree 2 of 50\n",
      "building tree 3 of 50\n",
      "building tree 4 of 50\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n",
      "[CV 1/5] END ...................n_estimators=50;, score=0.820 total time= 1.9min\n",
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n",
      "[CV 4/5] END ..................n_estimators=100;, score=0.823 total time= 3.5min\n",
      "building tree 1 of 50\n",
      "building tree 2 of 50\n",
      "building tree 3 of 50\n",
      "building tree 4 of 50\n",
      "building tree 5 of 50\n",
      "building tree 6 of 50\n",
      "building tree 7 of 50\n",
      "building tree 8 of 50\n",
      "building tree 9 of 50\n",
      "building tree 10 of 50\n",
      "building tree 11 of 50\n",
      "building tree 12 of 50\n",
      "building tree 13 of 50\n",
      "building tree 14 of 50\n",
      "building tree 15 of 50\n",
      "building tree 16 of 50\n",
      "building tree 17 of 50\n",
      "building tree 18 of 50\n",
      "building tree 19 of 50\n",
      "building tree 20 of 50\n",
      "building tree 21 of 50\n",
      "building tree 22 of 50\n",
      "building tree 23 of 50\n",
      "building tree 24 of 50\n",
      "building tree 25 of 50\n",
      "building tree 26 of 50\n",
      "building tree 27 of 50\n",
      "building tree 28 of 50\n",
      "building tree 29 of 50\n",
      "building tree 30 of 50\n",
      "building tree 31 of 50\n",
      "building tree 32 of 50\n",
      "building tree 33 of 50\n",
      "building tree 34 of 50\n",
      "building tree 35 of 50\n",
      "building tree 36 of 50\n",
      "building tree 37 of 50\n",
      "building tree 38 of 50\n",
      "building tree 39 of 50\n",
      "building tree 40 of 50\n",
      "building tree 41 of 50\n",
      "building tree 42 of 50\n",
      "building tree 43 of 50\n",
      "building tree 44 of 50\n",
      "building tree 45 of 50\n",
      "building tree 46 of 50\n",
      "building tree 47 of 50\n",
      "building tree 48 of 50\n",
      "building tree 49 of 50\n",
      "building tree 50 of 50\n",
      "[CV 4/5] END ...................n_estimators=50;, score=0.822 total time= 1.9min\n",
      "building tree 1 of 100\n",
      "building tree 2 of 100\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n",
      "[CV 5/5] END ..................n_estimators=100;, score=0.829 total time= 3.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:  6.9min\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed: 12.7min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed: 12.8min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed: 12.8min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed: 11.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed: 11.5min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 500building tree 2 of 500\n",
      "\n",
      "building tree 3 of 500\n",
      "building tree 4 of 500\n",
      "building tree 5 of 500building tree 6 of 500building tree 7 of 500\n",
      "\n",
      "building tree 8 of 500\n",
      "\n",
      "building tree 9 of 500\n",
      "building tree 10 of 500\n",
      "building tree 11 of 500\n",
      "building tree 12 of 500\n",
      "building tree 13 of 500\n",
      "building tree 14 of 500\n",
      "building tree 15 of 500\n",
      "building tree 16 of 500\n",
      "building tree 17 of 500\n",
      "building tree 18 of 500\n",
      "building tree 19 of 500\n",
      "building tree 20 of 500\n",
      "building tree 21 of 500\n",
      "building tree 22 of 500\n",
      "building tree 23 of 500\n",
      "building tree 24 of 500\n",
      "building tree 25 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 26 of 500\n",
      "building tree 27 of 500\n",
      "building tree 28 of 500\n",
      "building tree 29 of 500\n",
      "building tree 30 of 500\n",
      "building tree 31 of 500\n",
      "building tree 32 of 500\n",
      "building tree 33 of 500\n",
      "building tree 34 of 500\n",
      "building tree 35 of 500\n",
      "building tree 36 of 500\n",
      "building tree 37 of 500\n",
      "building tree 38 of 500\n",
      "building tree 39 of 500\n",
      "building tree 40 of 500\n",
      "building tree 41 of 500\n",
      "building tree 42 of 500\n",
      "building tree 43 of 500\n",
      "building tree 44 of 500\n",
      "building tree 45 of 500\n",
      "building tree 46 of 500\n",
      "building tree 47 of 500\n",
      "building tree 48 of 500\n",
      "building tree 49 of 500\n",
      "building tree 50 of 500\n",
      "building tree 51 of 500\n",
      "building tree 52 of 500\n",
      "building tree 53 of 500\n",
      "building tree 54 of 500\n",
      "building tree 55 of 500\n",
      "building tree 56 of 500\n",
      "building tree 57 of 500\n",
      "building tree 58 of 500\n",
      "building tree 59 of 500\n",
      "building tree 60 of 500\n",
      "building tree 61 of 500\n",
      "building tree 62 of 500\n",
      "building tree 63 of 500\n",
      "building tree 64 of 500\n",
      "building tree 65 of 500\n",
      "building tree 66 of 500\n",
      "building tree 67 of 500\n",
      "building tree 68 of 500\n",
      "building tree 69 of 500\n",
      "building tree 70 of 500\n",
      "building tree 71 of 500\n",
      "building tree 72 of 500\n",
      "building tree 73 of 500\n",
      "building tree 74 of 500\n",
      "building tree 75 of 500\n",
      "building tree 76 of 500\n",
      "building tree 77 of 500\n",
      "building tree 78 of 500\n",
      "building tree 79 of 500\n",
      "building tree 80 of 500\n",
      "building tree 81 of 500\n",
      "building tree 82 of 500\n",
      "building tree 83 of 500\n",
      "building tree 84 of 500\n",
      "building tree 85 of 500\n",
      "building tree 86 of 500\n",
      "building tree 87 of 500\n",
      "building tree 88 of 500\n",
      "building tree 89 of 500\n",
      "building tree 90 of 500\n",
      "building tree 91 of 500\n",
      "building tree 92 of 500\n",
      "building tree 93 of 500\n",
      "building tree 94 of 500\n",
      "building tree 95 of 500\n",
      "building tree 96 of 500\n",
      "building tree 97 of 500\n",
      "building tree 98 of 500\n",
      "building tree 99 of 500\n",
      "building tree 100 of 500\n",
      "building tree 101 of 500\n",
      "building tree 102 of 500\n",
      "building tree 103 of 500\n",
      "building tree 104 of 500\n",
      "building tree 105 of 500\n",
      "building tree 106 of 500\n",
      "building tree 107 of 500\n",
      "building tree 108 of 500\n",
      "building tree 109 of 500\n",
      "building tree 110 of 500\n",
      "building tree 111 of 500\n",
      "building tree 112 of 500\n",
      "building tree 113 of 500\n",
      "building tree 114 of 500\n",
      "building tree 115 of 500\n",
      "building tree 116 of 500\n",
      "building tree 117 of 500\n",
      "building tree 118 of 500\n",
      "building tree 119 of 500\n",
      "building tree 120 of 500\n",
      "building tree 121 of 500\n",
      "building tree 122 of 500\n",
      "building tree 123 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:    3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 124 of 500\n",
      "building tree 125 of 500\n",
      "building tree 126 of 500\n",
      "building tree 127 of 500\n",
      "building tree 128 of 500\n",
      "building tree 129 of 500\n",
      "building tree 130 of 500\n",
      "building tree 131 of 500\n",
      "building tree 132 of 500\n",
      "building tree 133 of 500\n",
      "building tree 134 of 500\n",
      "building tree 135 of 500\n",
      "building tree 136 of 500\n",
      "building tree 137 of 500\n",
      "building tree 138 of 500\n",
      "building tree 139 of 500\n",
      "building tree 140 of 500\n",
      "building tree 141 of 500\n",
      "building tree 142 of 500\n",
      "building tree 143 of 500\n",
      "building tree 144 of 500\n",
      "building tree 145 of 500\n",
      "building tree 146 of 500\n",
      "building tree 147 of 500\n",
      "building tree 148 of 500\n",
      "building tree 149 of 500\n",
      "building tree 150 of 500\n",
      "building tree 151 of 500\n",
      "building tree 152 of 500\n",
      "building tree 153 of 500\n",
      "building tree 154 of 500\n",
      "building tree 155 of 500\n",
      "building tree 156 of 500\n",
      "building tree 157 of 500\n",
      "building tree 158 of 500\n",
      "building tree 159 of 500\n",
      "building tree 160 of 500\n",
      "building tree 161 of 500\n",
      "building tree 162 of 500\n",
      "building tree 163 of 500\n",
      "building tree 164 of 500\n",
      "building tree 165 of 500\n",
      "building tree 166 of 500\n",
      "building tree 167 of 500\n",
      "building tree 168 of 500\n",
      "building tree 169 of 500\n",
      "building tree 170 of 500\n",
      "building tree 171 of 500\n",
      "building tree 172 of 500\n",
      "building tree 173 of 500\n",
      "building tree 174 of 500\n",
      "building tree 175 of 500\n",
      "building tree 176 of 500\n",
      "building tree 177 of 500\n",
      "building tree 178 of 500\n",
      "building tree 179 of 500\n",
      "building tree 180 of 500building tree 181 of 500\n",
      "\n",
      "building tree 182 of 500\n",
      "building tree 183 of 500\n",
      "building tree 184 of 500\n",
      "building tree 185 of 500\n",
      "building tree 186 of 500\n",
      "building tree 187 of 500\n",
      "building tree 188 of 500\n",
      "building tree 189 of 500\n",
      "building tree 190 of 500\n",
      "building tree 191 of 500\n",
      "building tree 192 of 500\n",
      "building tree 193 of 500\n",
      "building tree 194 of 500\n",
      "building tree 195 of 500\n",
      "building tree 196 of 500\n",
      "building tree 197 of 500\n",
      "building tree 198 of 500\n",
      "building tree 199 of 500\n",
      "building tree 200 of 500\n",
      "building tree 201 of 500\n",
      "building tree 202 of 500\n",
      "building tree 203 of 500\n",
      "building tree 204 of 500\n",
      "building tree 205 of 500\n",
      "building tree 206 of 500\n",
      "building tree 207 of 500\n",
      "building tree 208 of 500\n",
      "building tree 209 of 500\n",
      "building tree 210 of 500\n",
      "building tree 211 of 500\n",
      "building tree 212 of 500\n",
      "building tree 213 of 500\n",
      "building tree 214 of 500\n",
      "building tree 215 of 500\n",
      "building tree 216 of 500\n",
      "building tree 217 of 500\n",
      "building tree 218 of 500\n",
      "building tree 219 of 500\n",
      "building tree 220 of 500\n",
      "building tree 221 of 500\n",
      "building tree 222 of 500\n",
      "building tree 223 of 500\n",
      "building tree 224 of 500\n",
      "building tree 225 of 500\n",
      "building tree 226 of 500\n",
      "building tree 227 of 500\n",
      "building tree 228 of 500\n",
      "building tree 229 of 500\n",
      "building tree 230 of 500\n",
      "building tree 231 of 500\n",
      "building tree 232 of 500\n",
      "building tree 233 of 500\n",
      "building tree 234 of 500\n",
      "building tree 235 of 500\n",
      "building tree 236 of 500\n",
      "building tree 237 of 500\n",
      "building tree 238 of 500\n",
      "building tree 239 of 500\n",
      "building tree 240 of 500\n",
      "building tree 241 of 500\n",
      "building tree 242 of 500\n",
      "building tree 243 of 500\n",
      "building tree 244 of 500\n",
      "building tree 245 of 500\n",
      "building tree 246 of 500\n",
      "building tree 247 of 500\n",
      "building tree 248 of 500\n",
      "building tree 249 of 500\n",
      "building tree 250 of 500\n",
      "building tree 251 of 500\n",
      "building tree 252 of 500\n",
      "building tree 253 of 500\n",
      "building tree 254 of 500\n",
      "building tree 255 of 500\n",
      "building tree 256 of 500\n",
      "building tree 257 of 500\n",
      "building tree 258 of 500\n",
      "building tree 259 of 500\n",
      "building tree 260 of 500\n",
      "building tree 261 of 500\n",
      "building tree 262 of 500\n",
      "building tree 263 of 500\n",
      "building tree 264 of 500\n",
      "building tree 265 of 500\n",
      "building tree 266 of 500\n",
      "building tree 267 of 500\n",
      "building tree 268 of 500\n",
      "building tree 269 of 500\n",
      "building tree 270 of 500\n",
      "building tree 271 of 500\n",
      "building tree 272 of 500\n",
      "building tree 273 of 500\n",
      "building tree 274 of 500\n",
      "building tree 275 of 500\n",
      "building tree 276 of 500\n",
      "building tree 277 of 500\n",
      "building tree 278 of 500\n",
      "building tree 279 of 500\n",
      "building tree 280 of 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:    7.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 281 of 500\n",
      "building tree 282 of 500\n",
      "building tree 283 of 500\n",
      "building tree 284 of 500\n",
      "building tree 285 of 500\n",
      "building tree 286 of 500\n",
      "building tree 287 of 500\n",
      "building tree 288 of 500\n",
      "building tree 289 of 500\n",
      "building tree 290 of 500\n",
      "building tree 291 of 500\n",
      "building tree 292 of 500\n",
      "building tree 293 of 500\n",
      "building tree 294 of 500\n",
      "building tree 295 of 500\n",
      "building tree 296 of 500\n",
      "building tree 297 of 500\n",
      "building tree 298 of 500\n",
      "building tree 299 of 500\n",
      "building tree 300 of 500\n",
      "building tree 301 of 500\n",
      "building tree 302 of 500\n",
      "building tree 303 of 500\n",
      "building tree 304 of 500\n",
      "building tree 305 of 500\n",
      "building tree 306 of 500\n",
      "building tree 307 of 500\n",
      "building tree 308 of 500\n",
      "building tree 309 of 500\n",
      "building tree 310 of 500\n",
      "building tree 311 of 500\n",
      "building tree 312 of 500\n",
      "building tree 313 of 500\n",
      "building tree 314 of 500\n",
      "building tree 315 of 500\n",
      "building tree 316 of 500\n",
      "building tree 317 of 500\n",
      "building tree 318 of 500\n",
      "building tree 319 of 500\n",
      "building tree 320 of 500\n",
      "building tree 321 of 500\n",
      "building tree 322 of 500\n",
      "building tree 323 of 500\n",
      "building tree 324 of 500\n",
      "building tree 325 of 500\n",
      "building tree 326 of 500\n",
      "building tree 327 of 500\n",
      "building tree 328 of 500\n",
      "building tree 329 of 500\n",
      "building tree 330 of 500\n",
      "building tree 331 of 500\n",
      "building tree 332 of 500\n",
      "building tree 333 of 500\n",
      "building tree 334 of 500\n",
      "building tree 335 of 500\n",
      "building tree 336 of 500\n",
      "building tree 337 of 500\n",
      "building tree 338 of 500\n",
      "building tree 339 of 500\n",
      "building tree 340 of 500\n",
      "building tree 341 of 500\n",
      "building tree 342 of 500\n",
      "building tree 343 of 500\n",
      "building tree 344 of 500\n",
      "building tree 345 of 500\n",
      "building tree 346 of 500\n",
      "building tree 347 of 500\n",
      "building tree 348 of 500\n",
      "building tree 349 of 500\n",
      "building tree 350 of 500\n",
      "building tree 351 of 500\n",
      "building tree 352 of 500\n",
      "building tree 353 of 500\n",
      "building tree 354 of 500\n",
      "building tree 355 of 500\n",
      "building tree 356 of 500\n",
      "building tree 357 of 500\n",
      "building tree 358 of 500\n",
      "building tree 359 of 500\n",
      "building tree 360 of 500\n",
      "building tree 361 of 500\n",
      "building tree 362 of 500\n",
      "building tree 363 of 500\n",
      "building tree 364 of 500\n",
      "building tree 365 of 500\n",
      "building tree 366 of 500\n",
      "building tree 367 of 500\n",
      "building tree 368 of 500\n",
      "building tree 369 of 500\n",
      "building tree 370 of 500\n",
      "building tree 371 of 500\n",
      "building tree 372 of 500\n",
      "building tree 373 of 500\n",
      "building tree 374 of 500\n",
      "building tree 375 of 500\n",
      "building tree 376 of 500\n",
      "building tree 377 of 500\n",
      "building tree 378 of 500\n",
      "building tree 379 of 500\n",
      "building tree 380 of 500\n",
      "building tree 381 of 500\n",
      "building tree 382 of 500\n",
      "building tree 383 of 500\n",
      "building tree 384 of 500\n",
      "building tree 385 of 500\n",
      "building tree 386 of 500\n",
      "building tree 387 of 500\n",
      "building tree 388 of 500\n",
      "building tree 389 of 500\n",
      "building tree 390 of 500\n",
      "building tree 391 of 500\n",
      "building tree 392 of 500\n",
      "building tree 393 of 500\n",
      "building tree 394 of 500\n",
      "building tree 395 of 500\n",
      "building tree 396 of 500\n",
      "building tree 397 of 500\n",
      "building tree 398 of 500\n",
      "building tree 399 of 500\n",
      "building tree 400 of 500\n",
      "building tree 401 of 500\n",
      "building tree 402 of 500\n",
      "building tree 403 of 500\n",
      "building tree 404 of 500\n",
      "building tree 405 of 500\n",
      "building tree 406 of 500\n",
      "building tree 407 of 500\n",
      "building tree 408 of 500\n",
      "building tree 409 of 500\n",
      "building tree 410 of 500\n",
      "building tree 411 of 500\n",
      "building tree 412 of 500\n",
      "building tree 413 of 500\n",
      "building tree 414 of 500\n",
      "building tree 415 of 500\n",
      "building tree 416 of 500\n",
      "building tree 417 of 500\n",
      "building tree 418 of 500\n",
      "building tree 419 of 500\n",
      "building tree 420 of 500\n",
      "building tree 421 of 500\n",
      "building tree 422 of 500\n",
      "building tree 423 of 500\n",
      "building tree 424 of 500\n",
      "building tree 425 of 500\n",
      "building tree 426 of 500\n",
      "building tree 427 of 500\n",
      "building tree 428 of 500\n",
      "building tree 429 of 500\n",
      "building tree 430 of 500\n",
      "building tree 431 of 500\n",
      "building tree 432 of 500\n",
      "building tree 433 of 500\n",
      "building tree 434 of 500\n",
      "building tree 435 of 500\n",
      "building tree 436 of 500\n",
      "building tree 437 of 500\n",
      "building tree 438 of 500\n",
      "building tree 439 of 500\n",
      "building tree 440 of 500\n",
      "building tree 441 of 500\n",
      "building tree 442 of 500\n",
      "building tree 443 of 500\n",
      "building tree 444 of 500\n",
      "building tree 445 of 500\n",
      "building tree 446 of 500\n",
      "building tree 447 of 500\n",
      "building tree 448 of 500\n",
      "building tree 449 of 500\n",
      "building tree 450 of 500\n",
      "building tree 451 of 500\n",
      "building tree 452 of 500\n",
      "building tree 453 of 500\n",
      "building tree 454 of 500\n",
      "building tree 455 of 500\n",
      "building tree 456 of 500\n",
      "building tree 457 of 500\n",
      "building tree 458 of 500\n",
      "building tree 459 of 500\n",
      "building tree 460 of 500\n",
      "building tree 461 of 500\n",
      "building tree 462 of 500\n",
      "building tree 463 of 500\n",
      "building tree 464 of 500\n",
      "building tree 465 of 500\n",
      "building tree 466 of 500\n",
      "building tree 467 of 500\n",
      "building tree 468 of 500\n",
      "building tree 469 of 500\n",
      "building tree 470 of 500\n",
      "building tree 471 of 500\n",
      "building tree 472 of 500\n",
      "building tree 473 of 500\n",
      "building tree 474 of 500\n",
      "building tree 475 of 500\n",
      "building tree 476 of 500\n",
      "building tree 477 of 500\n",
      "building tree 478 of 500\n",
      "building tree 479 of 500\n",
      "building tree 480 of 500\n",
      "building tree 481 of 500\n",
      "building tree 482 of 500\n",
      "building tree 483 of 500\n",
      "building tree 484 of 500\n",
      "building tree 485 of 500\n",
      "building tree 486 of 500\n",
      "building tree 487 of 500\n",
      "building tree 488 of 500\n",
      "building tree 489 of 500\n",
      "building tree 490 of 500\n",
      "building tree 491 of 500\n",
      "building tree 492 of 500\n",
      "building tree 493 of 500\n",
      "building tree 494 of 500\n",
      "building tree 495 of 500\n",
      "building tree 496 of 500\n",
      "building tree 497 of 500\n",
      "building tree 498 of 500\n",
      "building tree 499 of 500\n",
      "building tree 500 of 500\n",
      "{'n_estimators': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:   13.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=None, class_weight=\"balanced\", n_jobs=8, verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aed44e",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [50, 100, 500]\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(rf, rf_params, n_jobs=8, verbose=3)\n",
    "rf_cv.fit(bigram_train, humor_train)\n",
    "print(rf_cv.best_params_)\n",
    "# {'n_estimators': 100}: 0.8332"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c83a27b",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The humor dataset, unlike the disaster tweets dataset, included labels for every piece of text in the dataset. As such, we are able to directly compute accuracy/F1 scores and also to plot confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4d5f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_cm(preds):\n",
    "    return confusion_matrix(humor_test, preds, labels=[False, True], normalize='all')\n",
    "\n",
    "def plot_confusion_matrix(cm, title):\n",
    "    fx = sb.heatmap(cm, annot=True, cmap='turbo')\n",
    "\n",
    "    # labels the title and x, y axis of plot\n",
    "    fx.set_title(title + '\\n\\n');\n",
    "    fx.set_xlabel('Predicted Values')\n",
    "    fx.set_ylabel('Actual Values ');\n",
    "\n",
    "    # labels the boxes\n",
    "    fx.xaxis.set_ticklabels(['False','True'])\n",
    "    fx.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7607f",
   "metadata": {},
   "source": [
    "#### Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a52be7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  16 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done 112 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=8)]: Done 272 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=8)]: Done 500 out of 500 | elapsed:   35.0s finished\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'rf_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/max/Library/Mobile Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine Learning/Final Project/comp-562-final-project/COMP 562 Final Project.ipynb Cell 45'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/max/Library/Mobile%20Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine%20Learning/Final%20Project/comp-562-final-project/COMP%20562%20Final%20Project.ipynb#ch0000024?line=1'>2</a>\u001b[0m rf_cm \u001b[39m=\u001b[39m get_cm(humor_preds)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/max/Library/Mobile%20Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine%20Learning/Final%20Project/comp-562-final-project/COMP%20562%20Final%20Project.ipynb#ch0000024?line=2'>3</a>\u001b[0m plot_confusion_matrix(rf_cm, \u001b[39m\"\u001b[39m\u001b[39mRandom forest with bigrams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/max/Library/Mobile%20Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine%20Learning/Final%20Project/comp-562-final-project/COMP%20562%20Final%20Project.ipynb#ch0000024?line=3'>4</a>\u001b[0m rf_cv\u001b[39m.\u001b[39mscore(bigram_test, humor_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rf_cv' is not defined"
     ]
    }
   ],
   "source": [
    "humor_preds = rf.predict(bigram_test)\n",
    "rf_cm = get_cm(humor_preds)\n",
    "plot_confusion_matrix(rf_cm, \"Random forest with bigrams\")\n",
    "rf_cv.score(bigram_test, humor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e7ba8",
   "metadata": {},
   "source": [
    "### Multilayer perceptron classifier\n",
    "#### Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlpc = MLPClassifier(tol=.001, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bcd59",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d6df991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CV...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Iteration 1, loss = 0.55270584\n",
      "Iteration 1, loss = 0.55314522\n",
      "Iteration 1, loss = 0.54498469\n",
      "Iteration 1, loss = 0.54842985\n",
      "Iteration 1, loss = 0.55090679\n",
      "Iteration 1, loss = 0.54417146\n",
      "Iteration 1, loss = 0.54949766\n",
      "Iteration 1, loss = 0.54595447\n",
      "Iteration 2, loss = 0.13707193\n",
      "Iteration 2, loss = 0.14322471\n",
      "Iteration 2, loss = 0.15014362\n",
      "Iteration 2, loss = 0.14126301\n",
      "Iteration 2, loss = 0.14208745\n",
      "Iteration 2, loss = 0.14655035\n",
      "Iteration 2, loss = 0.14494800\n",
      "Iteration 2, loss = 0.14528933\n",
      "Iteration 3, loss = 0.03362346\n",
      "Iteration 3, loss = 0.02917841\n",
      "Iteration 3, loss = 0.03521583\n",
      "Iteration 3, loss = 0.03116114\n",
      "Iteration 3, loss = 0.03070502\n",
      "Iteration 3, loss = 0.03070561\n",
      "Iteration 3, loss = 0.03500117\n",
      "Iteration 3, loss = 0.02960893\n",
      "Iteration 4, loss = 0.01250560\n",
      "Iteration 4, loss = 0.01792308\n",
      "Iteration 4, loss = 0.01738343\n",
      "Iteration 4, loss = 0.01319112\n",
      "Iteration 4, loss = 0.01304526\n",
      "Iteration 4, loss = 0.01762028\n",
      "Iteration 4, loss = 0.01293304\n",
      "Iteration 4, loss = 0.01267102\n",
      "Iteration 5, loss = 0.01276538\n",
      "Iteration 5, loss = 0.00739916\n",
      "Iteration 5, loss = 0.00771797\n",
      "Iteration 5, loss = 0.01239305\n",
      "Iteration 5, loss = 0.01252534\n",
      "Iteration 5, loss = 0.00763348\n",
      "Iteration 5, loss = 0.00769436\n",
      "Iteration 5, loss = 0.00749404\n",
      "Iteration 6, loss = 0.01051334\n",
      "Iteration 6, loss = 0.00528780\n",
      "Iteration 6, loss = 0.00512075\n",
      "Iteration 6, loss = 0.01020602\n",
      "Iteration 6, loss = 0.01028034\n",
      "Iteration 6, loss = 0.00528809\n",
      "Iteration 6, loss = 0.00516584\n",
      "Iteration 6, loss = 0.00526180\n",
      "Iteration 7, loss = 0.00928819\n",
      "Iteration 7, loss = 0.00398212\n",
      "Iteration 7, loss = 0.00387598\n",
      "Iteration 7, loss = 0.00900995\n",
      "Iteration 7, loss = 0.00906669\n",
      "Iteration 7, loss = 0.00398003\n",
      "Iteration 7, loss = 0.00396653\n",
      "Iteration 7, loss = 0.00389329\n",
      "Iteration 8, loss = 0.00852099\n",
      "Iteration 8, loss = 0.00826264\n",
      "Iteration 8, loss = 0.00318771\n",
      "Iteration 8, loss = 0.00311674\n",
      "Iteration 8, loss = 0.00831028\n",
      "Iteration 8, loss = 0.00318305\n",
      "Iteration 8, loss = 0.00312040\n",
      "Iteration 8, loss = 0.00317565\n",
      "Iteration 9, loss = 0.00798638\n",
      "Iteration 9, loss = 0.00774377\n",
      "Iteration 9, loss = 0.00267023\n",
      "Iteration 9, loss = 0.00261681\n",
      "Iteration 9, loss = 0.00778489\n",
      "Iteration 9, loss = 0.00266025\n",
      "Iteration 9, loss = 0.00261062\n",
      "Iteration 9, loss = 0.00265678\n",
      "Iteration 10, loss = 0.00758206\n",
      "Iteration 10, loss = 0.00735577\n",
      "Iteration 10, loss = 0.00226877\n",
      "Iteration 10, loss = 0.00231191\n",
      "Iteration 10, loss = 0.00738978\n",
      "Iteration 10, loss = 0.00229927\n",
      "Iteration 10, loss = 0.00229822\n",
      "Iteration 10, loss = 0.00225763\n",
      "Iteration 11, loss = 0.00725519\n",
      "Iteration 11, loss = 0.00704018\n",
      "Iteration 11, loss = 0.00201853\n",
      "Iteration 11, loss = 0.00707209\n",
      "Iteration 11, loss = 0.00205319\n",
      "Iteration 11, loss = 0.00203921\n",
      "Iteration 11, loss = 0.00200249\n",
      "Iteration 11, loss = 0.00203910\n",
      "Iteration 12, loss = 0.00697894\n",
      "Iteration 12, loss = 0.00680403\n",
      "Iteration 12, loss = 0.00677678\n",
      "Iteration 12, loss = 0.00183118\n",
      "Iteration 12, loss = 0.00184577\n",
      "Iteration 12, loss = 0.00186180\n",
      "Iteration 12, loss = 0.00184515\n",
      "Iteration 12, loss = 0.00181243\n",
      "Iteration 13, loss = 0.00673435\n",
      "Iteration 13, loss = 0.00656926\n",
      "Iteration 13, loss = 0.00168777\n",
      "Iteration 13, loss = 0.00654247\n",
      "Iteration 13, loss = 0.00171493\n",
      "Iteration 13, loss = 0.00169725\n",
      "Iteration 13, loss = 0.00169823\n",
      "Iteration 13, loss = 0.00166694\n",
      "Iteration 14, loss = 0.00651391\n",
      "Iteration 14, loss = 0.00635659\n",
      "Iteration 14, loss = 0.00633388\n",
      "Iteration 14, loss = 0.00157465\n",
      "Iteration 14, loss = 0.00160030\n",
      "Iteration 14, loss = 0.00158239\n",
      "Iteration 14, loss = 0.00158112\n",
      "Iteration 14, loss = 0.00155301\n",
      "Iteration 15, loss = 0.00631081\n",
      "Iteration 15, loss = 0.00616170\n",
      "Iteration 15, loss = 0.00148433\n",
      "Iteration 15, loss = 0.00150866\n",
      "Iteration 15, loss = 0.00613937\n",
      "Iteration 15, loss = 0.00149037\n",
      "Iteration 15, loss = 0.00148880\n",
      "Iteration 15, loss = 0.00146186\n",
      "Iteration 16, loss = 0.00612087\n",
      "Iteration 16, loss = 0.00141068\n",
      "Iteration 16, loss = 0.00597935\n",
      "Iteration 16, loss = 0.00143452\n",
      "Iteration 16, loss = 0.00595991\n",
      "Iteration 16, loss = 0.00141546\n",
      "Iteration 16, loss = 0.00138816\n",
      "Iteration 16, loss = 0.00141374\n",
      "Iteration 17, loss = 0.00594318\n",
      "Iteration 17, loss = 0.00580879\n",
      "Iteration 17, loss = 0.00134984\n",
      "Iteration 17, loss = 0.00137316\n",
      "Iteration 17, loss = 0.00579060\n",
      "Iteration 17, loss = 0.00132712\n",
      "Iteration 17, loss = 0.00135182\n",
      "Iteration 17, loss = 0.00135380\n",
      "Iteration 18, loss = 0.00577486\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .......................alpha=0.001;, score=0.883 total time= 6.6min\n",
      "Iteration 18, loss = 0.00564696\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .......................alpha=0.001;, score=0.878 total time= 6.6min\n",
      "Iteration 18, loss = 0.00129847\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ......................alpha=0.0001;, score=0.880 total time= 6.6min\n",
      "Iteration 18, loss = 0.00132182\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.00562953\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ......................alpha=0.0001;, score=0.884 total time= 6.6min\n",
      "[CV 1/5] END .......................alpha=0.001;, score=0.885 total time= 6.6min\n",
      "Iteration 18, loss = 0.00127609\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ......................alpha=0.0001;, score=0.889 total time= 6.6min\n",
      "Iteration 18, loss = 0.00130243\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.00130015\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ......................alpha=0.0001;, score=0.890 total time= 6.6min\n",
      "[CV 4/5] END ......................alpha=0.0001;, score=0.875 total time= 6.6min\n",
      "Iteration 1, loss = 0.54108451\n",
      "Iteration 1, loss = 0.55082509\n",
      "Iteration 1, loss = 0.55516595\n",
      "Iteration 1, loss = 0.55449752\n",
      "Iteration 1, loss = 0.55191049\n",
      "Iteration 1, loss = 0.58628199\n",
      "Iteration 1, loss = 0.55903222\n",
      "Iteration 1, loss = 0.55478994\n",
      "Iteration 2, loss = 0.13825583\n",
      "Iteration 2, loss = 0.16852293\n",
      "Iteration 2, loss = 0.14473702\n",
      "Iteration 2, loss = 0.17358865\n",
      "Iteration 2, loss = 0.32446819\n",
      "Iteration 2, loss = 0.16686486\n",
      "Iteration 2, loss = 0.17453237\n",
      "Iteration 2, loss = 0.18112782\n",
      "Iteration 3, loss = 0.03361557\n",
      "Iteration 3, loss = 0.06790229\n",
      "Iteration 3, loss = 0.06965097\n",
      "Iteration 3, loss = 0.03408491\n",
      "Iteration 3, loss = 0.23496077\n",
      "Iteration 3, loss = 0.06630938\n",
      "Iteration 3, loss = 0.07096504\n",
      "Iteration 3, loss = 0.07098921\n",
      "Iteration 4, loss = 0.05084078\n",
      "Iteration 4, loss = 0.01725193\n",
      "Iteration 4, loss = 0.05163179\n",
      "Iteration 4, loss = 0.01750784\n",
      "Iteration 4, loss = 0.04973399\n",
      "Iteration 4, loss = 0.20495359\n",
      "Iteration 4, loss = 0.05288189\n",
      "Iteration 4, loss = 0.05244434\n",
      "Iteration 5, loss = 0.04408405\n",
      "Iteration 5, loss = 0.01232030\n",
      "Iteration 5, loss = 0.04470011\n",
      "Iteration 5, loss = 0.01260009\n",
      "Iteration 5, loss = 0.04315910\n",
      "Iteration 5, loss = 0.18931293\n",
      "Iteration 5, loss = 0.04581918\n",
      "Iteration 5, loss = 0.04533036\n",
      "Iteration 6, loss = 0.03986385\n",
      "Iteration 6, loss = 0.04036172\n",
      "Iteration 6, loss = 0.01014877\n",
      "Iteration 6, loss = 0.01042410\n",
      "Iteration 6, loss = 0.03905722\n",
      "Iteration 6, loss = 0.04140544\n",
      "Iteration 6, loss = 0.18160554\n",
      "Iteration 6, loss = 0.04090405\n",
      "Iteration 7, loss = 0.03672639\n",
      "Iteration 7, loss = 0.03719372\n",
      "Iteration 7, loss = 0.00896793\n",
      "Iteration 7, loss = 0.00923638\n",
      "Iteration 7, loss = 0.03601964\n",
      "Iteration 7, loss = 0.17474746\n",
      "Iteration 7, loss = 0.03764552\n",
      "Iteration 7, loss = 0.03814461\n",
      "Iteration 8, loss = 0.03425163\n",
      "Iteration 8, loss = 0.03470528\n",
      "Iteration 8, loss = 0.00849215\n",
      "Iteration 8, loss = 0.00822864\n",
      "Iteration 8, loss = 0.03360990\n",
      "Iteration 8, loss = 0.16991761\n",
      "Iteration 8, loss = 0.03558949\n",
      "Iteration 8, loss = 0.03508084\n",
      "Iteration 9, loss = 0.03224160\n",
      "Iteration 9, loss = 0.00797063\n",
      "Iteration 9, loss = 0.03264612\n",
      "Iteration 9, loss = 0.00771753\n",
      "Iteration 9, loss = 0.03162263\n",
      "Iteration 9, loss = 0.03345831\n",
      "Iteration 9, loss = 0.16506067\n",
      "Iteration 9, loss = 0.03299536\n",
      "Iteration 10, loss = 0.00757264\n",
      "Iteration 10, loss = 0.03094141\n",
      "Iteration 10, loss = 0.03053597\n",
      "Iteration 10, loss = 0.02995248\n",
      "Iteration 10, loss = 0.00733157\n",
      "Iteration 10, loss = 0.03128946\n",
      "Iteration 10, loss = 0.16268973\n",
      "Iteration 10, loss = 0.03173682\n",
      "Iteration 11, loss = 0.02911825\n",
      "Iteration 11, loss = 0.00725041\n",
      "Iteration 11, loss = 0.02952595\n",
      "Iteration 11, loss = 0.02858746\n",
      "Iteration 11, loss = 0.00702297\n",
      "Iteration 11, loss = 0.03028560\n",
      "Iteration 11, loss = 0.16169869\n",
      "Iteration 11, loss = 0.02986746\n",
      "Iteration 12, loss = 0.02792559\n",
      "Iteration 12, loss = 0.00697559\n",
      "Iteration 12, loss = 0.02829831\n",
      "Iteration 12, loss = 0.02737360\n",
      "Iteration 12, loss = 0.00676243\n",
      "Iteration 12, loss = 0.02902252\n",
      "Iteration 12, loss = 0.15890462\n",
      "Iteration 12, loss = 0.02862860\n",
      "Iteration 13, loss = 0.02690668\n",
      "Iteration 13, loss = 0.00673237\n",
      "Iteration 13, loss = 0.02728514\n",
      "Iteration 13, loss = 0.02638257\n",
      "Iteration 13, loss = 0.00653268\n",
      "Iteration 13, loss = 0.02798495\n",
      "Iteration 13, loss = 0.15744473\n",
      "Iteration 13, loss = 0.02754457\n",
      "Iteration 14, loss = 0.00651267\n",
      "Iteration 14, loss = 0.02599217\n",
      "Iteration 14, loss = 0.02641381\n",
      "Iteration 14, loss = 0.02552316\n",
      "Iteration 14, loss = 0.00632542\n",
      "Iteration 14, loss = 0.15399283\n",
      "Iteration 14, loss = 0.02674209\n",
      "Iteration 14, loss = 0.02711523\n",
      "Iteration 15, loss = 0.02528265\n",
      "Iteration 15, loss = 0.00630933\n",
      "Iteration 15, loss = 0.02564626\n",
      "Iteration 15, loss = 0.02480502\n",
      "Iteration 15, loss = 0.00613350\n",
      "Iteration 15, loss = 0.02593336\n",
      "Iteration 15, loss = 0.15254724\n",
      "Iteration 15, loss = 0.02632863\n",
      "Iteration 16, loss = 0.02465156\n",
      "Iteration 16, loss = 0.00611997\n",
      "Iteration 16, loss = 0.02499656\n",
      "Iteration 16, loss = 0.02422350\n",
      "Iteration 16, loss = 0.00595530\n",
      "Iteration 16, loss = 0.02523464\n",
      "Iteration 16, loss = 0.15048701\n",
      "Iteration 16, loss = 0.02564088\n",
      "Iteration 17, loss = 0.02405360\n",
      "Iteration 17, loss = 0.02446625\n",
      "Iteration 17, loss = 0.00594164\n",
      "Iteration 17, loss = 0.02362852\n",
      "Iteration 17, loss = 0.00578746\n",
      "Iteration 17, loss = 0.15029313\n",
      "Iteration 17, loss = 0.02474999\n",
      "Iteration 17, loss = 0.02515093\n",
      "Iteration 18, loss = 0.02362826\n",
      "Iteration 18, loss = 0.02402777\n",
      "Iteration 18, loss = 0.00577356\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .......................alpha=0.001;, score=0.873 total time= 6.7min\n",
      "Iteration 18, loss = 0.02326127\n",
      "Iteration 18, loss = 0.00562760\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .......................alpha=0.001;, score=0.867 total time= 6.7min\n",
      "Iteration 18, loss = 0.02428291\n",
      "Iteration 18, loss = 0.15018854\n",
      "Iteration 18, loss = 0.02462916\n",
      "Iteration 19, loss = 0.02336416\n",
      "Iteration 19, loss = 0.02373382\n",
      "Iteration 1, loss = 0.57902291\n",
      "Iteration 19, loss = 0.02299387\n",
      "Iteration 1, loss = 0.58730448\n",
      "Iteration 19, loss = 0.02396699\n",
      "Iteration 19, loss = 0.15003891\n",
      "Iteration 19, loss = 0.02426064\n",
      "Iteration 20, loss = 0.02312735\n",
      "Iteration 20, loss = 0.02338909\n",
      "Iteration 2, loss = 0.31466940\n",
      "Iteration 20, loss = 0.02261488\n",
      "Iteration 2, loss = 0.33050910\n",
      "Iteration 20, loss = 0.02362253\n",
      "Iteration 20, loss = 0.14867034\n",
      "Iteration 20, loss = 0.02398285\n",
      "Iteration 21, loss = 0.02286656\n",
      "Iteration 21, loss = 0.02310740\n",
      "Iteration 21, loss = 0.02234276\n",
      "Iteration 3, loss = 0.23291930\n",
      "Iteration 3, loss = 0.24338091\n",
      "Iteration 21, loss = 0.02331366\n",
      "Iteration 21, loss = 0.14850103\n",
      "Iteration 21, loss = 0.02380053\n",
      "Iteration 22, loss = 0.02305295\n",
      "Iteration 22, loss = 0.02281450\n",
      "Iteration 22, loss = 0.02217986\n",
      "Iteration 4, loss = 0.20351683\n",
      "Iteration 4, loss = 0.21244915\n",
      "Iteration 22, loss = 0.02330721\n",
      "Iteration 22, loss = 0.14759266\n",
      "Iteration 22, loss = 0.02372500\n",
      "Iteration 23, loss = 0.02266198\n",
      "Iteration 23, loss = 0.02289801\n",
      "Iteration 5, loss = 0.18960923\n",
      "Iteration 23, loss = 0.02208690\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ........................alpha=0.01;, score=0.873 total time= 8.5min\n",
      "Iteration 5, loss = 0.19551045\n",
      "Iteration 23, loss = 0.02324480\n",
      "Iteration 23, loss = 0.14573093\n",
      "Iteration 23, loss = 0.02376587\n",
      "Iteration 24, loss = 0.02263982\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ........................alpha=0.01;, score=0.880 total time= 8.9min\n",
      "Iteration 24, loss = 0.02301693\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ........................alpha=0.01;, score=0.871 total time= 8.9min\n",
      "Iteration 6, loss = 0.18042918\n",
      "Iteration 1, loss = 0.57398240\n",
      "Iteration 6, loss = 0.18601749\n",
      "Iteration 24, loss = 0.02330848\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.14339023\n",
      "[CV 4/5] END ........................alpha=0.01;, score=0.857 total time= 8.9min\n",
      "Iteration 24, loss = 0.02367630\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ........................alpha=0.01;, score=0.867 total time= 8.9min\n",
      "Iteration 1, loss = 0.58158870\n",
      "Iteration 7, loss = 0.17462973\n",
      "Iteration 2, loss = 0.30832579\n",
      "Iteration 7, loss = 0.17922160\n",
      "Iteration 25, loss = 0.14233761\n",
      "Iteration 2, loss = 0.31745578\n",
      "Iteration 8, loss = 0.16908614\n",
      "Iteration 3, loss = 0.22800918\n",
      "Iteration 8, loss = 0.17482187\n",
      "Iteration 26, loss = 0.14651689\n",
      "Iteration 3, loss = 0.23267896\n",
      "Iteration 9, loss = 0.16580771\n",
      "Iteration 4, loss = 0.19966776\n",
      "Iteration 9, loss = 0.16992942\n",
      "Iteration 27, loss = 0.14570154\n",
      "Iteration 4, loss = 0.20325485\n",
      "Iteration 10, loss = 0.16265211\n",
      "Iteration 5, loss = 0.18579913\n",
      "Iteration 10, loss = 0.16615263\n",
      "Iteration 28, loss = 0.14477237\n",
      "Iteration 5, loss = 0.18903774\n",
      "Iteration 11, loss = 0.16183215\n",
      "Iteration 11, loss = 0.16347250\n",
      "Iteration 6, loss = 0.17780544\n",
      "Iteration 29, loss = 0.14338557\n",
      "Iteration 6, loss = 0.17972661\n",
      "Iteration 12, loss = 0.15960795\n",
      "Iteration 12, loss = 0.15970933\n",
      "Iteration 7, loss = 0.17259853\n",
      "Iteration 30, loss = 0.14512753\n",
      "Iteration 7, loss = 0.17391855\n",
      "Iteration 13, loss = 0.15900141\n",
      "Iteration 8, loss = 0.16743729\n",
      "Iteration 13, loss = 0.15892993\n",
      "Iteration 31, loss = 0.14244531\n",
      "Iteration 8, loss = 0.16994717\n",
      "Iteration 14, loss = 0.15617364\n",
      "Iteration 9, loss = 0.16413218\n",
      "Iteration 14, loss = 0.15855707\n",
      "Iteration 32, loss = 0.14151646\n",
      "Iteration 9, loss = 0.16579185\n",
      "Iteration 15, loss = 0.15298107\n",
      "Iteration 10, loss = 0.16062979\n",
      "Iteration 15, loss = 0.15581296\n",
      "Iteration 33, loss = 0.14033969\n",
      "Iteration 10, loss = 0.16155097\n",
      "Iteration 16, loss = 0.15261195\n",
      "Iteration 11, loss = 0.15880624\n",
      "Iteration 16, loss = 0.15502642\n",
      "Iteration 34, loss = 0.14058955\n",
      "Iteration 11, loss = 0.15980408\n",
      "Iteration 17, loss = 0.15105212\n",
      "Iteration 12, loss = 0.15693033\n",
      "Iteration 17, loss = 0.15320506\n",
      "Iteration 35, loss = 0.14163185\n",
      "Iteration 12, loss = 0.15750078\n",
      "Iteration 18, loss = 0.14946107\n",
      "Iteration 13, loss = 0.15621276\n",
      "Iteration 18, loss = 0.15135795\n",
      "Iteration 36, loss = 0.13892978\n",
      "Iteration 19, loss = 0.14958997\n",
      "Iteration 13, loss = 0.15746685\n",
      "Iteration 14, loss = 0.15374979\n",
      "Iteration 19, loss = 0.15180744\n",
      "Iteration 37, loss = 0.13676511\n",
      "Iteration 20, loss = 0.14851575\n",
      "Iteration 14, loss = 0.15457669\n",
      "Iteration 15, loss = 0.15320342\n",
      "Iteration 20, loss = 0.15052240\n",
      "Iteration 38, loss = 0.13809473\n",
      "Iteration 21, loss = 0.14759045\n",
      "Iteration 15, loss = 0.15324127\n",
      "Iteration 16, loss = 0.15007640\n",
      "Iteration 21, loss = 0.14857149\n",
      "Iteration 39, loss = 0.13759659\n",
      "Iteration 22, loss = 0.14787276\n",
      "Iteration 16, loss = 0.15363702\n",
      "Iteration 17, loss = 0.15038359\n",
      "Iteration 22, loss = 0.14909183\n",
      "Iteration 40, loss = 0.13669674\n",
      "Iteration 17, loss = 0.15148557\n",
      "Iteration 23, loss = 0.14642380\n",
      "Iteration 23, loss = 0.14831300\n",
      "Iteration 18, loss = 0.14758629\n",
      "Iteration 41, loss = 0.13750718\n",
      "Iteration 24, loss = 0.14715620\n",
      "Iteration 18, loss = 0.14945410\n",
      "Iteration 19, loss = 0.14817724\n",
      "Iteration 24, loss = 0.14845883\n",
      "Iteration 42, loss = 0.13889309\n",
      "Iteration 25, loss = 0.14691177\n",
      "Iteration 19, loss = 0.14872774\n",
      "Iteration 20, loss = 0.15067109\n",
      "Iteration 43, loss = 0.13618801\n",
      "Iteration 25, loss = 0.14661728\n",
      "Iteration 26, loss = 0.14723027\n",
      "Iteration 20, loss = 0.14900520\n",
      "Iteration 26, loss = 0.14640231\n",
      "Iteration 44, loss = 0.13526241\n",
      "Iteration 21, loss = 0.14963149\n",
      "Iteration 21, loss = 0.14937634\n",
      "Iteration 27, loss = 0.14551435\n",
      "Iteration 27, loss = 0.14629361\n",
      "Iteration 45, loss = 0.13634693\n",
      "Iteration 22, loss = 0.14823135\n",
      "Iteration 22, loss = 0.14794466\n",
      "Iteration 28, loss = 0.14313533\n",
      "Iteration 28, loss = 0.14628257\n",
      "Iteration 46, loss = 0.13449353\n",
      "Iteration 23, loss = 0.14563823\n",
      "Iteration 23, loss = 0.14728412\n",
      "Iteration 29, loss = 0.14241431\n",
      "Iteration 29, loss = 0.14483388\n",
      "Iteration 47, loss = 0.13605751\n",
      "Iteration 24, loss = 0.14660714\n",
      "Iteration 24, loss = 0.14704044\n",
      "Iteration 30, loss = 0.14045698\n",
      "Iteration 48, loss = 0.13339210\n",
      "Iteration 30, loss = 0.14541311\n",
      "Iteration 25, loss = 0.14934064\n",
      "Iteration 25, loss = 0.14869572\n",
      "Iteration 31, loss = 0.14162546\n",
      "Iteration 49, loss = 0.13269314\n",
      "Iteration 31, loss = 0.14503444\n",
      "Iteration 26, loss = 0.14774563\n",
      "Iteration 26, loss = 0.14581221\n",
      "Iteration 32, loss = 0.14365473\n",
      "Iteration 50, loss = 0.13125637\n",
      "Iteration 32, loss = 0.14353740\n",
      "Iteration 27, loss = 0.14497215\n",
      "Iteration 27, loss = 0.14481563\n",
      "Iteration 33, loss = 0.14133106\n",
      "Iteration 51, loss = 0.13188225\n",
      "Iteration 33, loss = 0.13865378\n",
      "Iteration 28, loss = 0.14287198\n",
      "Iteration 34, loss = 0.14210284\n",
      "Iteration 28, loss = 0.14372854\n",
      "Iteration 52, loss = 0.13180758\n",
      "Iteration 34, loss = 0.14057968\n",
      "Iteration 29, loss = 0.14322009\n",
      "Iteration 35, loss = 0.14281142\n",
      "Iteration 29, loss = 0.14481439\n",
      "Iteration 53, loss = 0.12964895\n",
      "Iteration 30, loss = 0.14296081\n",
      "Iteration 35, loss = 0.14208152\n",
      "Iteration 36, loss = 0.14066094\n",
      "Iteration 30, loss = 0.14616220\n",
      "Iteration 31, loss = 0.14276891\n",
      "Iteration 54, loss = 0.13108905\n",
      "Iteration 36, loss = 0.14246749\n",
      "Iteration 37, loss = 0.13858830\n",
      "Iteration 31, loss = 0.14608516\n",
      "Iteration 32, loss = 0.14300922\n",
      "Iteration 37, loss = 0.14087919\n",
      "Iteration 55, loss = 0.12944989\n",
      "Iteration 38, loss = 0.13965127\n",
      "Iteration 32, loss = 0.14283884\n",
      "Iteration 33, loss = 0.14358278\n",
      "Iteration 56, loss = 0.12934509\n",
      "Iteration 38, loss = 0.14062308\n",
      "Iteration 39, loss = 0.14215896\n",
      "Iteration 33, loss = 0.14197353\n",
      "Iteration 34, loss = 0.14170082\n",
      "Iteration 39, loss = 0.14011987\n",
      "Iteration 57, loss = 0.12740957\n",
      "Iteration 40, loss = 0.13932210\n",
      "Iteration 34, loss = 0.14133888\n",
      "Iteration 35, loss = 0.14018868\n",
      "Iteration 40, loss = 0.13870631\n",
      "Iteration 58, loss = 0.12557183\n",
      "Iteration 41, loss = 0.13772078\n",
      "Iteration 35, loss = 0.14074899\n",
      "Iteration 36, loss = 0.13980353\n",
      "Iteration 41, loss = 0.13778596\n",
      "Iteration 59, loss = 0.12752498\n",
      "Iteration 42, loss = 0.13640831\n",
      "Iteration 36, loss = 0.13962716\n",
      "Iteration 37, loss = 0.13899096\n",
      "Iteration 42, loss = 0.13693975\n",
      "Iteration 60, loss = 0.12630514\n",
      "Iteration 43, loss = 0.13475896\n",
      "Iteration 37, loss = 0.13898581\n",
      "Iteration 38, loss = 0.14013925\n",
      "Iteration 61, loss = 0.12578293\n",
      "Iteration 43, loss = 0.13574833\n",
      "Iteration 44, loss = 0.13489777\n",
      "Iteration 38, loss = 0.13826297\n",
      "Iteration 39, loss = 0.13788543\n",
      "Iteration 62, loss = 0.12466507\n",
      "Iteration 44, loss = 0.13775590\n",
      "Iteration 45, loss = 0.13659547\n",
      "Iteration 39, loss = 0.13690614\n",
      "Iteration 40, loss = 0.13760342\n",
      "Iteration 63, loss = 0.12421478\n",
      "Iteration 45, loss = 0.13782233\n",
      "Iteration 46, loss = 0.13712953\n",
      "Iteration 40, loss = 0.13897420\n",
      "Iteration 41, loss = 0.13889042\n",
      "Iteration 64, loss = 0.12501950\n",
      "Iteration 46, loss = 0.13488182\n",
      "Iteration 47, loss = 0.13530894\n",
      "Iteration 41, loss = 0.14033092\n",
      "Iteration 42, loss = 0.13696261\n",
      "Iteration 65, loss = 0.12478720\n",
      "Iteration 47, loss = 0.13473824\n",
      "Iteration 48, loss = 0.13381882\n",
      "Iteration 42, loss = 0.13884244\n",
      "Iteration 43, loss = 0.13662629\n",
      "Iteration 66, loss = 0.12316569\n",
      "Iteration 48, loss = 0.13671638\n",
      "Iteration 49, loss = 0.13275750\n",
      "Iteration 43, loss = 0.13995730\n",
      "Iteration 44, loss = 0.13810467\n",
      "Iteration 67, loss = 0.12332250\n",
      "Iteration 49, loss = 0.13575859\n",
      "Iteration 50, loss = 0.13245984\n",
      "Iteration 44, loss = 0.13805328\n",
      "Iteration 45, loss = 0.13833766\n",
      "Iteration 68, loss = 0.12238772\n",
      "Iteration 50, loss = 0.13357816\n",
      "Iteration 51, loss = 0.12891521\n",
      "Iteration 45, loss = 0.13673461\n",
      "Iteration 46, loss = 0.13828029\n",
      "Iteration 69, loss = 0.12264304\n",
      "Iteration 51, loss = 0.13420899\n",
      "Iteration 52, loss = 0.13059336\n",
      "Iteration 46, loss = 0.13313766\n",
      "Iteration 47, loss = 0.13563175\n",
      "Iteration 70, loss = 0.12134328\n",
      "Iteration 52, loss = 0.13341227\n",
      "Iteration 53, loss = 0.13357328\n",
      "Iteration 47, loss = 0.13530291\n",
      "Iteration 48, loss = 0.13560181\n",
      "Iteration 71, loss = 0.12094548\n",
      "Iteration 53, loss = 0.13145364\n",
      "Iteration 54, loss = 0.13243111\n",
      "Iteration 48, loss = 0.13586802\n",
      "Iteration 49, loss = 0.13380796\n",
      "Iteration 72, loss = 0.12026852\n",
      "Iteration 54, loss = 0.13179123\n",
      "Iteration 55, loss = 0.13376133\n",
      "Iteration 49, loss = 0.13560005\n",
      "Iteration 50, loss = 0.13406727\n",
      "Iteration 73, loss = 0.11998799\n",
      "Iteration 55, loss = 0.13033541\n",
      "Iteration 56, loss = 0.13283670\n",
      "Iteration 50, loss = 0.13490221\n",
      "Iteration 51, loss = 0.13262035\n",
      "Iteration 74, loss = 0.11894694\n",
      "Iteration 56, loss = 0.12867815\n",
      "Iteration 57, loss = 0.13186648\n",
      "Iteration 51, loss = 0.13480170\n",
      "Iteration 52, loss = 0.13366301\n",
      "Iteration 75, loss = 0.11937653\n",
      "Iteration 57, loss = 0.12894570\n",
      "Iteration 58, loss = 0.12928312\n",
      "Iteration 52, loss = 0.13471357\n",
      "Iteration 53, loss = 0.13248210\n",
      "Iteration 58, loss = 0.12832999\n",
      "Iteration 76, loss = 0.11762289\n",
      "Iteration 59, loss = 0.12598213\n",
      "Iteration 53, loss = 0.13490715\n",
      "Iteration 54, loss = 0.13282208\n",
      "Iteration 59, loss = 0.12996806\n",
      "Iteration 77, loss = 0.11742028\n",
      "Iteration 60, loss = 0.12479466\n",
      "Iteration 54, loss = 0.13258950\n",
      "Iteration 55, loss = 0.13333702\n",
      "Iteration 78, loss = 0.11799330\n",
      "Iteration 60, loss = 0.12816059\n",
      "Iteration 61, loss = 0.12699557\n",
      "Iteration 55, loss = 0.13158712\n",
      "Iteration 56, loss = 0.13444194\n",
      "Iteration 79, loss = 0.11673257\n",
      "Iteration 61, loss = 0.12481285\n",
      "Iteration 62, loss = 0.12602701\n",
      "Iteration 56, loss = 0.13061190\n",
      "Iteration 57, loss = 0.13228963\n",
      "Iteration 80, loss = 0.11634050\n",
      "Iteration 62, loss = 0.12423917\n",
      "Iteration 63, loss = 0.12565306\n",
      "Iteration 57, loss = 0.13026798\n",
      "Iteration 58, loss = 0.13026679\n",
      "Iteration 81, loss = 0.11552289\n",
      "Iteration 63, loss = 0.12496649\n",
      "Iteration 64, loss = 0.12623843\n",
      "Iteration 58, loss = 0.12809530\n",
      "Iteration 59, loss = 0.12810984\n",
      "Iteration 82, loss = 0.11410828\n",
      "Iteration 64, loss = 0.12485117\n",
      "Iteration 65, loss = 0.12662129\n",
      "Iteration 59, loss = 0.12808646\n",
      "Iteration 60, loss = 0.12730804\n",
      "Iteration 83, loss = 0.11282509\n",
      "Iteration 65, loss = 0.12478992\n",
      "Iteration 66, loss = 0.12699818\n",
      "Iteration 60, loss = 0.12885817\n",
      "Iteration 61, loss = 0.12803000\n",
      "Iteration 66, loss = 0.12274878\n",
      "Iteration 84, loss = 0.11445810\n",
      "Iteration 67, loss = 0.12598855\n",
      "Iteration 61, loss = 0.12909474\n",
      "Iteration 62, loss = 0.12831040\n",
      "Iteration 67, loss = 0.12571943\n",
      "Iteration 85, loss = 0.11439578\n",
      "Iteration 68, loss = 0.12578076\n",
      "Iteration 62, loss = 0.12759496\n",
      "Iteration 63, loss = 0.12794373\n",
      "Iteration 86, loss = 0.11238294\n",
      "Iteration 68, loss = 0.12487472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "/Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END .........................alpha=0.1;, score=0.874 total time=25.4min\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.851 total time=16.9min\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.855 total time=16.6min\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.869 total time=18.8min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/max/Library/Mobile Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine Learning/Final Project/comp-562-final-project/COMP 562 Final Project.ipynb Cell 45'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/max/Library/Mobile%20Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine%20Learning/Final%20Project/comp-562-final-project/COMP%20562%20Final%20Project.ipynb#ch0000025?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStarting CV...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/max/Library/Mobile%20Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine%20Learning/Final%20Project/comp-562-final-project/COMP%20562%20Final%20Project.ipynb#ch0000025?line=7'>8</a>\u001b[0m mlpc_cv \u001b[39m=\u001b[39m GridSearchCV(mlpc, mlpc_params, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/max/Library/Mobile%20Documents/com~apple~CloudDocs/UNC/Junior/Spring/Machine%20Learning/Final%20Project/comp-562-final-project/COMP%20562%20Final%20Project.ipynb#ch0000025?line=8'>9</a>\u001b[0m mlpc_cv\u001b[39m.\u001b[39;49mfit(bigram_train, humor_train)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=884'>885</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=885'>886</a>\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=886'>887</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=888'>889</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=890'>891</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=892'>893</a>\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=893'>894</a>\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=894'>895</a>\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=1389'>1390</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=1390'>1391</a>\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=1391'>1392</a>\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=829'>830</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=830'>831</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=831'>832</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=832'>833</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=833'>834</a>\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=834'>835</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=835'>836</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=837'>838</a>\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=838'>839</a>\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=839'>840</a>\u001b[0m         clone(base_estimator),\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=840'>841</a>\u001b[0m         X,\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=841'>842</a>\u001b[0m         y,\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=842'>843</a>\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=843'>844</a>\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=844'>845</a>\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=845'>846</a>\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=846'>847</a>\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=847'>848</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=848'>849</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=849'>850</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=850'>851</a>\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=851'>852</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=852'>853</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=854'>855</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=855'>856</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=856'>857</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=857'>858</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=858'>859</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py?line=859'>860</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=1052'>1053</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=1054'>1055</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=1055'>1056</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=1056'>1057</a>\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=1057'>1058</a>\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=932'>933</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=933'>934</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=934'>935</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=935'>936</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py?line=936'>937</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=538'>539</a>\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=539'>540</a>\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=540'>541</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=541'>542</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=542'>543</a>\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py?line=543'>544</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py?line=437'>438</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py?line=438'>439</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py?line=440'>441</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py?line=442'>443</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py?line=443'>444</a>\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/threading.py?line=317'>318</a>\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/threading.py?line=318'>319</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/threading.py?line=319'>320</a>\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/threading.py?line=320'>321</a>\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/max/.pyenv/versions/3.10.2/lib/python3.10/threading.py?line=321'>322</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlpc_params = {\n",
    "    \"alpha\": [.0001, .001, .01, .1]\n",
    "}\n",
    "print(\"Starting CV...\")\n",
    "mlpc_cv = GridSearchCV(mlpc, mlpc_params, verbose=3, n_jobs=8)\n",
    "mlpc_cv.fit(bigram_train, humor_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167e5ce",
   "metadata": {},
   "source": [
    "#### Confusion matrix and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46086c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "humor_preds = mlpc.predict(bigram_test)\n",
    "mlpc_cm = get_cm(humor_preds)\n",
    "plot_confusion_matrix(mlpc_cm, \"Multilayer perceptron with bigrams\")\n",
    "mlpc_cv.score(bigram_test, humor_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
