{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6183c9b",
   "metadata": {},
   "source": [
    "## Plan\n",
    "### Turning tweets into features\n",
    "\n",
    "- Start with trigrams, can tune later\n",
    "- Can consider bigrams, bag of words, or other n-grams\n",
    "- Ignore location information, at least for now\n",
    "- Almost all tweets have keywords, use as another feature\n",
    "- Make sure to process \"keyword\" values, removing special characters\n",
    "\n",
    "### Criteria for disaster\n",
    "- Meant to track if tweets are referring to ongoing disasters\n",
    "- Also includes historical events\n",
    "\n",
    "\n",
    "### Training\n",
    "- Train and validate our model on `train.csv` \n",
    "- Test by sending results to Kaggle\n",
    "\n",
    "### Random forest\n",
    "- Use Gini criterion for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdba33",
   "metadata": {},
   "source": [
    "## Importing and vectorizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f756010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c957f4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08227f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4651bfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_string(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"http://t\\.co/\\S+\", \"\", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b09e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://t.co/\n"
     ]
    }
   ],
   "source": [
    "print(standardize_string(\"http://t.co/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa81936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x89', '\\x9d', '¡', '¢', '£', '¤', '¨', '©', 'ª', '«', '¬', '´', '¼', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', '÷', 'û', 'ü']\n"
     ]
    }
   ],
   "source": [
    "all_characters = set()\n",
    "\n",
    "for tweet in train_df['text']:\n",
    "    all_characters = all_characters.union(set(standardize_string(tweet)))\n",
    "\n",
    "char_list = list(all_characters)\n",
    "char_list.sort()\n",
    "print(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc3fafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '#', '@', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', 'ü', ' ']\n"
     ]
    }
   ],
   "source": [
    "included_chars = list(string.ascii_lowercase + string.digits) + ['#', '@', 'â', 'ã', 'å', 'ç', 'è', 'ê', 'ì', 'ï', 'ñ', 'ò', 'ó', 'ü', ' ']\n",
    "print(included_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d2846",
   "metadata": {},
   "source": [
    "### Stripping characters\n",
    "- Try both with and without removing special characters\n",
    "- Consider skipping data points with bad characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "096b1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(s):\n",
    "    for c in char_list:\n",
    "        if c not in included_chars:\n",
    "            s = s.replace(c, \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3140441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_array(t):\n",
    "    t = standardize_string(t)\n",
    "    tweet_array = t.split()\n",
    "    return tweet_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d38ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data():\n",
    "    all_tweets = []\n",
    "    for tweet in train_df['text']:\n",
    "        tweet_array = tweet_to_array(tweet)\n",
    "        invalid_fields = []\n",
    "        for field in tweet_array:\n",
    "            for c in excluded_chars:\n",
    "                if c in field:\n",
    "                    invalid_fields.append(field)\n",
    "                    continue\n",
    "        \n",
    "        for f in invalid_fields:\n",
    "            del f\n",
    "        all_tweets.append(tweet_array)\n",
    "        \n",
    "    return all_tweets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5496e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Jarem:\n",
    "# mock-up of random forest, using these sites as reference:\n",
    "# https://machinelearningmastery.com/random-forest-ensemble-in-python/\n",
    "# https://www.kaggle.com/code/philculliton/nlp-getting-started-tutorial/notebook\n",
    "# getting super bad performance - worse than chance often - and I don't know much hyperparameter tuning will fix it\n",
    "# not sure what I'm doing wrong here - n-grams and text formatting seem to make it worse\n",
    "\n",
    "def format_tweet(t):\n",
    "    # Makes lowercase\n",
    "    formatted_tweet = t.lower()\n",
    "    # Removed links\n",
    "    formatted_tweet = re.sub(\" http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    formatted_tweet = re.sub(\"http(s|)://t\\.co/\\S+\", \"\", formatted_tweet)\n",
    "    # Removes any special characters, other than a-z, numbers, spaces, hashtags, and @\n",
    "    formatted_tweet = remove_special_characters(formatted_tweet)\n",
    "    final_tweet_array = []\n",
    "    \n",
    "    # Removes multiple consecutive spaces\n",
    "    for i, char in enumerate(formatted_tweet):\n",
    "        if i == 0:\n",
    "            if char != ' ':\n",
    "                final_tweet_array.append(char)\n",
    "                continue\n",
    "        prev_char = formatted_tweet[i-1]\n",
    "        if char == ' ' and prev_char == ' ':\n",
    "            continue\n",
    "        final_tweet_array.append(char)\n",
    "    final_tweet = \"\".join(final_tweet_array)\n",
    "    return final_tweet\n",
    "\n",
    "#basic string formatting\n",
    "formatted_train_tweets = []\n",
    "for i, tweet in enumerate(train_df[\"text\"]):\n",
    "    formatted_train_tweets.append(format_tweet(tweet))\n",
    "\n",
    "formatted_test_tweets = []\n",
    "for tweet in test_df[\"text\"]:\n",
    "    formatted_test_tweets.append(format_tweet(tweet))\n",
    "    \n",
    "test_ids = test_df['id']\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "bigram_train = bigram_vectorizer.fit_transform(formatted_train_tweets)\n",
    "bigram_test = bigram_vectorizer.transform(formatted_test_tweets)\n",
    "\n",
    "\n",
    "# 1-gram no string formatting\n",
    "# array([0.55543823, 0.50891089, 0.54221388, 0.51913133, 0.68794326])\n",
    "# 1 and 2-gram, no string formatting\n",
    "# array([0.46118721, 0.45027322, 0.43412527, 0.44141069, 0.61523626])\n",
    "# 1-gram basic string formatting\n",
    "# array([0.57556936, 0.48219736, 0.5530303 , 0.51859504, 0.68586387])\n",
    "# 1 & 2-gram, basic string formatting\n",
    "# array([0.5039019 , 0.41150442, 0.41241685, 0.45823928, 0.62327416])\n",
    "# bigram only, basic string formatting\n",
    "# array([0.24096386, 0.25725095, 0.1682243 , 0.17475728, 0.31060606])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7f4b9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m rf_parameters \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m500\u001b[39m]\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     13\u001b[0m rf_cv \u001b[38;5;241m=\u001b[39m GridSearchCV(rf, parameters, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrf_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(rf_cv\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:891\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    885\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    886\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    887\u001b[0m     )\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 891\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    895\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1392\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1392\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/sklearn/model_selection/_search.py:838\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    834\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    835\u001b[0m         )\n\u001b[1;32m    836\u001b[0m     )\n\u001b[0;32m--> 838\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    859\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    860\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1056\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 935\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/envs/machine-learning/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/concurrent/futures/_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=10, max_depth=None, class_weight=\"balanced\")\n",
    "\n",
    "rf_parameters = {\n",
    "    'min_samples_split': range(2, 5),\n",
    "    'min_samples_leaf': range(1, 4),\n",
    "    'n_estimators': [50, 100, 500]\n",
    "}\n",
    "\n",
    "\n",
    "rf_cv = GridSearchCV(rf, parameters, verbose=3, n_jobs=10)\n",
    "rf_cv.fit(bigram_train, train_df['target'])\n",
    "print(rf_cv.best_params_)\n",
    "\n",
    "# {'min_samples_leaf': 2, 'min_samples_split': 4, 'n_estimators': 500}\n",
    "# {'class_weight': 'balanced', 'max_depth': None, 'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39aa682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 1 1]\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.641 total time= 1.3min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.667 total time=  35.2s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.654 total time= 1.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.611 total time= 5.9min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.671 total time=  24.3s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.671 total time=  46.4s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.670 total time= 3.8min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.630 total time=  42.6s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.649 total time=  22.7s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.653 total time=  46.3s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.625 total time= 3.5min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.632 total time=  10.8s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.612 total time=  11.7s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.625 total time=  20.5s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.605 total time= 1.7min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.657 total time= 1.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.616 total time=  36.4s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.614 total time= 1.2min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.658 total time= 5.8min\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.617 total time=  23.5s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.627 total time=  46.8s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.613 total time= 3.8min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.651 total time=  44.9s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.628 total time=  22.1s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.624 total time=  44.1s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.634 total time= 3.5min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.642 total time=  11.4s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.613 total time=  10.7s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.645 total time=  22.3s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.628 total time= 1.7min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.605 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.618 total time=  34.5s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.641 total time= 1.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.599 total time= 5.7min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.632 total time=  21.0s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.635 total time=  45.2s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.652 total time= 3.8min\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.619 total time=  22.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.612 total time=  45.7s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.625 total time=  23.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.609 total time=  46.3s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.616 total time= 3.8min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.621 total time=  19.3s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.643 total time= 1.8min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.642 total time=  37.8s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.658 total time= 6.2min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.618 total time=  34.3s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.609 total time= 1.1min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.611 total time= 5.7min\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.615 total time= 3.8min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.641 total time=  23.2s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.611 total time= 1.6min\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.618 total time= 1.8min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.609 total time= 1.2min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.642 total time=  36.4s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.619 total time= 1.1min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.641 total time= 5.9min\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.620 total time=  22.5s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.611 total time=  43.7s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.622 total time= 3.6min\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.628 total time=  21.1s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.626 total time=  42.1s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.631 total time=  22.2s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.632 total time=  43.0s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.655 total time= 3.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=50;, score=0.671 total time=  11.7s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.620 total time=  22.5s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=500;, score=0.670 total time= 1.8min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.616 total time=  36.7s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.601 total time= 5.9min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.640 total time=  34.3s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.617 total time= 1.1min\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.639 total time= 5.8min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=100;, score=0.663 total time=  45.8s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=50;, score=0.670 total time=  23.0s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=100;, score=0.675 total time=  45.4s\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=4, n_estimators=500;, score=0.668 total time= 3.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=3, n_estimators=100;, score=0.673 total time=  23.2s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.626 total time=  10.5s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.649 total time=  22.6s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.626 total time= 1.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.610 total time=  36.3s\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.618 total time= 5.9min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.621 total time=  32.6s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.640 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.619 total time= 5.6min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.658 total time=  21.5s\n",
      "[CV 3/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.622 total time= 3.6min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.624 total time=  10.2s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.665 total time=  11.8s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.660 total time=  23.0s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.667 total time= 1.9min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.615 total time=  20.1s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.620 total time= 1.4min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.604 total time=  35.5s\n",
      "[CV 1/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.637 total time= 6.2min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.660 total time=  34.4s\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.658 total time= 1.1min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.662 total time= 5.7min\n",
      "[CV 5/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.673 total time= 3.8min\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.621 total time=  20.2s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.641 total time= 1.8min\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.603 total time=  10.3s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.629 total time=  22.1s\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.644 total time= 1.4min\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=100;, score=0.609 total time= 1.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=50;, score=0.598 total time=  34.6s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=100;, score=0.599 total time= 1.2min\n",
      "[CV 2/5] END min_samples_leaf=1, min_samples_split=3, n_estimators=500;, score=0.618 total time= 5.7min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=50;, score=0.653 total time=  22.9s\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=100;, score=0.649 total time=  48.2s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=2, n_estimators=500;, score=0.634 total time= 3.6min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.655 total time=  23.2s\n",
      "[CV 4/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=50;, score=0.614 total time=  22.3s\n",
      "[CV 2/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.631 total time= 3.5min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.647 total time=  11.3s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.615 total time=   9.7s\n",
      "[CV 3/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.605 total time=  21.0s\n",
      "[CV 2/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.628 total time= 1.7min\n",
      "[CV 1/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.643 total time=  11.1s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.624 total time=  11.8s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.624 total time=  23.6s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.621 total time= 1.3min\n",
      "[CV 5/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.659 total time=  37.1s\n",
      "[CV 4/5] END min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.609 total time= 6.2min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=50;, score=0.599 total time=  33.4s\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=100;, score=0.595 total time= 1.1min\n",
      "[CV 3/5] END min_samples_leaf=1, min_samples_split=4, n_estimators=500;, score=0.601 total time= 5.7min\n",
      "[CV 1/5] END min_samples_leaf=2, min_samples_split=3, n_estimators=500;, score=0.654 total time= 3.8min\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.622 total time=  11.4s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=100;, score=0.621 total time=  21.9s\n",
      "[CV 4/5] END min_samples_leaf=3, min_samples_split=2, n_estimators=500;, score=0.621 total time= 1.8min\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=50;, score=0.673 total time=  12.1s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=100;, score=0.677 total time=  23.8s\n",
      "[CV 5/5] END min_samples_leaf=3, min_samples_split=4, n_estimators=500;, score=0.670 total time= 1.3min\n"
     ]
    }
   ],
   "source": [
    "rf_predicted_classes = rf_cv.predict(bigram_test)\n",
    "print(rf_predicted_classes)\n",
    "rf_out_array = []\n",
    "for i, pred_class in enumerate(rf_predicted_classes):\n",
    "    rf_out_array.append([int(test_ids[i]), pred_class])\n",
    "\n",
    "np.savetxt(\"rf-results.csv\", rf_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5649b3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Iteration 1, loss = 0.64550188\n",
      "Iteration 2, loss = 0.37110657\n",
      "Iteration 3, loss = 0.16113751\n",
      "Iteration 4, loss = 0.09245533\n",
      "Iteration 5, loss = 0.06919119\n",
      "Iteration 6, loss = 0.05685333\n",
      "Iteration 7, loss = 0.05051970\n",
      "Iteration 8, loss = 0.04652030\n",
      "Iteration 9, loss = 0.04329465\n",
      "Iteration 10, loss = 0.04027972\n",
      "Iteration 11, loss = 0.03963507\n",
      "Iteration 12, loss = 0.03706136\n",
      "Iteration 13, loss = 0.03634189\n",
      "Iteration 14, loss = 0.03578973\n",
      "Iteration 15, loss = 0.03438279\n",
      "Iteration 16, loss = 0.03405455\n",
      "Iteration 17, loss = 0.03368897\n",
      "Iteration 18, loss = 0.03330628\n",
      "Iteration 19, loss = 0.03267945\n",
      "Iteration 20, loss = 0.03235284\n",
      "Iteration 21, loss = 0.03212069\n",
      "Iteration 22, loss = 0.03165786\n",
      "Iteration 23, loss = 0.03103042\n",
      "Iteration 24, loss = 0.03137677\n",
      "Iteration 25, loss = 0.03200844\n",
      "Iteration 26, loss = 0.03082211\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "{'alpha': 0.0001}\n",
      "Iteration 1, loss = 0.65317392\n",
      "Iteration 2, loss = 0.41496627\n",
      "Iteration 3, loss = 0.20253419\n",
      "Iteration 4, loss = 0.11423674\n",
      "Iteration 5, loss = 0.08132673\n",
      "Iteration 6, loss = 0.06560472\n",
      "Iteration 7, loss = 0.05682019\n",
      "Iteration 8, loss = 0.05060551\n",
      "Iteration 9, loss = 0.04640707\n",
      "Iteration 10, loss = 0.04314637\n",
      "Iteration 11, loss = 0.04158098\n",
      "Iteration 12, loss = 0.03941803\n",
      "Iteration 13, loss = 0.03815076\n",
      "Iteration 14, loss = 0.03740764\n",
      "Iteration 15, loss = 0.03639001\n",
      "Iteration 16, loss = 0.03535214\n",
      "Iteration 17, loss = 0.03510264\n",
      "Iteration 18, loss = 0.03463497\n",
      "Iteration 19, loss = 0.03490439\n",
      "Iteration 20, loss = 0.03415402\n",
      "Iteration 21, loss = 0.03361590\n",
      "Iteration 22, loss = 0.03298625\n",
      "Iteration 23, loss = 0.03334798\n",
      "Iteration 24, loss = 0.03269923\n",
      "Iteration 25, loss = 0.03289551\n",
      "Iteration 26, loss = 0.03328964\n",
      "Iteration 27, loss = 0.03245165\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ......................alpha=0.0001;, score=0.638 total time= 2.1min\n",
      "Iteration 1, loss = 0.66046207\n",
      "Iteration 2, loss = 0.42538834\n",
      "Iteration 3, loss = 0.22122301\n",
      "Iteration 4, loss = 0.13974113\n",
      "Iteration 5, loss = 0.10833237\n",
      "Iteration 6, loss = 0.09336862\n",
      "Iteration 7, loss = 0.08565146\n",
      "Iteration 8, loss = 0.07993487\n",
      "Iteration 9, loss = 0.07638670\n",
      "Iteration 10, loss = 0.07270396\n",
      "Iteration 11, loss = 0.06998818\n",
      "Iteration 12, loss = 0.06821153\n",
      "Iteration 13, loss = 0.06685982\n",
      "Iteration 14, loss = 0.06567689\n",
      "Iteration 15, loss = 0.06418436\n",
      "Iteration 16, loss = 0.06291569\n",
      "Iteration 17, loss = 0.06262780\n",
      "Iteration 18, loss = 0.06056730\n",
      "Iteration 19, loss = 0.06171131\n",
      "Iteration 20, loss = 0.05970244\n",
      "Iteration 21, loss = 0.06032510\n",
      "Iteration 22, loss = 0.05907584\n",
      "Iteration 23, loss = 0.05815379\n",
      "Iteration 24, loss = 0.05690870\n",
      "Iteration 25, loss = 0.05783689\n",
      "Iteration 26, loss = 0.05608311\n",
      "Iteration 27, loss = 0.05571830\n",
      "Iteration 28, loss = 0.05689447\n",
      "Iteration 29, loss = 0.05651511\n",
      "Iteration 30, loss = 0.05569270\n",
      "Iteration 31, loss = 0.05479608\n",
      "Iteration 32, loss = 0.05537167\n",
      "Iteration 33, loss = 0.05516342\n",
      "Iteration 34, loss = 0.05644232\n",
      "Iteration 35, loss = 0.05724458\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ........................alpha=0.01;, score=0.630 total time= 2.7min\n",
      "Iteration 1, loss = 0.65221208\n",
      "Iteration 2, loss = 0.41504004\n",
      "Iteration 3, loss = 0.20209081\n",
      "Iteration 4, loss = 0.11705366\n",
      "Iteration 5, loss = 0.08456082\n",
      "Iteration 6, loss = 0.06930862\n",
      "Iteration 7, loss = 0.05990593\n",
      "Iteration 8, loss = 0.05391782\n",
      "Iteration 9, loss = 0.05035345\n",
      "Iteration 10, loss = 0.04682661\n",
      "Iteration 11, loss = 0.04402299\n",
      "Iteration 12, loss = 0.04267937\n",
      "Iteration 13, loss = 0.04162773\n",
      "Iteration 14, loss = 0.04017923\n",
      "Iteration 15, loss = 0.03960293\n",
      "Iteration 16, loss = 0.03828057\n",
      "Iteration 17, loss = 0.03809258\n",
      "Iteration 18, loss = 0.03763343\n",
      "Iteration 19, loss = 0.03695362\n",
      "Iteration 20, loss = 0.03673868\n",
      "Iteration 21, loss = 0.03647125\n",
      "Iteration 22, loss = 0.03626439\n",
      "Iteration 23, loss = 0.03569102\n",
      "Iteration 24, loss = 0.03595346\n",
      "Iteration 25, loss = 0.03562392\n",
      "Iteration 26, loss = 0.03545468\n",
      "Iteration 27, loss = 0.03484960\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .......................alpha=0.001;, score=0.664 total time= 2.1min\n",
      "Iteration 1, loss = 0.64576575\n",
      "Iteration 2, loss = 0.38625242\n",
      "Iteration 3, loss = 0.19782391\n",
      "Iteration 4, loss = 0.12857387\n",
      "Iteration 5, loss = 0.10185237\n",
      "Iteration 6, loss = 0.08847874\n",
      "Iteration 7, loss = 0.08049272\n",
      "Iteration 8, loss = 0.07601593\n",
      "Iteration 9, loss = 0.07243790\n",
      "Iteration 10, loss = 0.07047842\n",
      "Iteration 11, loss = 0.06791775\n",
      "Iteration 12, loss = 0.06533851\n",
      "Iteration 13, loss = 0.06329469\n",
      "Iteration 14, loss = 0.06165398\n",
      "Iteration 15, loss = 0.06133235\n",
      "Iteration 16, loss = 0.05950696\n",
      "Iteration 17, loss = 0.05848685\n",
      "Iteration 18, loss = 0.05747577\n",
      "Iteration 19, loss = 0.05757608\n",
      "Iteration 20, loss = 0.05702606\n",
      "Iteration 21, loss = 0.05757157\n",
      "Iteration 22, loss = 0.05627145\n",
      "Iteration 23, loss = 0.05648833\n",
      "Iteration 24, loss = 0.05495353\n",
      "Iteration 25, loss = 0.05361427\n",
      "Iteration 26, loss = 0.05384618\n",
      "Iteration 27, loss = 0.05394131\n",
      "Iteration 28, loss = 0.05268696\n",
      "Iteration 29, loss = 0.05295758\n",
      "Iteration 30, loss = 0.05350960\n",
      "Iteration 31, loss = 0.05182249\n",
      "Iteration 32, loss = 0.05197436\n",
      "Iteration 33, loss = 0.05122342\n",
      "Iteration 34, loss = 0.05192788\n",
      "Iteration 35, loss = 0.05197503\n",
      "Iteration 36, loss = 0.05102077\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ........................alpha=0.01;, score=0.661 total time= 2.8min\n",
      "Iteration 1, loss = 0.66172333\n",
      "Iteration 2, loss = 0.43237638\n",
      "Iteration 3, loss = 0.21769556\n",
      "Iteration 4, loss = 0.11491747\n",
      "Iteration 5, loss = 0.07820873\n",
      "Iteration 6, loss = 0.06026215\n",
      "Iteration 7, loss = 0.05096366\n",
      "Iteration 8, loss = 0.04432530\n",
      "Iteration 9, loss = 0.04012989\n",
      "Iteration 10, loss = 0.03667077\n",
      "Iteration 11, loss = 0.03454083\n",
      "Iteration 12, loss = 0.03284944\n",
      "Iteration 13, loss = 0.03119575\n",
      "Iteration 14, loss = 0.02996884\n",
      "Iteration 15, loss = 0.02939237\n",
      "Iteration 16, loss = 0.02818640\n",
      "Iteration 17, loss = 0.02763807\n",
      "Iteration 18, loss = 0.02694175\n",
      "Iteration 19, loss = 0.02717640\n",
      "Iteration 20, loss = 0.02585025\n",
      "Iteration 21, loss = 0.02571828\n",
      "Iteration 22, loss = 0.02623989\n",
      "Iteration 23, loss = 0.02512339\n",
      "Iteration 24, loss = 0.02465531\n",
      "Iteration 25, loss = 0.02492296\n",
      "Iteration 26, loss = 0.02427301\n",
      "Iteration 27, loss = 0.02487444\n",
      "Iteration 28, loss = 0.02423897\n",
      "Iteration 29, loss = 0.02457499\n",
      "Iteration 30, loss = 0.02485590\n",
      "Iteration 31, loss = 0.02407482\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ......................alpha=0.0001;, score=0.644 total time= 2.4min\n",
      "Iteration 1, loss = 0.66879439\n",
      "Iteration 2, loss = 0.42250105\n",
      "Iteration 3, loss = 0.22247718\n",
      "Iteration 4, loss = 0.13991050\n",
      "Iteration 5, loss = 0.10763078\n",
      "Iteration 6, loss = 0.09322590\n",
      "Iteration 7, loss = 0.08415975\n",
      "Iteration 8, loss = 0.07916952\n",
      "Iteration 9, loss = 0.07482786\n",
      "Iteration 10, loss = 0.07195289\n",
      "Iteration 11, loss = 0.06968726\n",
      "Iteration 12, loss = 0.06741951\n",
      "Iteration 13, loss = 0.06584164\n",
      "Iteration 14, loss = 0.06561827\n",
      "Iteration 15, loss = 0.06334375\n",
      "Iteration 16, loss = 0.06223303\n",
      "Iteration 17, loss = 0.06099782\n",
      "Iteration 18, loss = 0.06055524\n",
      "Iteration 19, loss = 0.05976191\n",
      "Iteration 20, loss = 0.05961862\n",
      "Iteration 21, loss = 0.05895822\n",
      "Iteration 22, loss = 0.05874212\n",
      "Iteration 23, loss = 0.05780151\n",
      "Iteration 24, loss = 0.05632984\n",
      "Iteration 25, loss = 0.05712668\n",
      "Iteration 26, loss = 0.05495556\n",
      "Iteration 27, loss = 0.05504486\n",
      "Iteration 28, loss = 0.05669137\n",
      "Iteration 29, loss = 0.05558894\n",
      "Iteration 30, loss = 0.05446095\n",
      "Iteration 31, loss = 0.05510448\n",
      "Iteration 32, loss = 0.05578781\n",
      "Iteration 33, loss = 0.05533970\n",
      "Iteration 34, loss = 0.05398461\n",
      "Iteration 35, loss = 0.05359996\n",
      "Iteration 36, loss = 0.05387214\n",
      "Iteration 37, loss = 0.05483705\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ........................alpha=0.01;, score=0.679 total time= 2.8min\n",
      "Iteration 1, loss = 0.67720245\n",
      "Iteration 2, loss = 0.44783308\n",
      "Iteration 3, loss = 0.22982617\n",
      "Iteration 4, loss = 0.12739609\n",
      "Iteration 5, loss = 0.09043628\n",
      "Iteration 6, loss = 0.07228062\n",
      "Iteration 7, loss = 0.06227469\n",
      "Iteration 8, loss = 0.05592714\n",
      "Iteration 9, loss = 0.05123422\n",
      "Iteration 10, loss = 0.04865936\n",
      "Iteration 11, loss = 0.04637561\n",
      "Iteration 12, loss = 0.04455833\n",
      "Iteration 13, loss = 0.04251020\n",
      "Iteration 14, loss = 0.04204837\n",
      "Iteration 15, loss = 0.04057752\n",
      "Iteration 16, loss = 0.04052380\n",
      "Iteration 17, loss = 0.03995718\n",
      "Iteration 18, loss = 0.03952755\n",
      "Iteration 19, loss = 0.03784785\n",
      "Iteration 20, loss = 0.03848225\n",
      "Iteration 21, loss = 0.03767283\n",
      "Iteration 22, loss = 0.03810687\n",
      "Iteration 23, loss = 0.03698384\n",
      "Iteration 24, loss = 0.03711771\n",
      "Iteration 25, loss = 0.03667199\n",
      "Iteration 26, loss = 0.03597571\n",
      "Iteration 27, loss = 0.03636658\n",
      "Iteration 28, loss = 0.03649255\n",
      "Iteration 29, loss = 0.03668753\n",
      "Iteration 30, loss = 0.03531386\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .......................alpha=0.001;, score=0.703 total time= 2.4min\n",
      "Iteration 1, loss = 0.66951963\n",
      "Iteration 2, loss = 0.42314903\n",
      "Iteration 3, loss = 0.21755112\n",
      "Iteration 4, loss = 0.13375225\n",
      "Iteration 5, loss = 0.10150572\n",
      "Iteration 6, loss = 0.08730009\n",
      "Iteration 7, loss = 0.07855028\n",
      "Iteration 8, loss = 0.07300919\n",
      "Iteration 9, loss = 0.06871895\n",
      "Iteration 10, loss = 0.06666191\n",
      "Iteration 11, loss = 0.06387116\n",
      "Iteration 12, loss = 0.06190092\n",
      "Iteration 13, loss = 0.06240723\n",
      "Iteration 14, loss = 0.05957506\n",
      "Iteration 15, loss = 0.05854504\n",
      "Iteration 16, loss = 0.05662449\n",
      "Iteration 17, loss = 0.05617187\n",
      "Iteration 18, loss = 0.05597276\n",
      "Iteration 19, loss = 0.05497089\n",
      "Iteration 20, loss = 0.05337786\n",
      "Iteration 21, loss = 0.05436452\n",
      "Iteration 22, loss = 0.05266437\n",
      "Iteration 23, loss = 0.05261219\n",
      "Iteration 24, loss = 0.05241489\n",
      "Iteration 25, loss = 0.05082624\n",
      "Iteration 26, loss = 0.05226766\n",
      "Iteration 27, loss = 0.05077899\n",
      "Iteration 28, loss = 0.05098616\n",
      "Iteration 29, loss = 0.05052464\n",
      "Iteration 30, loss = 0.05007631\n",
      "Iteration 31, loss = 0.05064161\n",
      "Iteration 32, loss = 0.05043393\n",
      "Iteration 33, loss = 0.05149201\n",
      "Iteration 34, loss = 0.04828600\n",
      "Iteration 35, loss = 0.04970748\n",
      "Iteration 36, loss = 0.04936251\n",
      "Iteration 37, loss = 0.04963945\n",
      "Iteration 38, loss = 0.04803425\n",
      "Iteration 39, loss = 0.04945204\n",
      "Iteration 40, loss = 0.04838654\n",
      "Iteration 41, loss = 0.04849394\n",
      "Iteration 42, loss = 0.04734651\n",
      "Iteration 43, loss = 0.04769495\n",
      "Iteration 44, loss = 0.04880685\n",
      "Iteration 45, loss = 0.04836132\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ........................alpha=0.01;, score=0.639 total time= 3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65722038\n",
      "Iteration 2, loss = 0.43626507\n",
      "Iteration 3, loss = 0.21774106\n",
      "Iteration 4, loss = 0.12112475\n",
      "Iteration 5, loss = 0.08493901\n",
      "Iteration 6, loss = 0.06897639\n",
      "Iteration 7, loss = 0.05892004\n",
      "Iteration 8, loss = 0.05208061\n",
      "Iteration 9, loss = 0.04809691\n",
      "Iteration 10, loss = 0.04520381\n",
      "Iteration 11, loss = 0.04264817\n",
      "Iteration 12, loss = 0.04104417\n",
      "Iteration 13, loss = 0.03928738\n",
      "Iteration 14, loss = 0.03786395\n",
      "Iteration 15, loss = 0.03649119\n",
      "Iteration 16, loss = 0.03614867\n",
      "Iteration 17, loss = 0.03583490\n",
      "Iteration 18, loss = 0.03624316\n",
      "Iteration 19, loss = 0.03435628\n",
      "Iteration 20, loss = 0.03388058\n",
      "Iteration 21, loss = 0.03388917\n",
      "Iteration 22, loss = 0.03327046\n",
      "Iteration 23, loss = 0.03332787\n",
      "Iteration 24, loss = 0.03297975\n",
      "Iteration 25, loss = 0.03287427\n",
      "Iteration 26, loss = 0.03236315\n",
      "Iteration 27, loss = 0.03217436\n",
      "Iteration 28, loss = 0.03156761\n",
      "Iteration 29, loss = 0.03180140\n",
      "Iteration 30, loss = 0.03212690\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ......................alpha=0.0001;, score=0.712 total time= 2.4min\n",
      "Iteration 1, loss = 0.64938168\n",
      "Iteration 2, loss = 0.40306638\n",
      "Iteration 3, loss = 0.20994945\n",
      "Iteration 4, loss = 0.13256145\n",
      "Iteration 5, loss = 0.10304767\n",
      "Iteration 6, loss = 0.08818630\n",
      "Iteration 7, loss = 0.07824522\n",
      "Iteration 8, loss = 0.07357071\n",
      "Iteration 9, loss = 0.06932397\n",
      "Iteration 10, loss = 0.06577380\n",
      "Iteration 11, loss = 0.06362716\n",
      "Iteration 12, loss = 0.06169846\n",
      "Iteration 13, loss = 0.05920234\n",
      "Iteration 14, loss = 0.05928878\n",
      "Iteration 15, loss = 0.05635870\n",
      "Iteration 16, loss = 0.05550996\n",
      "Iteration 17, loss = 0.05534489\n",
      "Iteration 18, loss = 0.05338877\n",
      "Iteration 19, loss = 0.05338197\n",
      "Iteration 20, loss = 0.05235159\n",
      "Iteration 21, loss = 0.05194279\n",
      "Iteration 22, loss = 0.05026898\n",
      "Iteration 23, loss = 0.05031123\n",
      "Iteration 24, loss = 0.04994820\n",
      "Iteration 25, loss = 0.04995533\n",
      "Iteration 26, loss = 0.04852336\n",
      "Iteration 27, loss = 0.04920398\n",
      "Iteration 28, loss = 0.04808030\n",
      "Iteration 29, loss = 0.04770749\n",
      "Iteration 30, loss = 0.04778935\n",
      "Iteration 31, loss = 0.04689205\n",
      "Iteration 32, loss = 0.04771159\n",
      "Iteration 33, loss = 0.04683653\n",
      "Iteration 34, loss = 0.04705541\n",
      "Iteration 35, loss = 0.04581633\n",
      "Iteration 36, loss = 0.04649624\n",
      "Iteration 37, loss = 0.04496807\n",
      "Iteration 38, loss = 0.04487257\n",
      "Iteration 39, loss = 0.04527693\n",
      "Iteration 40, loss = 0.04492736\n",
      "Iteration 41, loss = 0.04616491\n",
      "Iteration 42, loss = 0.04501146\n",
      "Iteration 43, loss = 0.04453554\n",
      "Iteration 44, loss = 0.04437157\n",
      "Iteration 45, loss = 0.04431739\n",
      "Iteration 46, loss = 0.04394727\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ........................alpha=0.01;, score=0.626 total time= 3.4min\n",
      "Iteration 1, loss = 0.65846491\n",
      "Iteration 2, loss = 0.41971797\n",
      "Iteration 3, loss = 0.20358946\n",
      "Iteration 4, loss = 0.11490802\n",
      "Iteration 5, loss = 0.07955809\n",
      "Iteration 6, loss = 0.06377862\n",
      "Iteration 7, loss = 0.05389268\n",
      "Iteration 8, loss = 0.04728210\n",
      "Iteration 9, loss = 0.04339373\n",
      "Iteration 10, loss = 0.04085705\n",
      "Iteration 11, loss = 0.03774066\n",
      "Iteration 12, loss = 0.03607751\n",
      "Iteration 13, loss = 0.03490893\n",
      "Iteration 14, loss = 0.03346366\n",
      "Iteration 15, loss = 0.03264543\n",
      "Iteration 16, loss = 0.03255518\n",
      "Iteration 17, loss = 0.03120296\n",
      "Iteration 18, loss = 0.03118674\n",
      "Iteration 19, loss = 0.03012506\n",
      "Iteration 20, loss = 0.03008262\n",
      "Iteration 21, loss = 0.02973851\n",
      "Iteration 22, loss = 0.02852052\n",
      "Iteration 23, loss = 0.02872994\n",
      "Iteration 24, loss = 0.02898247\n",
      "Iteration 25, loss = 0.02839248\n",
      "Iteration 26, loss = 0.02856783\n",
      "Iteration 27, loss = 0.02828055\n",
      "Iteration 28, loss = 0.02833636\n",
      "Iteration 29, loss = 0.02804044\n",
      "Iteration 30, loss = 0.02869530\n",
      "Iteration 31, loss = 0.02819906\n",
      "Iteration 32, loss = 0.02817668\n",
      "Iteration 33, loss = 0.02787702\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .......................alpha=0.001;, score=0.635 total time= 2.6min\n",
      "Iteration 1, loss = 0.67095575\n",
      "Iteration 2, loss = 0.44364881\n",
      "Iteration 3, loss = 0.29825083\n",
      "Iteration 4, loss = 0.23914736\n",
      "Iteration 5, loss = 0.21339967\n",
      "Iteration 6, loss = 0.19797950\n",
      "Iteration 7, loss = 0.18793681\n",
      "Iteration 8, loss = 0.17864630\n",
      "Iteration 9, loss = 0.17332453\n",
      "Iteration 10, loss = 0.16736739\n",
      "Iteration 11, loss = 0.16248447\n",
      "Iteration 12, loss = 0.15834361\n",
      "Iteration 13, loss = 0.15484538\n",
      "Iteration 14, loss = 0.15382777\n",
      "Iteration 15, loss = 0.15172546\n",
      "Iteration 16, loss = 0.15220454\n",
      "Iteration 17, loss = 0.14869894\n",
      "Iteration 18, loss = 0.14730537\n",
      "Iteration 19, loss = 0.14539576\n",
      "Iteration 20, loss = 0.14596486\n",
      "Iteration 21, loss = 0.14241584\n",
      "Iteration 22, loss = 0.14096514\n",
      "Iteration 23, loss = 0.14089438\n",
      "Iteration 24, loss = 0.13907728\n",
      "Iteration 25, loss = 0.14225488\n",
      "Iteration 26, loss = 0.14112245\n",
      "Iteration 27, loss = 0.13793690\n",
      "Iteration 28, loss = 0.13703242\n",
      "Iteration 29, loss = 0.13689491\n",
      "Iteration 30, loss = 0.13634579\n",
      "Iteration 31, loss = 0.13525885\n",
      "Iteration 32, loss = 0.13516179\n",
      "Iteration 33, loss = 0.13449982\n",
      "Iteration 34, loss = 0.13436995\n",
      "Iteration 35, loss = 0.13346252\n",
      "Iteration 36, loss = 0.12995341\n",
      "Iteration 37, loss = 0.13371832\n",
      "Iteration 38, loss = 0.13395535\n",
      "Iteration 39, loss = 0.13284362\n",
      "Iteration 40, loss = 0.13418197\n",
      "Iteration 41, loss = 0.13171243\n",
      "Iteration 42, loss = 0.13196043\n",
      "Iteration 43, loss = 0.13400483\n",
      "Iteration 44, loss = 0.13162239\n",
      "Iteration 45, loss = 0.12793855\n",
      "Iteration 46, loss = 0.12947111\n",
      "Iteration 47, loss = 0.12939962\n",
      "Iteration 48, loss = 0.12733732\n",
      "Iteration 49, loss = 0.12953224\n",
      "Iteration 50, loss = 0.13010372\n",
      "Iteration 51, loss = 0.13109341\n",
      "Iteration 52, loss = 0.12966113\n",
      "Iteration 53, loss = 0.13050591\n",
      "Iteration 54, loss = 0.12832275\n",
      "Iteration 55, loss = 0.12936669\n",
      "Iteration 56, loss = 0.12697777\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .........................alpha=0.1;, score=0.659 total time= 3.8min\n",
      "Iteration 1, loss = 0.65236097\n",
      "Iteration 2, loss = 0.41008488\n",
      "Iteration 3, loss = 0.19994527\n",
      "Iteration 4, loss = 0.11642592\n",
      "Iteration 5, loss = 0.08408810\n",
      "Iteration 6, loss = 0.06925653\n",
      "Iteration 7, loss = 0.05965191\n",
      "Iteration 8, loss = 0.05429208\n",
      "Iteration 9, loss = 0.04997928\n",
      "Iteration 10, loss = 0.04791088\n",
      "Iteration 11, loss = 0.04640971\n",
      "Iteration 12, loss = 0.04412154\n",
      "Iteration 13, loss = 0.04227866\n",
      "Iteration 14, loss = 0.04111040\n",
      "Iteration 15, loss = 0.04060377\n",
      "Iteration 16, loss = 0.04006780\n",
      "Iteration 17, loss = 0.03980595\n",
      "Iteration 18, loss = 0.03878941\n",
      "Iteration 19, loss = 0.03905939\n",
      "Iteration 20, loss = 0.03877897\n",
      "Iteration 21, loss = 0.03748823\n",
      "Iteration 22, loss = 0.03825623\n",
      "Iteration 23, loss = 0.03740967\n",
      "Iteration 24, loss = 0.03719190\n",
      "Iteration 25, loss = 0.03675721\n",
      "Iteration 26, loss = 0.03752014\n",
      "Iteration 27, loss = 0.03673683\n",
      "Iteration 28, loss = 0.03702455\n",
      "Iteration 29, loss = 0.03566867\n",
      "Iteration 30, loss = 0.03602015\n",
      "Iteration 31, loss = 0.03560090\n",
      "Iteration 32, loss = 0.03574855\n",
      "Iteration 33, loss = 0.03609886\n",
      "Iteration 34, loss = 0.03569791\n",
      "Iteration 35, loss = 0.03630148\n",
      "Iteration 36, loss = 0.03601188\n",
      "Iteration 37, loss = 0.03582914\n",
      "Iteration 38, loss = 0.03526301\n",
      "Iteration 39, loss = 0.03494211\n",
      "Iteration 40, loss = 0.03571865\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .......................alpha=0.001;, score=0.638 total time= 3.1min\n",
      "Iteration 1, loss = 0.66984642\n",
      "Iteration 2, loss = 0.46235071\n",
      "Iteration 3, loss = 0.31088969\n",
      "Iteration 4, loss = 0.24929398\n",
      "Iteration 5, loss = 0.22272905\n",
      "Iteration 6, loss = 0.20637693\n",
      "Iteration 7, loss = 0.19413867\n",
      "Iteration 8, loss = 0.18679620\n",
      "Iteration 9, loss = 0.18006836\n",
      "Iteration 10, loss = 0.17535918\n",
      "Iteration 11, loss = 0.17073037\n",
      "Iteration 12, loss = 0.16537158\n",
      "Iteration 13, loss = 0.16241506\n",
      "Iteration 14, loss = 0.15796367\n",
      "Iteration 15, loss = 0.15592245\n",
      "Iteration 16, loss = 0.15491169\n",
      "Iteration 17, loss = 0.15198673\n",
      "Iteration 18, loss = 0.15040240\n",
      "Iteration 19, loss = 0.15171750\n",
      "Iteration 20, loss = 0.15015886\n",
      "Iteration 21, loss = 0.14842596\n",
      "Iteration 22, loss = 0.14891544\n",
      "Iteration 23, loss = 0.14765622\n",
      "Iteration 24, loss = 0.14327214\n",
      "Iteration 25, loss = 0.14388196\n",
      "Iteration 26, loss = 0.14284347\n",
      "Iteration 27, loss = 0.14380138\n",
      "Iteration 28, loss = 0.14298236\n",
      "Iteration 29, loss = 0.14124153\n",
      "Iteration 30, loss = 0.13958533\n",
      "Iteration 31, loss = 0.13928291\n",
      "Iteration 32, loss = 0.14000755\n",
      "Iteration 33, loss = 0.14078459\n",
      "Iteration 34, loss = 0.14036174\n",
      "Iteration 35, loss = 0.13942458\n",
      "Iteration 36, loss = 0.14042699\n",
      "Iteration 37, loss = 0.13874521\n",
      "Iteration 38, loss = 0.13401112\n",
      "Iteration 39, loss = 0.13813767\n",
      "Iteration 40, loss = 0.13647138\n",
      "Iteration 41, loss = 0.13335173\n",
      "Iteration 42, loss = 0.13381491\n",
      "Iteration 43, loss = 0.13455379\n",
      "Iteration 44, loss = 0.13523491\n",
      "Iteration 45, loss = 0.13609919\n",
      "Iteration 46, loss = 0.13544082\n",
      "Iteration 47, loss = 0.13428615\n",
      "Iteration 48, loss = 0.13276931\n",
      "Iteration 49, loss = 0.13319550\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.677 total time= 3.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65830580\n",
      "Iteration 2, loss = 0.43244254\n",
      "Iteration 3, loss = 0.21163385\n",
      "Iteration 4, loss = 0.12001160\n",
      "Iteration 5, loss = 0.08455803\n",
      "Iteration 6, loss = 0.06679121\n",
      "Iteration 7, loss = 0.05698308\n",
      "Iteration 8, loss = 0.05125013\n",
      "Iteration 9, loss = 0.04681886\n",
      "Iteration 10, loss = 0.04464012\n",
      "Iteration 11, loss = 0.04103793\n",
      "Iteration 12, loss = 0.03952796\n",
      "Iteration 13, loss = 0.03831575\n",
      "Iteration 14, loss = 0.03781712\n",
      "Iteration 15, loss = 0.03633931\n",
      "Iteration 16, loss = 0.03540449\n",
      "Iteration 17, loss = 0.03477819\n",
      "Iteration 18, loss = 0.03486516\n",
      "Iteration 19, loss = 0.03428989\n",
      "Iteration 20, loss = 0.03414960\n",
      "Iteration 21, loss = 0.03372396\n",
      "Iteration 22, loss = 0.03367372\n",
      "Iteration 23, loss = 0.03248387\n",
      "Iteration 24, loss = 0.03285296\n",
      "Iteration 25, loss = 0.03306433\n",
      "Iteration 26, loss = 0.03244188\n",
      "Iteration 27, loss = 0.03171769\n",
      "Iteration 28, loss = 0.03188833\n",
      "Iteration 29, loss = 0.03160686\n",
      "Iteration 30, loss = 0.03152192\n",
      "Iteration 31, loss = 0.03097679\n",
      "Iteration 32, loss = 0.03131951\n",
      "Iteration 33, loss = 0.03110688\n",
      "Iteration 34, loss = 0.03182258\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .......................alpha=0.001;, score=0.660 total time= 2.7min\n",
      "Iteration 1, loss = 0.66139602\n",
      "Iteration 2, loss = 0.44370640\n",
      "Iteration 3, loss = 0.30439942\n",
      "Iteration 4, loss = 0.24661643\n",
      "Iteration 5, loss = 0.22104643\n",
      "Iteration 6, loss = 0.20450759\n",
      "Iteration 7, loss = 0.19300632\n",
      "Iteration 8, loss = 0.18511814\n",
      "Iteration 9, loss = 0.17751260\n",
      "Iteration 10, loss = 0.17150875\n",
      "Iteration 11, loss = 0.16822318\n",
      "Iteration 12, loss = 0.16588978\n",
      "Iteration 13, loss = 0.16044312\n",
      "Iteration 14, loss = 0.15650656\n",
      "Iteration 15, loss = 0.15346822\n",
      "Iteration 16, loss = 0.15202769\n",
      "Iteration 17, loss = 0.15049294\n",
      "Iteration 18, loss = 0.15123070\n",
      "Iteration 19, loss = 0.14908797\n",
      "Iteration 20, loss = 0.14529991\n",
      "Iteration 21, loss = 0.14315703\n",
      "Iteration 22, loss = 0.14434887\n",
      "Iteration 23, loss = 0.14248607\n",
      "Iteration 24, loss = 0.14179419\n",
      "Iteration 25, loss = 0.14114292\n",
      "Iteration 26, loss = 0.14186308\n",
      "Iteration 27, loss = 0.14018736\n",
      "Iteration 28, loss = 0.14203162\n",
      "Iteration 29, loss = 0.13869709\n",
      "Iteration 30, loss = 0.13883300\n",
      "Iteration 31, loss = 0.13795202\n",
      "Iteration 32, loss = 0.13877023\n",
      "Iteration 33, loss = 0.13720576\n",
      "Iteration 34, loss = 0.13820859\n",
      "Iteration 35, loss = 0.13503876\n",
      "Iteration 36, loss = 0.13621634\n",
      "Iteration 37, loss = 0.13466939\n",
      "Iteration 38, loss = 0.13517486\n",
      "Iteration 39, loss = 0.13827033\n",
      "Iteration 40, loss = 0.13502714\n",
      "Iteration 41, loss = 0.13345059\n",
      "Iteration 42, loss = 0.13403879\n",
      "Iteration 43, loss = 0.13461279\n",
      "Iteration 44, loss = 0.13514259\n",
      "Iteration 45, loss = 0.13387166\n",
      "Iteration 46, loss = 0.12974034\n",
      "Iteration 47, loss = 0.13036666\n",
      "Iteration 48, loss = 0.13258544\n",
      "Iteration 49, loss = 0.13080208\n",
      "Iteration 50, loss = 0.13345740\n",
      "Iteration 51, loss = 0.13215487\n",
      "Iteration 52, loss = 0.12978043\n",
      "Iteration 53, loss = 0.13032926\n",
      "Iteration 54, loss = 0.12961463\n",
      "Iteration 55, loss = 0.13193268\n",
      "Iteration 56, loss = 0.13271290\n",
      "Iteration 57, loss = 0.13096821\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.631 total time= 3.8min\n",
      "Iteration 1, loss = 0.66717984\n",
      "Iteration 2, loss = 0.43866320\n",
      "Iteration 3, loss = 0.23196219\n",
      "Iteration 4, loss = 0.12578751\n",
      "Iteration 5, loss = 0.08643313\n",
      "Iteration 6, loss = 0.06790210\n",
      "Iteration 7, loss = 0.05817036\n",
      "Iteration 8, loss = 0.05063194\n",
      "Iteration 9, loss = 0.04721984\n",
      "Iteration 10, loss = 0.04325857\n",
      "Iteration 11, loss = 0.04126689\n",
      "Iteration 12, loss = 0.03884941\n",
      "Iteration 13, loss = 0.03785002\n",
      "Iteration 14, loss = 0.03662959\n",
      "Iteration 15, loss = 0.03620890\n",
      "Iteration 16, loss = 0.03475793\n",
      "Iteration 17, loss = 0.03416316\n",
      "Iteration 18, loss = 0.03339191\n",
      "Iteration 19, loss = 0.03280232\n",
      "Iteration 20, loss = 0.03260475\n",
      "Iteration 21, loss = 0.03183084\n",
      "Iteration 22, loss = 0.03178784\n",
      "Iteration 23, loss = 0.03186239\n",
      "Iteration 24, loss = 0.03108505\n",
      "Iteration 25, loss = 0.03129847\n",
      "Iteration 26, loss = 0.03093098\n",
      "Iteration 27, loss = 0.02989758\n",
      "Iteration 28, loss = 0.03014692\n",
      "Iteration 29, loss = 0.03032393\n",
      "Iteration 30, loss = 0.02996979\n",
      "Iteration 31, loss = 0.02987744\n",
      "Iteration 32, loss = 0.03036695\n",
      "Iteration 33, loss = 0.02982088\n",
      "Iteration 34, loss = 0.02975403\n",
      "Iteration 35, loss = 0.02998942\n",
      "Iteration 36, loss = 0.02939726\n",
      "Iteration 37, loss = 0.02952386\n",
      "Iteration 38, loss = 0.02949022\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ......................alpha=0.0001;, score=0.668 total time= 3.0min\n",
      "Iteration 1, loss = 0.66437975\n",
      "Iteration 2, loss = 0.44810657\n",
      "Iteration 3, loss = 0.30051576\n",
      "Iteration 4, loss = 0.24288594\n",
      "Iteration 5, loss = 0.21541707\n",
      "Iteration 6, loss = 0.19940906\n",
      "Iteration 7, loss = 0.18845642\n",
      "Iteration 8, loss = 0.17949962\n",
      "Iteration 9, loss = 0.17185560\n",
      "Iteration 10, loss = 0.16713878\n",
      "Iteration 11, loss = 0.16320314\n",
      "Iteration 12, loss = 0.15915306\n",
      "Iteration 13, loss = 0.15664545\n",
      "Iteration 14, loss = 0.15348729\n",
      "Iteration 15, loss = 0.15209800\n",
      "Iteration 16, loss = 0.14867155\n",
      "Iteration 17, loss = 0.14606332\n",
      "Iteration 18, loss = 0.14544374\n",
      "Iteration 19, loss = 0.14382951\n",
      "Iteration 20, loss = 0.14278827\n",
      "Iteration 21, loss = 0.14356000\n",
      "Iteration 22, loss = 0.14213916\n",
      "Iteration 23, loss = 0.14158546\n",
      "Iteration 24, loss = 0.13765319\n",
      "Iteration 25, loss = 0.13735062\n",
      "Iteration 26, loss = 0.13715940\n",
      "Iteration 27, loss = 0.13801017\n",
      "Iteration 28, loss = 0.13542999\n",
      "Iteration 29, loss = 0.13397953\n",
      "Iteration 30, loss = 0.13549784\n",
      "Iteration 31, loss = 0.13474644\n",
      "Iteration 32, loss = 0.13492282\n",
      "Iteration 33, loss = 0.13204963\n",
      "Iteration 34, loss = 0.13224741\n",
      "Iteration 35, loss = 0.13144070\n",
      "Iteration 36, loss = 0.13209747\n",
      "Iteration 37, loss = 0.13164231\n",
      "Iteration 38, loss = 0.12873006\n",
      "Iteration 39, loss = 0.13002886\n",
      "Iteration 40, loss = 0.12964712\n",
      "Iteration 41, loss = 0.13242529\n",
      "Iteration 42, loss = 0.13043880\n",
      "Iteration 43, loss = 0.13212590\n",
      "Iteration 44, loss = 0.12785059\n",
      "Iteration 45, loss = 0.12949311\n",
      "Iteration 46, loss = 0.12883677\n",
      "Iteration 47, loss = 0.12998417\n",
      "Iteration 48, loss = 0.12789923\n",
      "Iteration 49, loss = 0.12579163\n",
      "Iteration 50, loss = 0.12937021\n",
      "Iteration 51, loss = 0.12537637\n",
      "Iteration 52, loss = 0.12910574\n",
      "Iteration 53, loss = 0.12530868\n",
      "Iteration 54, loss = 0.12764147\n",
      "Iteration 55, loss = 0.12670751\n",
      "Iteration 56, loss = 0.12390713\n",
      "Iteration 57, loss = 0.12599874\n",
      "Iteration 58, loss = 0.12111373\n",
      "Iteration 59, loss = 0.12278413\n",
      "Iteration 60, loss = 0.12547508\n",
      "Iteration 61, loss = 0.12508865\n",
      "Iteration 62, loss = 0.12439514\n",
      "Iteration 63, loss = 0.12525848\n",
      "Iteration 64, loss = 0.12348661\n",
      "Iteration 65, loss = 0.12244130\n",
      "Iteration 66, loss = 0.12467769\n",
      "Iteration 67, loss = 0.12364194\n",
      "Iteration 68, loss = 0.12481927\n",
      "Iteration 69, loss = 0.12623630\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.642 total time= 4.0min\n",
      "Iteration 1, loss = 0.67495284\n",
      "Iteration 2, loss = 0.44057935\n",
      "Iteration 3, loss = 0.22880554\n",
      "Iteration 4, loss = 0.11863795\n",
      "Iteration 5, loss = 0.08108055\n",
      "Iteration 6, loss = 0.06367253\n",
      "Iteration 7, loss = 0.05329246\n",
      "Iteration 8, loss = 0.04668989\n",
      "Iteration 9, loss = 0.04237924\n",
      "Iteration 10, loss = 0.03908798\n",
      "Iteration 11, loss = 0.03714621\n",
      "Iteration 12, loss = 0.03552001\n",
      "Iteration 13, loss = 0.03370596\n",
      "Iteration 14, loss = 0.03310953\n",
      "Iteration 15, loss = 0.03185094\n",
      "Iteration 16, loss = 0.03115928\n",
      "Iteration 17, loss = 0.03094331\n",
      "Iteration 18, loss = 0.02981307\n",
      "Iteration 19, loss = 0.02965588\n",
      "Iteration 20, loss = 0.02949151\n",
      "Iteration 21, loss = 0.02928492\n",
      "Iteration 22, loss = 0.02921655\n",
      "Iteration 23, loss = 0.02925486\n",
      "Iteration 24, loss = 0.02765305\n",
      "Iteration 25, loss = 0.02828510\n",
      "Iteration 26, loss = 0.02755385\n",
      "Iteration 27, loss = 0.02813084\n",
      "Iteration 28, loss = 0.02856527\n",
      "Iteration 29, loss = 0.02804729\n",
      "Iteration 30, loss = 0.02765062\n",
      "Iteration 31, loss = 0.02684916\n",
      "Iteration 32, loss = 0.02721486\n",
      "Iteration 33, loss = 0.02697769\n",
      "Iteration 34, loss = 0.02662105\n",
      "Iteration 35, loss = 0.02645728\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ......................alpha=0.0001;, score=0.664 total time= 2.7min\n",
      "Iteration 1, loss = 0.67673172\n",
      "Iteration 2, loss = 0.46800745\n",
      "Iteration 3, loss = 0.31140035\n",
      "Iteration 4, loss = 0.24444527\n",
      "Iteration 5, loss = 0.21568595\n",
      "Iteration 6, loss = 0.19678194\n",
      "Iteration 7, loss = 0.18410424\n",
      "Iteration 8, loss = 0.17617901\n",
      "Iteration 9, loss = 0.16884526\n",
      "Iteration 10, loss = 0.16393161\n",
      "Iteration 11, loss = 0.15764720\n",
      "Iteration 12, loss = 0.15299620\n",
      "Iteration 13, loss = 0.14985916\n",
      "Iteration 14, loss = 0.14637564\n",
      "Iteration 15, loss = 0.14535891\n",
      "Iteration 16, loss = 0.14382300\n",
      "Iteration 17, loss = 0.14027266\n",
      "Iteration 18, loss = 0.14011346\n",
      "Iteration 19, loss = 0.13719422\n",
      "Iteration 20, loss = 0.13611934\n",
      "Iteration 21, loss = 0.13374214\n",
      "Iteration 22, loss = 0.13222693\n",
      "Iteration 23, loss = 0.13061625\n",
      "Iteration 24, loss = 0.13139100\n",
      "Iteration 25, loss = 0.12993268\n",
      "Iteration 26, loss = 0.13093411\n",
      "Iteration 27, loss = 0.13013044\n",
      "Iteration 28, loss = 0.13076636\n",
      "Iteration 29, loss = 0.12813513\n",
      "Iteration 30, loss = 0.12743426\n",
      "Iteration 31, loss = 0.12744515\n",
      "Iteration 32, loss = 0.12624557\n",
      "Iteration 33, loss = 0.12543505\n",
      "Iteration 34, loss = 0.12502725\n",
      "Iteration 35, loss = 0.12537802\n",
      "Iteration 36, loss = 0.12329962\n",
      "Iteration 37, loss = 0.12496308\n",
      "Iteration 38, loss = 0.12486191\n",
      "Iteration 39, loss = 0.12490725\n",
      "Iteration 40, loss = 0.12803006\n",
      "Iteration 41, loss = 0.12621816\n",
      "Iteration 42, loss = 0.12428786\n",
      "Iteration 43, loss = 0.12924919\n",
      "Iteration 44, loss = 0.12228202\n",
      "Iteration 45, loss = 0.12032001\n",
      "Iteration 46, loss = 0.12232871\n",
      "Iteration 47, loss = 0.12073736\n",
      "Iteration 48, loss = 0.12174429\n",
      "Iteration 49, loss = 0.12083305\n",
      "Iteration 50, loss = 0.11888996\n",
      "Iteration 51, loss = 0.11996531\n",
      "Iteration 52, loss = 0.11955364\n",
      "Iteration 53, loss = 0.11793848\n",
      "Iteration 54, loss = 0.12059467\n",
      "Iteration 55, loss = 0.11817430\n",
      "Iteration 56, loss = 0.11641049\n",
      "Iteration 57, loss = 0.11799932\n",
      "Iteration 58, loss = 0.11889867\n",
      "Iteration 59, loss = 0.12490704\n",
      "Iteration 60, loss = 0.11937767\n",
      "Iteration 61, loss = 0.12027085\n",
      "Iteration 62, loss = 0.12324339\n",
      "Iteration 63, loss = 0.12041221\n",
      "Iteration 64, loss = 0.12100144\n",
      "Iteration 65, loss = 0.11531591\n",
      "Iteration 66, loss = 0.11697730\n",
      "Iteration 67, loss = 0.11805728\n",
      "Iteration 68, loss = 0.11751331\n",
      "Iteration 69, loss = 0.11646189\n",
      "Iteration 70, loss = 0.11684752\n",
      "Iteration 71, loss = 0.11629812\n",
      "Iteration 72, loss = 0.11786632\n",
      "Iteration 73, loss = 0.11527536\n",
      "Iteration 74, loss = 0.11425047\n",
      "Iteration 75, loss = 0.11424418\n",
      "Iteration 76, loss = 0.11907825\n",
      "Iteration 77, loss = 0.11680083\n",
      "Iteration 78, loss = 0.11811563\n",
      "Iteration 79, loss = 0.11762561\n",
      "Iteration 80, loss = 0.11574388\n",
      "Iteration 81, loss = 0.11524541\n",
      "Iteration 82, loss = 0.11442230\n",
      "Iteration 83, loss = 0.11647140\n",
      "Iteration 84, loss = 0.11544657\n",
      "Iteration 85, loss = 0.11417997\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .........................alpha=0.1;, score=0.627 total time= 4.6min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlpc = MLPClassifier(verbose=True, tol=.001)\n",
    "mlpc_parameters = {\n",
    "    \"alpha\": [.0001, .001, .01, .1]\n",
    "}\n",
    "mlpc_cv = GridSearchCV(mlpc, mlpc_parameters, verbose=3, n_jobs=-1)\n",
    "mlpc_cv.fit(bigram_train, train_df['target'])\n",
    "print(mlpc_cv.best_params_)\n",
    "\n",
    "mlpc_predicted_classes = mlpc_cv.predict(bigram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b60346",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_out_array = []\n",
    "for i, pred_class in enumerate(mlpc_predicted_classes):\n",
    "    mlpc_out_array.append([int(test_ids[i]), pred_class])\n",
    "    \n",
    "np.savetxt(\"mlpc-results.csv\", mlpc_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "953d4736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Iteration 1, loss = 0.68355896\n",
      "Iteration 2, loss = 0.48614873\n",
      "Iteration 3, loss = 0.34652288\n",
      "Iteration 4, loss = 0.28740941\n",
      "Iteration 5, loss = 0.26029323\n",
      "Iteration 6, loss = 0.24369238\n",
      "Iteration 7, loss = 0.23209767\n",
      "Iteration 8, loss = 0.22412241\n",
      "Iteration 9, loss = 0.21758061\n",
      "Iteration 10, loss = 0.21094973\n",
      "Iteration 11, loss = 0.20776511\n",
      "Iteration 12, loss = 0.20285762\n",
      "Iteration 13, loss = 0.19865968\n",
      "Iteration 14, loss = 0.19885302\n",
      "Iteration 15, loss = 0.19540739\n",
      "Iteration 16, loss = 0.19169593\n",
      "Iteration 17, loss = 0.18792039\n",
      "Iteration 18, loss = 0.18964934\n",
      "Iteration 19, loss = 0.18932816\n",
      "Iteration 20, loss = 0.18457252\n",
      "Iteration 21, loss = 0.18629064\n",
      "Iteration 22, loss = 0.18258087\n",
      "Iteration 23, loss = 0.18106847\n",
      "Iteration 24, loss = 0.18118651\n",
      "Iteration 25, loss = 0.17810902\n",
      "Iteration 26, loss = 0.17847097\n",
      "Iteration 27, loss = 0.17870419\n",
      "Iteration 28, loss = 0.17757657\n",
      "Iteration 29, loss = 0.17859157\n",
      "Iteration 30, loss = 0.17950050\n",
      "Iteration 31, loss = 0.18145595\n",
      "Iteration 32, loss = 0.17257831\n",
      "Iteration 33, loss = 0.17117473\n",
      "Iteration 34, loss = 0.17288556\n",
      "Iteration 35, loss = 0.17171998\n",
      "Iteration 36, loss = 0.16920415\n",
      "Iteration 37, loss = 0.16850789\n",
      "Iteration 38, loss = 0.17100287\n",
      "Iteration 1, loss = 0.66845982\n",
      "Iteration 2, loss = 0.46734661\n",
      "Iteration 3, loss = 0.24053502\n",
      "Iteration 4, loss = 0.12767754\n",
      "Iteration 5, loss = 0.08920044\n",
      "Iteration 6, loss = 0.07235444\n",
      "Iteration 7, loss = 0.06262169\n",
      "Iteration 8, loss = 0.05576890\n",
      "Iteration 9, loss = 0.05111441\n",
      "Iteration 10, loss = 0.04881176\n",
      "Iteration 11, loss = 0.04612314\n",
      "Iteration 12, loss = 0.04475244\n",
      "Iteration 13, loss = 0.04355625\n",
      "Iteration 14, loss = 0.04163603\n",
      "Iteration 15, loss = 0.04050375\n",
      "Iteration 16, loss = 0.03988511\n",
      "Iteration 17, loss = 0.03899605\n",
      "Iteration 18, loss = 0.03874382\n",
      "Iteration 19, loss = 0.03774875\n",
      "Iteration 20, loss = 0.03813121\n",
      "Iteration 21, loss = 0.03779815\n",
      "Iteration 22, loss = 0.03745671\n",
      "Iteration 23, loss = 0.03670331\n",
      "Iteration 24, loss = 0.03635002\n",
      "Iteration 25, loss = 0.03643957\n",
      "Iteration 26, loss = 0.03544602\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ......................alpha=0.0001;, score=0.594 total time= 2.4min\n",
      "Iteration 1, loss = 0.66163123\n",
      "Iteration 2, loss = 0.44149868\n",
      "Iteration 3, loss = 0.22896353\n",
      "Iteration 4, loss = 0.14631194\n",
      "Iteration 5, loss = 0.11476668\n",
      "Iteration 6, loss = 0.10042217\n",
      "Iteration 7, loss = 0.09026335\n",
      "Iteration 8, loss = 0.08462924\n",
      "Iteration 9, loss = 0.07999379\n",
      "Iteration 10, loss = 0.07730272\n",
      "Iteration 11, loss = 0.07525474\n",
      "Iteration 12, loss = 0.07278582\n",
      "Iteration 13, loss = 0.07001206\n",
      "Iteration 14, loss = 0.06871232\n",
      "Iteration 15, loss = 0.06760476\n",
      "Iteration 16, loss = 0.06597548\n",
      "Iteration 17, loss = 0.06486897\n",
      "Iteration 18, loss = 0.06399222\n",
      "Iteration 19, loss = 0.06311053\n",
      "Iteration 20, loss = 0.06242622\n",
      "Iteration 21, loss = 0.06165651\n",
      "Iteration 22, loss = 0.06073562\n",
      "Iteration 23, loss = 0.06055591\n",
      "Iteration 24, loss = 0.05963291\n",
      "Iteration 25, loss = 0.06052975\n",
      "Iteration 26, loss = 0.05953114\n",
      "Iteration 27, loss = 0.05940121\n",
      "Iteration 28, loss = 0.05957269\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ........................alpha=0.01;, score=0.598 total time= 2.5min\n",
      "Iteration 39, loss = 0.17028900\n",
      "Iteration 40, loss = 0.17000697\n",
      "Iteration 41, loss = 0.17258994\n",
      "Iteration 42, loss = 0.16787115\n",
      "Iteration 43, loss = 0.16764346\n",
      "Iteration 44, loss = 0.16978147\n",
      "Iteration 45, loss = 0.16615842\n",
      "Iteration 46, loss = 0.16697978\n",
      "Iteration 47, loss = 0.16637171\n",
      "Iteration 48, loss = 0.16627304\n",
      "Iteration 49, loss = 0.16520794\n",
      "Iteration 50, loss = 0.16726994\n",
      "Iteration 51, loss = 0.16486873\n",
      "Iteration 52, loss = 0.16776010\n",
      "Iteration 53, loss = 0.16592083\n",
      "Iteration 54, loss = 0.16675422\n",
      "Iteration 1, loss = 0.66582203\n",
      "Iteration 2, loss = 0.45892875\n",
      "Iteration 3, loss = 0.22361181\n",
      "Iteration 4, loss = 0.12664112\n",
      "Iteration 5, loss = 0.09109428\n",
      "Iteration 6, loss = 0.07361674\n",
      "Iteration 7, loss = 0.06426647\n",
      "Iteration 8, loss = 0.05718441\n",
      "Iteration 9, loss = 0.05341914\n",
      "Iteration 10, loss = 0.05008648\n",
      "Iteration 11, loss = 0.04788362\n",
      "Iteration 12, loss = 0.04659853\n",
      "Iteration 13, loss = 0.04453454\n",
      "Iteration 14, loss = 0.04339544\n",
      "Iteration 15, loss = 0.04216639\n",
      "Iteration 16, loss = 0.04193455\n",
      "Iteration 17, loss = 0.04096329\n",
      "Iteration 18, loss = 0.04008648\n",
      "Iteration 19, loss = 0.04070374\n",
      "Iteration 20, loss = 0.03921975\n",
      "Iteration 21, loss = 0.03951928\n",
      "Iteration 22, loss = 0.03888038\n",
      "Iteration 23, loss = 0.03885872\n",
      "Iteration 24, loss = 0.03922975\n",
      "Iteration 25, loss = 0.03806906\n",
      "Iteration 26, loss = 0.03780450\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .......................alpha=0.001;, score=0.605 total time= 2.4min\n",
      "Iteration 1, loss = 0.68315238\n",
      "Iteration 2, loss = 0.46994035\n",
      "Iteration 3, loss = 0.25487259\n",
      "Iteration 4, loss = 0.15578429\n",
      "Iteration 5, loss = 0.11958688\n",
      "Iteration 6, loss = 0.10122079\n",
      "Iteration 7, loss = 0.09284686\n",
      "Iteration 8, loss = 0.08698750\n",
      "Iteration 9, loss = 0.08262257\n",
      "Iteration 10, loss = 0.08017716\n",
      "Iteration 11, loss = 0.07770749\n",
      "Iteration 12, loss = 0.07514944\n",
      "Iteration 13, loss = 0.07514697\n",
      "Iteration 14, loss = 0.07156893\n",
      "Iteration 15, loss = 0.07073086\n",
      "Iteration 16, loss = 0.06984961\n",
      "Iteration 17, loss = 0.06792751\n",
      "Iteration 18, loss = 0.06676401\n",
      "Iteration 19, loss = 0.06700523\n",
      "Iteration 20, loss = 0.06549914\n",
      "Iteration 21, loss = 0.06424534\n",
      "Iteration 22, loss = 0.06463387\n",
      "Iteration 23, loss = 0.06292391\n",
      "Iteration 24, loss = 0.06337692\n",
      "Iteration 25, loss = 0.06338540\n",
      "Iteration 26, loss = 0.06164546\n",
      "Iteration 27, loss = 0.06203400\n",
      "Iteration 28, loss = 0.06084516\n",
      "Iteration 29, loss = 0.06029098\n",
      "Iteration 30, loss = 0.06239537\n",
      "Iteration 31, loss = 0.06042765\n",
      "Iteration 32, loss = 0.06049614\n",
      "Iteration 33, loss = 0.05946069\n",
      "Iteration 34, loss = 0.06028477\n",
      "Iteration 35, loss = 0.05953314\n",
      "Iteration 36, loss = 0.05949793\n",
      "Iteration 37, loss = 0.06239548\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END ........................alpha=0.01;, score=0.593 total time= 3.3min\n",
      "Iteration 55, loss = 0.16460285\n",
      "Iteration 56, loss = 0.16085799\n",
      "Iteration 57, loss = 0.16023833\n",
      "Iteration 58, loss = 0.16075752\n",
      "Iteration 59, loss = 0.16819299\n",
      "Iteration 60, loss = 0.16371485\n",
      "Iteration 1, loss = 0.67879646\n",
      "Iteration 2, loss = 0.46986352\n",
      "Iteration 3, loss = 0.25478419\n",
      "Iteration 4, loss = 0.12723528\n",
      "Iteration 5, loss = 0.08514214\n",
      "Iteration 6, loss = 0.06623066\n",
      "Iteration 7, loss = 0.05597538\n",
      "Iteration 8, loss = 0.04907653\n",
      "Iteration 9, loss = 0.04513254\n",
      "Iteration 10, loss = 0.04157411\n",
      "Iteration 11, loss = 0.03837989\n",
      "Iteration 12, loss = 0.03711550\n",
      "Iteration 13, loss = 0.03473044\n",
      "Iteration 14, loss = 0.03352592\n",
      "Iteration 15, loss = 0.03331850\n",
      "Iteration 16, loss = 0.03198950\n",
      "Iteration 17, loss = 0.03088175\n",
      "Iteration 18, loss = 0.03053856\n",
      "Iteration 19, loss = 0.02980359\n",
      "Iteration 20, loss = 0.02930898\n",
      "Iteration 21, loss = 0.02904596\n",
      "Iteration 22, loss = 0.02866723\n",
      "Iteration 23, loss = 0.02812416\n",
      "Iteration 24, loss = 0.02822185\n",
      "Iteration 25, loss = 0.02771371\n",
      "Iteration 26, loss = 0.02777051\n",
      "Iteration 27, loss = 0.02746420\n",
      "Iteration 28, loss = 0.02772725\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ......................alpha=0.0001;, score=0.582 total time= 2.5min\n",
      "Iteration 1, loss = 0.66689076\n",
      "Iteration 2, loss = 0.45285482\n",
      "Iteration 3, loss = 0.22635740\n",
      "Iteration 4, loss = 0.14045877\n",
      "Iteration 5, loss = 0.10851918\n",
      "Iteration 6, loss = 0.09307201\n",
      "Iteration 7, loss = 0.08440083\n",
      "Iteration 8, loss = 0.07946151\n",
      "Iteration 9, loss = 0.07435784\n",
      "Iteration 10, loss = 0.07162272\n",
      "Iteration 11, loss = 0.06911923\n",
      "Iteration 12, loss = 0.06791088\n",
      "Iteration 13, loss = 0.06570777\n",
      "Iteration 14, loss = 0.06473723\n",
      "Iteration 15, loss = 0.06312945\n",
      "Iteration 16, loss = 0.06207960\n",
      "Iteration 17, loss = 0.06080749\n",
      "Iteration 18, loss = 0.05999960\n",
      "Iteration 19, loss = 0.05944939\n",
      "Iteration 20, loss = 0.05878324\n",
      "Iteration 21, loss = 0.05917246\n",
      "Iteration 22, loss = 0.05779018\n",
      "Iteration 23, loss = 0.05633840\n",
      "Iteration 24, loss = 0.05554760\n",
      "Iteration 25, loss = 0.05624973\n",
      "Iteration 26, loss = 0.05612389\n",
      "Iteration 27, loss = 0.05431304\n",
      "Iteration 28, loss = 0.05485513\n",
      "Iteration 29, loss = 0.05385858\n",
      "Iteration 30, loss = 0.05398981\n",
      "Iteration 31, loss = 0.05432183\n",
      "Iteration 32, loss = 0.05385114\n",
      "Iteration 33, loss = 0.05329737\n",
      "Iteration 34, loss = 0.05332273\n",
      "Iteration 35, loss = 0.05268959\n",
      "Iteration 36, loss = 0.05260131\n",
      "Iteration 37, loss = 0.05283772\n",
      "Iteration 38, loss = 0.05164917\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ........................alpha=0.01;, score=0.608 total time= 3.4min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67355953\n",
      "Iteration 2, loss = 0.46689167\n",
      "Iteration 3, loss = 0.25456639\n",
      "Iteration 4, loss = 0.13501289\n",
      "Iteration 5, loss = 0.08953687\n",
      "Iteration 6, loss = 0.06925091\n",
      "Iteration 7, loss = 0.05791273\n",
      "Iteration 8, loss = 0.05116710\n",
      "Iteration 9, loss = 0.04598761\n",
      "Iteration 10, loss = 0.04309239\n",
      "Iteration 11, loss = 0.04081037\n",
      "Iteration 12, loss = 0.03901257\n",
      "Iteration 13, loss = 0.03775687\n",
      "Iteration 14, loss = 0.03629759\n",
      "Iteration 15, loss = 0.03508685\n",
      "Iteration 16, loss = 0.03425316\n",
      "Iteration 17, loss = 0.03284821\n",
      "Iteration 18, loss = 0.03307943\n",
      "Iteration 19, loss = 0.03316874\n",
      "Iteration 20, loss = 0.03216909\n",
      "Iteration 21, loss = 0.03144714\n",
      "Iteration 22, loss = 0.03136626\n",
      "Iteration 23, loss = 0.03114611\n",
      "Iteration 24, loss = 0.03084622\n",
      "Iteration 25, loss = 0.03143071\n",
      "Iteration 26, loss = 0.03024149\n",
      "Iteration 27, loss = 0.02988824\n",
      "Iteration 28, loss = 0.03015446\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END ......................alpha=0.0001;, score=0.609 total time= 2.5min\n",
      "Iteration 1, loss = 0.68041795\n",
      "Iteration 2, loss = 0.47921060\n",
      "Iteration 3, loss = 0.25345948\n",
      "Iteration 4, loss = 0.15491153\n",
      "Iteration 5, loss = 0.11849534\n",
      "Iteration 6, loss = 0.10246956\n",
      "Iteration 7, loss = 0.09278454\n",
      "Iteration 8, loss = 0.08546843\n",
      "Iteration 9, loss = 0.08193018\n",
      "Iteration 10, loss = 0.07834228\n",
      "Iteration 11, loss = 0.07577671\n",
      "Iteration 12, loss = 0.07238973\n",
      "Iteration 13, loss = 0.07070306\n",
      "Iteration 14, loss = 0.06959055\n",
      "Iteration 15, loss = 0.06852689\n",
      "Iteration 16, loss = 0.06654773\n",
      "Iteration 17, loss = 0.06588798\n",
      "Iteration 18, loss = 0.06445857\n",
      "Iteration 19, loss = 0.06520316\n",
      "Iteration 20, loss = 0.06398959\n",
      "Iteration 21, loss = 0.06287060\n",
      "Iteration 22, loss = 0.06211186\n",
      "Iteration 23, loss = 0.06124883\n",
      "Iteration 24, loss = 0.06102075\n",
      "Iteration 25, loss = 0.06104174\n",
      "Iteration 26, loss = 0.06020852\n",
      "Iteration 27, loss = 0.05910820\n",
      "Iteration 28, loss = 0.06067841\n",
      "Iteration 29, loss = 0.05904389\n",
      "Iteration 30, loss = 0.05843524\n",
      "Iteration 31, loss = 0.05810266\n",
      "Iteration 32, loss = 0.05778180\n",
      "Iteration 33, loss = 0.05774801\n",
      "Iteration 34, loss = 0.05699699\n",
      "Iteration 35, loss = 0.05779932\n",
      "Iteration 36, loss = 0.05675929\n",
      "Iteration 37, loss = 0.05862292\n",
      "Iteration 38, loss = 0.05743455\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ........................alpha=0.01;, score=0.601 total time= 3.3min\n",
      "Iteration 61, loss = 0.16559550\n",
      "Iteration 62, loss = 0.16431660\n",
      "Iteration 63, loss = 0.16272132\n",
      "Iteration 64, loss = 0.15826583\n",
      "Iteration 65, loss = 0.15936393\n",
      "Iteration 66, loss = 0.16102028\n",
      "Iteration 67, loss = 0.15928779\n",
      "Iteration 68, loss = 0.15782402\n",
      "Iteration 69, loss = 0.16074492\n",
      "Iteration 70, loss = 0.15907845\n",
      "Iteration 71, loss = 0.15932920\n",
      "Iteration 72, loss = 0.15898408\n",
      "Iteration 73, loss = 0.16147341\n",
      "Iteration 74, loss = 0.15812337\n",
      "Iteration 75, loss = 0.15772243\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "{'alpha': 0.1}\n",
      "Iteration 1, loss = 0.66847392\n",
      "Iteration 2, loss = 0.46328723\n",
      "Iteration 3, loss = 0.23850882\n",
      "Iteration 4, loss = 0.12645413\n",
      "Iteration 5, loss = 0.08833113\n",
      "Iteration 6, loss = 0.07072242\n",
      "Iteration 7, loss = 0.06091601\n",
      "Iteration 8, loss = 0.05416614\n",
      "Iteration 9, loss = 0.04981496\n",
      "Iteration 10, loss = 0.04770941\n",
      "Iteration 11, loss = 0.04495465\n",
      "Iteration 12, loss = 0.04291451\n",
      "Iteration 13, loss = 0.04094672\n",
      "Iteration 14, loss = 0.03909367\n",
      "Iteration 15, loss = 0.03967367\n",
      "Iteration 16, loss = 0.03803346\n",
      "Iteration 17, loss = 0.03713471\n",
      "Iteration 18, loss = 0.03676308\n",
      "Iteration 19, loss = 0.03643257\n",
      "Iteration 20, loss = 0.03609238\n",
      "Iteration 21, loss = 0.03544801\n",
      "Iteration 22, loss = 0.03492318\n",
      "Iteration 23, loss = 0.03473514\n",
      "Iteration 24, loss = 0.03451836\n",
      "Iteration 25, loss = 0.03406903\n",
      "Iteration 26, loss = 0.03407608\n",
      "Iteration 27, loss = 0.03353491\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END ......................alpha=0.0001;, score=0.607 total time= 2.4min\n",
      "Iteration 1, loss = 0.67063808\n",
      "Iteration 2, loss = 0.44932967\n",
      "Iteration 3, loss = 0.22581977\n",
      "Iteration 4, loss = 0.13926947\n",
      "Iteration 5, loss = 0.10695522\n",
      "Iteration 6, loss = 0.09124146\n",
      "Iteration 7, loss = 0.08303362\n",
      "Iteration 8, loss = 0.07710231\n",
      "Iteration 9, loss = 0.07229233\n",
      "Iteration 10, loss = 0.06921782\n",
      "Iteration 11, loss = 0.06648250\n",
      "Iteration 12, loss = 0.06482466\n",
      "Iteration 13, loss = 0.06324167\n",
      "Iteration 14, loss = 0.06144584\n",
      "Iteration 15, loss = 0.05967636\n",
      "Iteration 16, loss = 0.05882280\n",
      "Iteration 17, loss = 0.05775964\n",
      "Iteration 18, loss = 0.05709771\n",
      "Iteration 19, loss = 0.05596973\n",
      "Iteration 20, loss = 0.05576118\n",
      "Iteration 21, loss = 0.05604039\n",
      "Iteration 22, loss = 0.05499278\n",
      "Iteration 23, loss = 0.05392587\n",
      "Iteration 24, loss = 0.05259059\n",
      "Iteration 25, loss = 0.05292397\n",
      "Iteration 26, loss = 0.05218487\n",
      "Iteration 27, loss = 0.05226637\n",
      "Iteration 28, loss = 0.05196333\n",
      "Iteration 29, loss = 0.05118811\n",
      "Iteration 30, loss = 0.05070210\n",
      "Iteration 31, loss = 0.05205497\n",
      "Iteration 32, loss = 0.05111042\n",
      "Iteration 33, loss = 0.05103584\n",
      "Iteration 34, loss = 0.04925060\n",
      "Iteration 35, loss = 0.04878733\n",
      "Iteration 36, loss = 0.05018803\n",
      "Iteration 37, loss = 0.05046648\n",
      "Iteration 38, loss = 0.05105270\n",
      "Iteration 39, loss = 0.04903097\n",
      "Iteration 40, loss = 0.04847927\n",
      "Iteration 41, loss = 0.04735532\n",
      "Iteration 42, loss = 0.04761094\n",
      "Iteration 43, loss = 0.04847939\n",
      "Iteration 44, loss = 0.04690688\n",
      "Iteration 45, loss = 0.04694302\n",
      "Iteration 46, loss = 0.04799945\n",
      "Iteration 47, loss = 0.04775270\n",
      "Iteration 48, loss = 0.04649877\n",
      "Iteration 49, loss = 0.04653241\n",
      "Iteration 50, loss = 0.04670566\n",
      "Iteration 51, loss = 0.04673851\n",
      "Iteration 52, loss = 0.04777876\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END ........................alpha=0.01;, score=0.578 total time= 4.3min\n",
      "Iteration 1, loss = 0.66481917\n",
      "Iteration 2, loss = 0.44008869\n",
      "Iteration 3, loss = 0.20636500\n",
      "Iteration 4, loss = 0.11883894\n",
      "Iteration 5, loss = 0.08449183\n",
      "Iteration 6, loss = 0.06853566\n",
      "Iteration 7, loss = 0.05860878\n",
      "Iteration 8, loss = 0.05320356\n",
      "Iteration 9, loss = 0.04867398\n",
      "Iteration 10, loss = 0.04643016\n",
      "Iteration 11, loss = 0.04340969\n",
      "Iteration 12, loss = 0.04180486\n",
      "Iteration 13, loss = 0.04047105\n",
      "Iteration 14, loss = 0.03947612\n",
      "Iteration 15, loss = 0.03915118\n",
      "Iteration 16, loss = 0.03810769\n",
      "Iteration 17, loss = 0.03846120\n",
      "Iteration 18, loss = 0.03680750\n",
      "Iteration 19, loss = 0.03610106\n",
      "Iteration 20, loss = 0.03630711\n",
      "Iteration 21, loss = 0.03536622\n",
      "Iteration 22, loss = 0.03545799\n",
      "Iteration 23, loss = 0.03558026\n",
      "Iteration 24, loss = 0.03510425\n",
      "Iteration 25, loss = 0.03534851\n",
      "Iteration 26, loss = 0.03488733\n",
      "Iteration 27, loss = 0.03482136\n",
      "Iteration 28, loss = 0.03377285\n",
      "Iteration 29, loss = 0.03380603\n",
      "Iteration 30, loss = 0.03346554\n",
      "Iteration 31, loss = 0.03398422\n",
      "Iteration 32, loss = 0.03316197\n",
      "Iteration 33, loss = 0.03356381\n",
      "Iteration 34, loss = 0.03403727\n",
      "Iteration 35, loss = 0.03363234\n",
      "Iteration 36, loss = 0.03344123\n",
      "Iteration 37, loss = 0.03339130\n",
      "Iteration 38, loss = 0.03431822\n",
      "Iteration 39, loss = 0.03365771\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .......................alpha=0.001;, score=0.608 total time= 3.5min\n",
      "Iteration 1, loss = 0.69917038\n",
      "Iteration 2, loss = 0.52958494\n",
      "Iteration 3, loss = 0.37145638\n",
      "Iteration 4, loss = 0.28147547\n",
      "Iteration 5, loss = 0.24540902\n",
      "Iteration 6, loss = 0.22661730\n",
      "Iteration 7, loss = 0.21319901\n",
      "Iteration 8, loss = 0.20337493\n",
      "Iteration 9, loss = 0.19554552\n",
      "Iteration 10, loss = 0.19020986\n",
      "Iteration 11, loss = 0.18456171\n",
      "Iteration 12, loss = 0.17977550\n",
      "Iteration 13, loss = 0.17696486\n",
      "Iteration 14, loss = 0.17378903\n",
      "Iteration 15, loss = 0.17102318\n",
      "Iteration 16, loss = 0.16903854\n",
      "Iteration 17, loss = 0.16690542\n",
      "Iteration 18, loss = 0.16443452\n",
      "Iteration 19, loss = 0.16267901\n",
      "Iteration 20, loss = 0.16238732\n",
      "Iteration 21, loss = 0.16112975\n",
      "Iteration 22, loss = 0.15931310\n",
      "Iteration 23, loss = 0.15667693\n",
      "Iteration 24, loss = 0.15657580\n",
      "Iteration 25, loss = 0.15632169\n",
      "Iteration 26, loss = 0.15483919\n",
      "Iteration 27, loss = 0.15456540\n",
      "Iteration 28, loss = 0.15345877\n",
      "Iteration 29, loss = 0.15180789\n",
      "Iteration 30, loss = 0.14925879\n",
      "Iteration 31, loss = 0.15213533\n",
      "Iteration 32, loss = 0.15242564\n",
      "Iteration 33, loss = 0.15217008\n",
      "Iteration 34, loss = 0.15013203\n",
      "Iteration 35, loss = 0.14908616\n",
      "Iteration 36, loss = 0.14920343\n",
      "Iteration 37, loss = 0.14474011\n",
      "Iteration 38, loss = 0.14778626\n",
      "Iteration 39, loss = 0.14826543\n",
      "Iteration 40, loss = 0.14763875\n",
      "Iteration 41, loss = 0.14595543\n",
      "Iteration 42, loss = 0.14500312\n",
      "Iteration 43, loss = 0.14650831\n",
      "Iteration 44, loss = 0.14461912\n",
      "Iteration 45, loss = 0.14799107\n",
      "Iteration 46, loss = 0.14439485\n",
      "Iteration 47, loss = 0.14529098\n",
      "Iteration 48, loss = 0.14409720\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .........................alpha=0.1;, score=0.616 total time= 3.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66149006\n",
      "Iteration 2, loss = 0.44871846\n",
      "Iteration 3, loss = 0.21380273\n",
      "Iteration 4, loss = 0.12195563\n",
      "Iteration 5, loss = 0.08529489\n",
      "Iteration 6, loss = 0.06863533\n",
      "Iteration 7, loss = 0.05859439\n",
      "Iteration 8, loss = 0.05201601\n",
      "Iteration 9, loss = 0.04775615\n",
      "Iteration 10, loss = 0.04470486\n",
      "Iteration 11, loss = 0.04229508\n",
      "Iteration 12, loss = 0.04022694\n",
      "Iteration 13, loss = 0.03880612\n",
      "Iteration 14, loss = 0.03775526\n",
      "Iteration 15, loss = 0.03710643\n",
      "Iteration 16, loss = 0.03553730\n",
      "Iteration 17, loss = 0.03526422\n",
      "Iteration 18, loss = 0.03436012\n",
      "Iteration 19, loss = 0.03420731\n",
      "Iteration 20, loss = 0.03380222\n",
      "Iteration 21, loss = 0.03360938\n",
      "Iteration 22, loss = 0.03255339\n",
      "Iteration 23, loss = 0.03368861\n",
      "Iteration 24, loss = 0.03287095\n",
      "Iteration 25, loss = 0.03275173\n",
      "Iteration 26, loss = 0.03237421\n",
      "Iteration 27, loss = 0.03119235\n",
      "Iteration 28, loss = 0.03209477\n",
      "Iteration 29, loss = 0.03129244\n",
      "Iteration 30, loss = 0.03106711\n",
      "Iteration 31, loss = 0.03126018\n",
      "Iteration 32, loss = 0.03108015\n",
      "Iteration 33, loss = 0.03164204\n",
      "Iteration 34, loss = 0.03152470\n",
      "Iteration 35, loss = 0.03126946\n",
      "Iteration 36, loss = 0.03131195\n",
      "Iteration 37, loss = 0.03092438\n",
      "Iteration 38, loss = 0.03076474\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .......................alpha=0.001;, score=0.580 total time= 3.4min\n",
      "Iteration 1, loss = 0.67800011\n",
      "Iteration 2, loss = 0.50064996\n",
      "Iteration 3, loss = 0.33744268\n",
      "Iteration 4, loss = 0.27088767\n",
      "Iteration 5, loss = 0.24128634\n",
      "Iteration 6, loss = 0.22335215\n",
      "Iteration 7, loss = 0.21048925\n",
      "Iteration 8, loss = 0.20052359\n",
      "Iteration 9, loss = 0.19273124\n",
      "Iteration 10, loss = 0.18817287\n",
      "Iteration 11, loss = 0.18245358\n",
      "Iteration 12, loss = 0.17925397\n",
      "Iteration 13, loss = 0.17571418\n",
      "Iteration 14, loss = 0.17151378\n",
      "Iteration 15, loss = 0.16967170\n",
      "Iteration 16, loss = 0.16872472\n",
      "Iteration 17, loss = 0.16513445\n",
      "Iteration 18, loss = 0.16459033\n",
      "Iteration 19, loss = 0.16375458\n",
      "Iteration 20, loss = 0.16124963\n",
      "Iteration 21, loss = 0.15906165\n",
      "Iteration 22, loss = 0.15754966\n",
      "Iteration 23, loss = 0.15581453\n",
      "Iteration 24, loss = 0.15593896\n",
      "Iteration 25, loss = 0.15419091\n",
      "Iteration 26, loss = 0.15336623\n",
      "Iteration 27, loss = 0.15124389\n",
      "Iteration 28, loss = 0.15060225\n",
      "Iteration 29, loss = 0.15315269\n",
      "Iteration 30, loss = 0.15179308\n",
      "Iteration 31, loss = 0.14932137\n",
      "Iteration 32, loss = 0.15141128\n",
      "Iteration 33, loss = 0.14837960\n",
      "Iteration 34, loss = 0.14872121\n",
      "Iteration 35, loss = 0.15044796\n",
      "Iteration 36, loss = 0.14671336\n",
      "Iteration 37, loss = 0.14614375\n",
      "Iteration 38, loss = 0.14462359\n",
      "Iteration 39, loss = 0.14319884\n",
      "Iteration 40, loss = 0.14446043\n",
      "Iteration 41, loss = 0.15298241\n",
      "Iteration 42, loss = 0.14815220\n",
      "Iteration 43, loss = 0.14656230\n",
      "Iteration 44, loss = 0.14752875\n",
      "Iteration 45, loss = 0.14771980\n",
      "Iteration 46, loss = 0.14339093\n",
      "Iteration 47, loss = 0.14491185\n",
      "Iteration 48, loss = 0.14545536\n",
      "Iteration 49, loss = 0.14418089\n",
      "Iteration 50, loss = 0.14402062\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 4/5] END .........................alpha=0.1;, score=0.609 total time= 3.8min\n",
      "Iteration 1, loss = 0.67315270\n",
      "Iteration 2, loss = 0.45038726\n",
      "Iteration 3, loss = 0.22666984\n",
      "Iteration 4, loss = 0.12495834\n",
      "Iteration 5, loss = 0.09008978\n",
      "Iteration 6, loss = 0.07427428\n",
      "Iteration 7, loss = 0.06419957\n",
      "Iteration 8, loss = 0.05957185\n",
      "Iteration 9, loss = 0.05362222\n",
      "Iteration 10, loss = 0.05121103\n",
      "Iteration 11, loss = 0.04941363\n",
      "Iteration 12, loss = 0.04745771\n",
      "Iteration 13, loss = 0.04598195\n",
      "Iteration 14, loss = 0.04563345\n",
      "Iteration 15, loss = 0.04405265\n",
      "Iteration 16, loss = 0.04396851\n",
      "Iteration 17, loss = 0.04368529\n",
      "Iteration 18, loss = 0.04245851\n",
      "Iteration 19, loss = 0.04197141\n",
      "Iteration 20, loss = 0.04124448\n",
      "Iteration 21, loss = 0.04082977\n",
      "Iteration 22, loss = 0.04090697\n",
      "Iteration 23, loss = 0.04120087\n",
      "Iteration 24, loss = 0.04080102\n",
      "Iteration 25, loss = 0.04082323\n",
      "Iteration 26, loss = 0.04052245\n",
      "Iteration 27, loss = 0.04093269\n",
      "Iteration 28, loss = 0.03970236\n",
      "Iteration 29, loss = 0.04045255\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .......................alpha=0.001;, score=0.592 total time= 2.6min\n",
      "Iteration 1, loss = 0.68823576\n",
      "Iteration 2, loss = 0.51020948\n",
      "Iteration 3, loss = 0.34257710\n",
      "Iteration 4, loss = 0.26612028\n",
      "Iteration 5, loss = 0.23415676\n",
      "Iteration 6, loss = 0.21715918\n",
      "Iteration 7, loss = 0.20358902\n",
      "Iteration 8, loss = 0.19546127\n",
      "Iteration 9, loss = 0.18931079\n",
      "Iteration 10, loss = 0.18275377\n",
      "Iteration 11, loss = 0.17789751\n",
      "Iteration 12, loss = 0.17572338\n",
      "Iteration 13, loss = 0.17355897\n",
      "Iteration 14, loss = 0.16809362\n",
      "Iteration 15, loss = 0.16601459\n",
      "Iteration 16, loss = 0.16434585\n",
      "Iteration 17, loss = 0.16301025\n",
      "Iteration 18, loss = 0.16118578\n",
      "Iteration 19, loss = 0.16083949\n",
      "Iteration 20, loss = 0.15802279\n",
      "Iteration 21, loss = 0.15802266\n",
      "Iteration 22, loss = 0.15733681\n",
      "Iteration 23, loss = 0.15687545\n",
      "Iteration 24, loss = 0.15734313\n",
      "Iteration 25, loss = 0.15379067\n",
      "Iteration 26, loss = 0.15516014\n",
      "Iteration 27, loss = 0.15141712\n",
      "Iteration 28, loss = 0.15192330\n",
      "Iteration 29, loss = 0.15161884\n",
      "Iteration 30, loss = 0.15058300\n",
      "Iteration 31, loss = 0.14993509\n",
      "Iteration 32, loss = 0.14922269\n",
      "Iteration 33, loss = 0.14591147\n",
      "Iteration 34, loss = 0.14663974\n",
      "Iteration 35, loss = 0.14793263\n",
      "Iteration 36, loss = 0.14567812\n",
      "Iteration 37, loss = 0.14725713\n",
      "Iteration 38, loss = 0.14497114\n",
      "Iteration 39, loss = 0.14491329\n",
      "Iteration 40, loss = 0.14581115\n",
      "Iteration 41, loss = 0.14634783\n",
      "Iteration 42, loss = 0.14492403\n",
      "Iteration 43, loss = 0.14537254\n",
      "Iteration 44, loss = 0.14388594\n",
      "Iteration 45, loss = 0.14486349\n",
      "Iteration 46, loss = 0.14557635\n",
      "Iteration 47, loss = 0.14438848\n",
      "Iteration 48, loss = 0.14497838\n",
      "Iteration 49, loss = 0.14192715\n",
      "Iteration 50, loss = 0.14362712\n",
      "Iteration 51, loss = 0.14180121\n",
      "Iteration 52, loss = 0.14130414\n",
      "Iteration 53, loss = 0.13961154\n",
      "Iteration 54, loss = 0.14282182\n",
      "Iteration 55, loss = 0.14282758\n",
      "Iteration 56, loss = 0.14676353\n",
      "Iteration 57, loss = 0.14214141\n",
      "Iteration 58, loss = 0.14453556\n",
      "Iteration 59, loss = 0.13994579\n",
      "Iteration 60, loss = 0.14215105\n",
      "Iteration 61, loss = 0.14328706\n",
      "Iteration 62, loss = 0.13990035\n",
      "Iteration 63, loss = 0.13932036\n",
      "Iteration 64, loss = 0.14202568\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/5] END .........................alpha=0.1;, score=0.603 total time= 4.8min\n",
      "Iteration 1, loss = 0.68474508\n",
      "Iteration 2, loss = 0.49054388\n",
      "Iteration 3, loss = 0.26417184\n",
      "Iteration 4, loss = 0.13897429\n",
      "Iteration 5, loss = 0.09695382\n",
      "Iteration 6, loss = 0.07776234\n",
      "Iteration 7, loss = 0.06717830\n",
      "Iteration 8, loss = 0.06040455\n",
      "Iteration 9, loss = 0.05625883\n",
      "Iteration 10, loss = 0.05288992\n",
      "Iteration 11, loss = 0.05015442\n",
      "Iteration 12, loss = 0.04882126\n",
      "Iteration 13, loss = 0.04696942\n",
      "Iteration 14, loss = 0.04616341\n",
      "Iteration 15, loss = 0.04509997\n",
      "Iteration 16, loss = 0.04385660\n",
      "Iteration 17, loss = 0.04365025\n",
      "Iteration 18, loss = 0.04265888\n",
      "Iteration 19, loss = 0.04222812\n",
      "Iteration 20, loss = 0.04181521\n",
      "Iteration 21, loss = 0.04071360\n",
      "Iteration 22, loss = 0.04061094\n",
      "Iteration 23, loss = 0.04001187\n",
      "Iteration 24, loss = 0.03957061\n",
      "Iteration 25, loss = 0.03991947\n",
      "Iteration 26, loss = 0.03961276\n",
      "Iteration 27, loss = 0.03960397\n",
      "Iteration 28, loss = 0.03922956\n",
      "Iteration 29, loss = 0.03896937\n",
      "Iteration 30, loss = 0.03856932\n",
      "Iteration 31, loss = 0.03901476\n",
      "Iteration 32, loss = 0.03857577\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END .......................alpha=0.001;, score=0.605 total time= 2.9min\n",
      "Iteration 1, loss = 0.67387858\n",
      "Iteration 2, loss = 0.48167887\n",
      "Iteration 3, loss = 0.33007922\n",
      "Iteration 4, loss = 0.26712974\n",
      "Iteration 5, loss = 0.23680378\n",
      "Iteration 6, loss = 0.21926967\n",
      "Iteration 7, loss = 0.20695379\n",
      "Iteration 8, loss = 0.19684552\n",
      "Iteration 9, loss = 0.18945121\n",
      "Iteration 10, loss = 0.18394023\n",
      "Iteration 11, loss = 0.17876869\n",
      "Iteration 12, loss = 0.17361874\n",
      "Iteration 13, loss = 0.17106494\n",
      "Iteration 14, loss = 0.16787043\n",
      "Iteration 15, loss = 0.16621942\n",
      "Iteration 16, loss = 0.16174380\n",
      "Iteration 17, loss = 0.16122212\n",
      "Iteration 18, loss = 0.16063020\n",
      "Iteration 19, loss = 0.15708035\n",
      "Iteration 20, loss = 0.15648512\n",
      "Iteration 21, loss = 0.15577943\n",
      "Iteration 22, loss = 0.15410978\n",
      "Iteration 23, loss = 0.15230227\n",
      "Iteration 24, loss = 0.15050672\n",
      "Iteration 25, loss = 0.14910319\n",
      "Iteration 26, loss = 0.14773512\n",
      "Iteration 27, loss = 0.14650510\n",
      "Iteration 28, loss = 0.14731434\n",
      "Iteration 29, loss = 0.14531098\n",
      "Iteration 30, loss = 0.14515171\n",
      "Iteration 31, loss = 0.14438576\n",
      "Iteration 32, loss = 0.14486015\n",
      "Iteration 33, loss = 0.14436966\n",
      "Iteration 34, loss = 0.14342149\n",
      "Iteration 35, loss = 0.14076993\n",
      "Iteration 36, loss = 0.14127814\n",
      "Iteration 37, loss = 0.14050302\n",
      "Iteration 38, loss = 0.14130134\n",
      "Iteration 39, loss = 0.14259498\n",
      "Iteration 40, loss = 0.14093254\n",
      "Iteration 41, loss = 0.14017812\n",
      "Iteration 42, loss = 0.14160432\n",
      "Iteration 43, loss = 0.13968391\n",
      "Iteration 44, loss = 0.13645197\n",
      "Iteration 45, loss = 0.13806121\n",
      "Iteration 46, loss = 0.14075363\n",
      "Iteration 47, loss = 0.13658608\n",
      "Iteration 48, loss = 0.13856133\n",
      "Iteration 49, loss = 0.13707410\n",
      "Iteration 50, loss = 0.13859114\n",
      "Iteration 51, loss = 0.14543834\n",
      "Iteration 52, loss = 0.14188835\n",
      "Iteration 53, loss = 0.13693600\n",
      "Iteration 54, loss = 0.13375499\n",
      "Iteration 55, loss = 0.13525105\n",
      "Iteration 56, loss = 0.13611639\n",
      "Iteration 57, loss = 0.13534475\n",
      "Iteration 58, loss = 0.13350302\n",
      "Iteration 59, loss = 0.13266327\n",
      "Iteration 60, loss = 0.13332094\n",
      "Iteration 61, loss = 0.13461260\n",
      "Iteration 62, loss = 0.13482733\n",
      "Iteration 63, loss = 0.13531282\n",
      "Iteration 64, loss = 0.13523984\n",
      "Iteration 65, loss = 0.13213858\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/5] END .........................alpha=0.1;, score=0.581 total time= 4.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67501335\n",
      "Iteration 2, loss = 0.47119074\n",
      "Iteration 3, loss = 0.25407266\n",
      "Iteration 4, loss = 0.13068693\n",
      "Iteration 5, loss = 0.09074118\n",
      "Iteration 6, loss = 0.07259510\n",
      "Iteration 7, loss = 0.06245738\n",
      "Iteration 8, loss = 0.05623104\n",
      "Iteration 9, loss = 0.05159528\n",
      "Iteration 10, loss = 0.04770466\n",
      "Iteration 11, loss = 0.04522594\n",
      "Iteration 12, loss = 0.04302965\n",
      "Iteration 13, loss = 0.04242634\n",
      "Iteration 14, loss = 0.04095443\n",
      "Iteration 15, loss = 0.03954160\n",
      "Iteration 16, loss = 0.03937467\n",
      "Iteration 17, loss = 0.03868617\n",
      "Iteration 18, loss = 0.03728549\n",
      "Iteration 19, loss = 0.03684525\n",
      "Iteration 20, loss = 0.03696367\n",
      "Iteration 21, loss = 0.03665829\n",
      "Iteration 22, loss = 0.03594930\n",
      "Iteration 23, loss = 0.03559408\n",
      "Iteration 24, loss = 0.03557445\n",
      "Iteration 25, loss = 0.03494436\n",
      "Iteration 26, loss = 0.03435305\n",
      "Iteration 27, loss = 0.03467107\n",
      "Iteration 28, loss = 0.03460492\n",
      "Iteration 29, loss = 0.03435554\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 5/5] END ......................alpha=0.0001;, score=0.607 total time= 2.6min\n",
      "Iteration 1, loss = 0.67371457\n",
      "Iteration 2, loss = 0.48935012\n",
      "Iteration 3, loss = 0.32775509\n",
      "Iteration 4, loss = 0.26431547\n",
      "Iteration 5, loss = 0.23660636\n",
      "Iteration 6, loss = 0.21835850\n",
      "Iteration 7, loss = 0.20719000\n",
      "Iteration 8, loss = 0.19840042\n",
      "Iteration 9, loss = 0.19198772\n",
      "Iteration 10, loss = 0.18578517\n",
      "Iteration 11, loss = 0.18104507\n",
      "Iteration 12, loss = 0.17736714\n",
      "Iteration 13, loss = 0.17352131\n",
      "Iteration 14, loss = 0.17266788\n",
      "Iteration 15, loss = 0.16875532\n",
      "Iteration 16, loss = 0.16885759\n",
      "Iteration 17, loss = 0.16606795\n",
      "Iteration 18, loss = 0.16478152\n",
      "Iteration 19, loss = 0.16096326\n",
      "Iteration 20, loss = 0.16081719\n",
      "Iteration 21, loss = 0.15782343\n",
      "Iteration 22, loss = 0.15955248\n",
      "Iteration 23, loss = 0.15846907\n",
      "Iteration 24, loss = 0.15682384\n",
      "Iteration 25, loss = 0.15631944\n",
      "Iteration 26, loss = 0.15499020\n",
      "Iteration 27, loss = 0.15390216\n",
      "Iteration 28, loss = 0.15154590\n",
      "Iteration 29, loss = 0.15040286\n",
      "Iteration 30, loss = 0.15006013\n",
      "Iteration 31, loss = 0.14888573\n",
      "Iteration 32, loss = 0.14932492\n",
      "Iteration 33, loss = 0.15090708\n",
      "Iteration 34, loss = 0.14890146\n",
      "Iteration 35, loss = 0.15121353\n",
      "Iteration 36, loss = 0.14839368\n",
      "Iteration 37, loss = 0.14638393\n",
      "Iteration 38, loss = 0.14759476\n",
      "Iteration 39, loss = 0.14994857\n",
      "Iteration 40, loss = 0.14939271\n",
      "Iteration 41, loss = 0.14635981\n",
      "Iteration 42, loss = 0.14795375\n",
      "Iteration 43, loss = 0.14696338\n",
      "Iteration 44, loss = 0.14694260\n",
      "Iteration 45, loss = 0.14434884\n",
      "Iteration 46, loss = 0.14400022\n",
      "Iteration 47, loss = 0.14297476\n",
      "Iteration 48, loss = 0.14307503\n",
      "Iteration 49, loss = 0.14164797\n",
      "Iteration 50, loss = 0.14279718\n",
      "Iteration 51, loss = 0.14420958\n",
      "Iteration 52, loss = 0.14158399\n",
      "Iteration 53, loss = 0.14118767\n",
      "Iteration 54, loss = 0.14811202\n",
      "Iteration 55, loss = 0.14733860\n",
      "Iteration 56, loss = 0.14291454\n",
      "Iteration 57, loss = 0.13875134\n",
      "Iteration 58, loss = 0.13930095\n",
      "Iteration 59, loss = 0.13854324\n",
      "Iteration 60, loss = 0.13698279\n",
      "Iteration 61, loss = 0.14230603\n",
      "Iteration 62, loss = 0.13907286\n",
      "Iteration 63, loss = 0.13935285\n",
      "Iteration 64, loss = 0.13834688\n",
      "Iteration 65, loss = 0.13816273\n",
      "Iteration 66, loss = 0.13696673\n",
      "Iteration 67, loss = 0.13706496\n",
      "Iteration 68, loss = 0.13803152\n",
      "Iteration 69, loss = 0.14096660\n",
      "Iteration 70, loss = 0.13773016\n",
      "Iteration 71, loss = 0.13572181\n",
      "Iteration 72, loss = 0.13712636\n",
      "Iteration 73, loss = 0.13350557\n",
      "Iteration 74, loss = 0.13739722\n",
      "Iteration 75, loss = 0.13728642\n",
      "Iteration 76, loss = 0.13752921\n",
      "Iteration 77, loss = 0.13838569\n",
      "Iteration 78, loss = 0.13580473\n",
      "Iteration 79, loss = 0.14121900\n",
      "Iteration 80, loss = 0.13917177\n",
      "Iteration 81, loss = 0.13888248\n",
      "Iteration 82, loss = 0.13513361\n",
      "Iteration 83, loss = 0.13582276\n",
      "Iteration 84, loss = 0.13638452\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/5] END .........................alpha=0.1;, score=0.605 total time= 5.5min\n"
     ]
    }
   ],
   "source": [
    "trigram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "trigram_train = trigram_vectorizer.fit_transform(formatted_train_tweets)\n",
    "trigram_test = trigram_vectorizer.transform(formatted_test_tweets)\n",
    "\n",
    "mlpc_cv.fit(trigram_train, train_df['target'])\n",
    "print(mlpc_cv.best_params_)\n",
    "\n",
    "mlpc_trigram_predicted_classes = mlpc_cv.predict(trigram_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df257d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpc_trigram_out_array = []\n",
    "for i, pred_class in enumerate(mlpc_trigram_predicted_classes):\n",
    "    mlpc_trigram_out_array.append([int(test_ids[i]), pred_class])\n",
    "    \n",
    "np.savetxt(\"mlpc-trigam-results.csv\", mlpc_trigram_out_array, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9497b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
